[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Causal Fairness Analysis - Software Tools",
    "section": "",
    "text": "This page contains a sequence of vignettes associated with the Causal Fairness Analysis paper (Plečko and Bareinboim 2024). In particular, here you can find all the code used to reproduce the results of the paper, formatted in a vignette-style fashion. There are also additional examples of analyses, which are not presented in the manuscript. In particular, you can find the following examples, grouped by different tasks (bias detection, fair prediction, fair decision-making):"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "pages/shai-challenge.html",
    "href": "pages/shai-challenge.html",
    "title": "COMPAS Analysis - A Causal Approach",
    "section": "",
    "text": "Across United States, courts are using algorithms to predict which of the defendants are likely to recidivate and re-offend. As is becoming apparent in the literature on fair machine learning, algorithms do not have any ethical or moral values a priori, and they are capable of learning, or even amplifying the existing societal bias. If left unattended, such algorithms could lead to the perpetuation of unfairness, raising a serious concern about the impact of AI on socially important questions in the long term.\nIn their seminal work from 2016 (Larson et al. 2016), the team from ProPublica (an independent, non-profit newsroom), analyzed data from a commercial tool called Correctional Offender Management Profiling for Alternative Sanctions (COMPAS), which was developed by Northpointe, Inc. (the company is today known as Equivant). The analysis was performed on a large number of individuals from Broward County, Florida, and compared the recidivism predictions produced by COMPAS with those that actually occurred in practice. The findings of their analysis sent alarm bells ringing to everyone concerned about issues of racial injustice. In particular, the ProPublica team demonstrated, among other things, that:\n\nblack defendants who did not recidivate over a two-year period were nearly twice as likely to be classified as higher risk compared to their white counterparts,\nwhite defendants who did recidivate over a two-year period were labeled as low risk twice as often as black re-offenders.\n\nThe above mentioned disparities observed by ProPublica are a starting point of an important investigation. What their analysis did not investigate is the causal explanation of how the disparities arose. In particular, using the language of causal models, we provide such an explanation as follows. For the observed disparity between the groups, we quantify how much of the disparity can be explained by:\n\nthe direct causal effect of race on the outcome (not explained by other features),\nthe indirect causal effect of race on previous offenses and degree of charge, which in turn influence the outcome,\nthe confounded effect of race, which is associated with age and sex in the dataset, both of which are predictive of outcome.\n\nWe remark here that our causal analysis should be thought of manipulating the ``signals” of race, or its perception, rather than race itself, which is an immutable characteristic of every individual (Weinberger 2022; Greiner and Rubin 2011).\nThe three steps of our analysis are the following: A. we first look at the causal explanation of the two-year recidivism rates \\(Y\\), which will help us understand the causal effects of race in the real world, A. we then look at the causal explanation of the Northpointe’s predictions, which will help us understand the causal effects of the predicted world, A. finally, we look at the causal explanation of the Northpointe’s predictions when subsetting to only the group of individuals who did not recidivate over a two-year period; such an analysis will help us understand how Northpointe’s predictions causally treat individuals who do not re-offend.\nWe now introduce the methodology used in our analysis, which is implemented in the faircause R-package. The package can be installed using:\n\ndevtools::install_github(\"dplecko/CFA\")"
  },
  {
    "objectID": "pages/shai-challenge.html#a-explaining-disparity-in-recidivism",
    "href": "pages/shai-challenge.html#a-explaining-disparity-in-recidivism",
    "title": "COMPAS Analysis - A Causal Approach",
    "section": "A: Explaining disparity in recidivism",
    "text": "A: Explaining disparity in recidivism\nAs our starting point, we first compute the disparity between groups in two-year recidivism:\n\ntapply(data$two_year_recid, data$race, mean)\n\nMajority Minority \n0.393643 0.480042 \n\n\nBased on this information, we can compute that: \\[\\begin{equation}\n  \\text{PG}_{x_0, x_1}(y) = 48\\% - 39.5\\% = 8.5\\%.\n\\end{equation}\\] We then apply the fairness_cookbook() functionality from the R-package, and choose two_year_recid (\\(Y\\)) as the outcome of interest:\n\nX <- \"race\"\nZ <- c(\"age\", \"sex\")\nW <- c(\"juv_fel\", \"juv_misd\", \"juv_other\", \"priors\", \"charge\")\nY <- c(\"two_year_recid\")\ntwo_year <- fairness_cookbook(data, X = X, W = W, Z = Z, Y = Y, \n                              x0 = \"Majority\", x1 = \"Minority\")\ntwo_year\n\nfaircause object:\n\nAttribute:        race \nOutcome:          two_year_recid \nConfounders:      age,sex \nMediators:        juv_fel,juv_misd,juv_other,priors,charge \n\n\nAfter performing the decomposition we can visualize how the decomposition, as shown in Figure 5.\n\n\n\n\n\nFigure 5: Causal decomposition of the parity gap for two-year recidivism.\n\n\n\n\n\n\n\n\n\n\n\n\nTherefore, we have the first crucial result: \\[\n\\begin{equation}\n  \\text{PG}_{x_0, x_1}(y) = \\underbrace{1.5\\%\\;(\\pm 1.8\\%)}_{\\text{race effect}} +  \\underbrace{4\\%\\;(\\pm 0.8\\%)}_{\\text{prior counts effect}} +  \\underbrace{3\\%\\;(\\pm 1.4\\%)}_{\\text{age/sex effect}}.\n\\end{equation}\n\\] Important to note is that race does not have a statistically significant direct effect on outcome."
  },
  {
    "objectID": "pages/shai-challenge.html#b-explaining-disparity-in-northpointes-recidivism-predictions",
    "href": "pages/shai-challenge.html#b-explaining-disparity-in-northpointes-recidivism-predictions",
    "title": "COMPAS Analysis - A Causal Approach",
    "section": "B: Explaining disparity in Northpointe’s recidivism predictions",
    "text": "B: Explaining disparity in Northpointe’s recidivism predictions\nOur next step is to analyze the causal decomposition of the Northpointe’s predictions.\n\ntapply(data$northpointe, data$race, mean)\n\n Majority  Minority \n0.3480033 0.5174370 \n\n\nTherefore, we can compute that: \\[\n\\begin{equation}\n  \\text{PG}_{x_0, x_1}(\\hat{y}) = 52\\% - 35\\% = 17\\%.\n\\end{equation}\n\\] Firstly, we notice that the disparity in the predicted outcome is larger than in the true outcome. But crucially, the question is how this disparity in predicted outcome arises from a causal point of view. We again apply the fairness_cookbook(), this time choosing northpointe (\\(\\hat{Y}\\)) as the outcome of interest:\n\nYhat <- \"northpointe\"\nnorth_decompose <- fairness_cookbook(data, X = X, W = W, Z = Z, Y = Yhat, \n                                     x0 = \"Majority\", x1 = \"Minority\")\nautoplot(north_decompose, decompose = \"xspec\", signed = FALSE) +\n  ggtitle(TeX(\"$PG_{x_0, x_1}(\\\\hat{y})$ decomposition\"))\n\n\n\n\nFigure 6: Causal decomposition of the parity gap for Northpointe’s recidivism prediction.\n\n\n\n\nIn particular, we obtain that: \\[\n\\begin{equation}\n  \\text{PG}_{x_0, x_1}(\\hat{y}) = \\underbrace{5.5\\% \\;(\\pm 1.75\\%)}_{\\text{race effect}} +  \\underbrace{7.5\\% \\;(\\pm 1.2\\%)}_{\\text{prior counts effect}} +  \\underbrace{3\\%\\;(\\pm 1.5\\%)}_{\\text{age/sex effect}}.\n\\end{equation}\n\\] The decomposition is also visualized in Figure 6. Crucially, we find that race does have a statistically significant direct effect on Northpointe’s predictions. Furthermore, race by itself explains one third of the observed disparity."
  },
  {
    "objectID": "pages/shai-challenge.html#c-explaining-disparity-in-northpointes-recidivism-predictions-for-those-who-did-not-recidivate",
    "href": "pages/shai-challenge.html#c-explaining-disparity-in-northpointes-recidivism-predictions-for-those-who-did-not-recidivate",
    "title": "COMPAS Analysis - A Causal Approach",
    "section": "C: Explaining disparity in Northpointe’s recidivism predictions for those who did not recidivate",
    "text": "C: Explaining disparity in Northpointe’s recidivism predictions for those who did not recidivate\nIn the final step, we again perform a decomposition of a disparity, but this time within the group of individuals who did not recidivate. Similarly as before, we can distinguish the direct, indirect, and spurious effects in this disparity. However, the quantity we are decomposing is not the parity gap, but the error rate related to the equality of opportunity criterion, i.e., \\[\n\\begin{equation}\n\\text{ER}_{x_0, x_1}(\\hat{y} | y = 0) = P(\\hat{Y} = 1 \\mid X = x_1, Y = 0) - P(\\hat{Y} = 1 \\mid X = x_0, Y = 0).\n\\end{equation}\n\\] In words, we compare the proportion of minority individuals who are flagged as high risk, but did not recidivate, to the proportion of majority individuals who are flagged as high risk but did not recidivate. We can compute this as:\n\nno_recid <- data$two_year_recid == 0\ntapply(data$northpointe[no_recid], data$race[no_recid], mean)\n\n Majority  Minority \n0.2345430 0.3769697 \n\n\nTherefore, we obtained that: \\[\n\\begin{equation}\n\\text{ER}_{x_0, x_1}(\\hat{y} \\mid y = 0) = 38\\% - 23\\% = 15\\%.\n\\end{equation}\n\\] In words, from defendants who do no recidivate, the minority group defendants are 15% more likely to be labeled as high risk. Once again, we can obtain a causal decomposition, but this time of the \\(\\text{ER}_{x_0, x_1}(\\hat{y} \\mid y = 0)\\) measures. For this purpose, we use the fairness_cookbook_eo() functionality:\n\neo_decompose <- fairness_cookbook_eo(data, X = X, W = W, Z = Z, Y = Y,\n                            Yhat = Yhat, x0 = \"Majority\", x1 = \"Minority\",\n                            ylvl = 0)\nautoplot(eo_decompose, decompose = \"xspec\", signed = FALSE, eo = TRUE)\n\n\n\n\nFigure 7: Causal decomposition of the error rate for the group of non-recidivists.\n\n\n\n\nThe visualization of the decomposition is shown in Figure 7 and can be written mathematically as: \\[\\begin{equation}\n  \\text{ER}_{x_0, x_1}(\\hat{y} \\mid y = 0) = \\underbrace{6.8\\% \\;(\\pm 2\\%)}_{\\text{race effect}} +  \\underbrace{5\\% \\;(\\pm 1.3\\%)}_{\\text{prior counts effect}} +  \\underbrace{3\\%\\;(\\pm 2\\%)}_{\\text{age/sex effect}}.\n\\end{equation}\\] We conclude that there is a direct effect of race on the prediction even within the group of individuals who do not recedivate. Race explains more than one third of the observed disparity in the group."
  },
  {
    "objectID": "pages/shai-challenge.html#key-findings",
    "href": "pages/shai-challenge.html#key-findings",
    "title": "COMPAS Analysis - A Causal Approach",
    "section": "Key Findings",
    "text": "Key Findings\nThe analysis presented in this submission consists of three steps, with each of the steps demonstrating a specific causal explanation. The first step of the analysis demonstrates that the direct effect of race is not statistically significant when analyzing the true recidivism rates. In stark contrast, the second analysis shows that for the predictions produced by Northpointe, the direct effect of race is statistically significant, explaining almost a third of the overall racial disparity. Finally, in the third analysis, we show that the direct effect of race on Northpointe’s predictions was significant even in the group of individuals who did not re-offend.\nTherefore, the absence of the direct effect in the true, observed outcome, and its presence in the predictions for both the overall dataset and the group of non-recidivists, indicates that the predictions produced by Northpointe are strongly racially prejudiced, and constitute a serious mistreatment of the racial minority groups."
  },
  {
    "objectID": "pages/shai-challenge.html#legal-implications",
    "href": "pages/shai-challenge.html#legal-implications",
    "title": "COMPAS Analysis - A Causal Approach",
    "section": "Legal Implications",
    "text": "Legal Implications\nWe quickly discuss some of the possible legal implications of the findings summarized above. Firstly, direct discrimination is explicitly considered in some provisions of the US legal system. For example, under the Title VII of the Civil Rights Act of 1964 (Act 1964a), disparate treatment of individuals based on race is strictly prohibited. Another example is the Fair Housing Act (Act 1964b), which also prohibits direct racial discrimination.\nInterestingly, recent works in legal scholarship have started to turn to interpreting the disparate treatment doctrine in causal language (Plecko and Bareinboim 2022), in particular looking at the direct effect of the protected attribute. The analysis proposed in this work also gives a causal explanation of how the overall disparity came about, and follows from a more broad framework for causal fairness analysis, which was designed with the intention of interpreting the legal doctrines of disparate treatment and disparate impact.\nHowever, the doctrines of disparate treatment and disparate impact would likely not be explicitly considered in a legal proceeding on the COMPAS tool. Nonetheless, the absence of a direct effect of race on the true observed outcome (Experiment A) and its presence in Northpointe’s predictions (Experiments B, C) indicate a clear issue and provide prima facie evidence of intentional racial discrimination, which is prohibited by the Equal Protection clause of the 14th Amendment of the United States Constitution.\nIn conclusion, our causal analysis of the COMPAS tool explains the disparities observed in the data, and demonstrates a clear direct effect of race on predictions of recidivism. As discussed above, such findings likely constitute a basis for legal action against a discriminatory practice from the software provider that produced the predictions."
  },
  {
    "objectID": "pages/college-admissions-t1.html",
    "href": "pages/college-admissions-t1.html",
    "title": "College Admissions: Task 1 over Time",
    "section": "",
    "text": "In this vignette, we analyze a synthetic College Admission dataset, to demonstrate how the task of bias detection could be performed over time, in a university. In particular, we have the following story.\nA university in the United States admits applicants every year. The university is interested in quantifying discrimination in the admission process and track it over time, between 2010 and 2020. The data generating process changes over time, and can be described as follows. Let \\(X\\) denote gender (\\(x_0\\) female, \\(x_1\\) male). Let \\(Z\\) be the age at time of application (\\(Z = 0\\) under 20 years, \\(Z = 1\\) over 20 years) and let \\(W\\) denote the department of application (\\(W = 0\\) for arts&humanities, \\(W = 1\\) for sciences). Finally, let \\(Y\\) denote the admission decision (\\(Y = 0\\) rejection, \\(Y = 1\\) acceptance). The application process changes over time and is given by \\[\n\\begin{empheq}[left ={\\mathcal{F}(t), P(U): \\empheqlbrace}]{align}\n                X & \\gets \\mathbb{1} ( U_X < 0.5 + 0.1U_{XZ})\\label{eq:fcb2-x}\\\\\n                Z & \\gets \\mathbb{1} (U_Z < 0.5 + \\kappa(t) U_{XZ})  \\\\\n                W & \\gets \\mathbb{1} (U_W < 0.5 + \\lambda(t) U_{XZ})  \\\\\n                Y & \\gets \\mathbb{1} (U_Y < 0.1 + \\alpha(t)X + \\beta(t)W + 0.1Z). \\label{eq:fcb2-y} \\\\\n                \\nonumber\\\\\n                    U_{XZ} &\\in \\{0,1\\}, P(U_{XZ} = 1) = 0.5, \\\\\n                    U_X &, U_Z, U_W, U_Y \\sim \\text{Unif}[0, 1].       \n\\end{empheq}\n\\] The coefficients \\(\\kappa(t), \\lambda(t), \\alpha(t), \\beta(t)\\) change every year, and obey the following dynamics: \\[\n\\begin{align}\n    \\kappa(t+1) &= 0.9\\kappa(t) \\\\\n    \\lambda(t+1) &= \\lambda(t) (1 - \\beta(t)) \\\\\n    \\beta(t+1) &= \\beta(t) (1 - \\lambda(t)) f(t), f(t) \\sim \\text{Unif}[0.8, 1.2] \\\\\n    \\alpha(t+1) &= 0.8\\alpha(t).\n\\end{align}\n\\] The equations can be interpreted as follows. The coefficient \\(\\kappa(t)\\) decreases over time, meaning that the overall age gap between the groups decreases. The coefficient \\(\\lambda(t)\\) decreases compared to the previous year, by an amount dependent on \\(\\beta(t)\\). In words, the rate of application to arts&humanities departments decreases if these departments have lower overall admission rates (i.e., students are less likely to apply to departments that are hard to get into). Further, \\(\\alpha(t)\\), which represents gender bias, decreases over time. Finally, \\(\\beta(t)\\) represent the increase in the probability of admission when applying to a science department. Its value depends on the value from the previous year, multiplied by \\((1 - \\lambda(t))\\) and the random variable \\(f(t)\\). Multiplication by the former factor describes the mechanism in which the benefit of applying to a science department decreases if a larger proportion of students apply for it. The latter factor describes a random variation over time which describes how well (in relative terms) the science departments are funded, and can be seen as depending on research and market dynamics in the sciences.\nWe now create functions for generating synethetic data as described above:\n\ncol_adm <- function(n, cfs, kap = cfs[[\"kap\"]], lam = cfs[[\"lam\"]],\n                    alf = cfs[[\"alf\"]], bet = cfs[[\"bet\"]]) {\n\n  u_xz <- rbinom(n, size = 1, prob = 0.5)\n  x <- rbinom(n, size = 1, prob = 0.5 + 0.1 * u_xz)\n  z <- rbinom(n, size = 1, prob = 0.5 + kap * u_xz)\n  d <- rbinom(n, size = 1, prob = 0.5 + lam * x)\n  y <- rbinom(n, size = 1, prob = 0.1 + alf * x + bet * d + 0.1 * z)\n\n  data.frame(x, z, d, y)\n\n}\n\ncfs_nxt <- function(cfs, kap = cfs[[\"kap\"]], lam = cfs[[\"lam\"]],\n                    alf = cfs[[\"alf\"]], bet = cfs[[\"bet\"]]) {\n\n  kapt <- kap * 0.9\n  lamt <- lam * (1 - bet)\n  bett <- bet * (1 - lam) * runif(1, 0.8, 1.2)\n  alft <- alf * 0.8\n\n  list(kap = kapt, lam = lamt, alf = alft, bet = bett)\n\n}\n\nThe head data scientist at the university decides to use the Fairness Cookbook for performing bias quantification. The SFM projection of the causal diagram \\(\\mathcal{G}\\) of the dataset is given by \\[\n\\begin{equation}\n        \\Pi_{\\text{SFM}}(\\mathcal{G}) = \\langle X = \\lbrace X \\rbrace,  Z = \\lbrace Z \\rbrace, W = \\lbrace W\\rbrace, Y = \\lbrace Y \\rbrace\\rangle.\n    \\end{equation}\n\\] After that, the analyst estimates the quantities \\[\n\\begin{align}\n    x\\text{-DE}^{\\text{sym}}_{x}(y \\mid x_0), x\\text{-DE}^{\\text{sym}}_{x}(y \\mid x_0), \\text{ and } x\\text{-SE}_{x_1, x_0}(y) \\;\\;\\; \\forall t \\in \\{2010, \\dots, 2020\\}.\n\\end{align}\n\\]\nThe following code generates the data, obtains the ground truth causal fairness measures, and also performs the fairness_cookbook() at each step.\n\ncol_adm_tim <- function(n, cfs) {\n\n  dat <- bqn <- gtr <- list()\n\n  for (t in seq_len(10)) {\n\n    # generate data for given year\n    dat[[t]] <- col_adm(n, cfs)\n\n    # compute decomposition\n    bqn[[t]] <- fairness_cookbook(dat[[t]], X = \"x\", Z = \"z\", W = \"d\", Y = \"y\",\n                                  x0 = 0, x1 = 1)\n\n    # get ground truth\n    gtr[[t]] <- c(cfs$alf, -(cfs$bet * cfs$lam),\n                  0.1 * ( (0.125 + 0.2 * (0.5 + cfs$kap))/(0.45) -\n                             (0.125 + 0.4 * (0.5 + cfs$kap))/(0.55) ))\n\n    # update the coefficients for each year\n    cfs <- cfs_nxt(cfs)\n\n  }\n\n  list(dat = dat, bqn = bqn, gtr = gtr)\n\n}\n\ncfs <- list(kap = 0.3, lam = 0.2, alf = 0.1, bet = 0.3)\n\nset.seed(22)\nres <- col_adm_tim(n = 5000L, cfs = cfs)\n\notp <- c()\nfor (t in seq_along(res[[\"bqn\"]])) {\n\n  x <- res[[\"bqn\"]][[t]]\n  x_tr <- res[[\"gtr\"]][[t]]\n  add_row <- data.frame(Spurious = x$measures$CtfSE[1],\n                        Indirect = x$measures$CtfIE[1],\n                        Direct = x$measures$CtfDE[1],\n                        Spurious_True = x_tr[3],\n                        Indirect_True = x_tr[2],\n                        Direct_True = x_tr[1],\n                        Year = 2010+t)\n\n  otp <- rbind(otp, add_row)\n\n}\n\ndf <- reshape2::melt(otp, id.vars = \"Year\", variable.name = \"Measure\")\ndf$Meas <- gsub(\"_.*\", \"\", df$Measure)\ndf$whc <- ifelse(grepl(\"_\", df$Measure), \"True (population)\",\n                 \"Estimated (from sample)\")\n\nggplot(df, aes(x = Year, y = value, color = Meas, linetype = whc)) +\n  geom_line() + geom_point() + theme_bw() +\n  ggtitle(\"Bias Quantification Over Time - College Admissions\") +\n  ylab(\"Value\") + scale_y_continuous(labels = scales::percent,\n                                     limits = c(-0.1, 0.15)) +\n  scale_x_continuous(breaks = seq(2010, 2020)) +\n  theme(\n    legend.position = \"bottom\",\n    legend.box.background = element_rect(),\n    legend.box = \"horizontal\"\n  ) + scale_linetype_discrete(name = \"Quantity\") +\n  scale_color_discrete(name = \"Effect\")\n\n\n\n\nFigure 1: Tracking bias over time in the synthetic College Admissions dataset from Example 5.2, between years 2010 and 2020. Both the estimated values from simulated samples (solid line) and the true population values (dashed lines) are shown, for direct (red), indirect (green), and spurious (blue) effects.\n\n\n\n\nThe temporal dynamics of the estimated measures of discrimination (together with the ground truth values obtained from the SCM \\(\\mathcal{M}_t\\)) are shown graphically in Figure 1."
  },
  {
    "objectID": "pages/ci-tests.html",
    "href": "pages/ci-tests.html",
    "title": "Conditional Independence Tests - Census & COMPAS",
    "section": "",
    "text": "The framework of Causal Fairness Analysis (Plecko and Bareinboim 2022) introduces the Standard Fairness Model (SFM), which is a type of causal diagram with slightly fewer assumptions than the typically used diagrams in the literature (in particular, SFM is a type of clustered diagram (see Anand et al. 2021)). In particular, the SFM can be seen in Figure 1.\n\n\n\n \n\n\n\n\n\nFigure 1: Standard Fairness Model\n\n\n\n\n \n\n\n\nWe in particular focus on the bidirected arrow between nodes \\(X\\) and \\(Z\\). Such an arrow in particular allows for the possibility that there are variations between \\(X\\) and \\(Z\\) that can be left unexplained in the model, or that unmeasured confounders may exist, or that we are dealing with selection bias. Practically speaking, assuming that no bidirected arrows exist is a strong assumption that does not hold in many settings. For instance, consider the widely recognized phenomenon in the fairness literature known as redlining (Zenou and Boccard 2000; Hernandez 2009). In some practical settings, the location where loan applicants live may correlate with their race. Applications might be rejected based on the zip code, disproportionately affecting certain minority groups in the real world.\nIt has been reported in the literature that correlation between gender and location, or religious and location may possibly exist, and therefore, should be acknowledged through modeling. For instance, the one-child policy affecting mainly urban areas in China had visible effects in terms of shifting the gender ratio towards males (Hesketh, Lu, and Xing 2005; Ding and Hesketh 2006). Beyond race or gender, religious segregation is also a recognized phenomenon in some urban areas (Brimicombe 2007). Again, while we make no claim that location affects race (or religion), or vice-versa, the bidirected arrows give a degree of modeling flexibility that allows for the encoding of such co-variations. Still, this is without making any commitment to whatever historical processes and other complex dynamics that took place and generated such imbalance in the first place. To corroborate this point, consider the following practical investigation.\nA data scientist is trying to understand the correlation between the features in the COMPAS dataset. The protected attribute \\(X\\) is race, and the demographic variables \\(Z_1\\), \\(Z_2\\) are age and sex. The data scientist tests two hypotheses, namely: \\[\\begin{align}\n    H^{(1)}_0: X \\perp\\!\\!\\!\\perp Z_1,\\\\\n    H^{(2)}_0: X \\perp\\!\\!\\!\\perp Z_2.\n\\end{align}\\] To do so, the data scientist first loads the COMPAS dataset from the faircause package:\n\ndata &lt;- get(data(\"compas\", package = \"faircause\"))\nknitr::kable(head(data), caption = \"COMPAS dataset.\")\n\n\nCOMPAS dataset.\n\n\n\n\n\n\n\n\n\n\n\n\n\nsex\nage\nrace\njuv_fel_count\njuv_misd_count\njuv_other_count\npriors_count\nc_charge_degree\ntwo_year_recid\n\n\n\n\nMale\n69\nNon-White\n0\n0\n0\n0\nF\n0\n\n\nMale\n34\nNon-White\n0\n0\n0\n0\nF\n1\n\n\nMale\n24\nNon-White\n0\n0\n1\n4\nF\n1\n\n\nMale\n23\nNon-White\n0\n1\n0\n1\nF\n0\n\n\nMale\n43\nNon-White\n0\n0\n0\n2\nF\n0\n\n\nMale\n44\nNon-White\n0\n0\n0\n0\nM\n0\n\n\n\n\n\nAfter loading the data, the data scientist visualizes the correlations of the variables:\n\nc1 &lt;- ggplot(data, aes(x = age, fill = race)) +\n  geom_density(alpha = 0.4) + theme_bw() +\n  theme(\n    legend.position = c(0.8, 0.8),\n    legend.box.background = element_rect()\n  ) + ggtitle(TeX(\"COMPAS: age $\\\\perp$ race rejected (p &lt; 0.001)\"))\n\nc2 &lt;- ggplot(data, aes(x = race, fill = sex)) +\n  geom_bar(position = \"fill\") +\n  geom_text(\n    aes(label = percent(round(after_stat(count)/tapply(after_stat(count), after_stat(x), sum)[after_stat(x)], 4))),\n            stat = \"count\", position = position_fill(0.5)\n  ) +\n  scale_y_continuous(labels = percent) + ylab(\"proportion\") +\n  theme_minimal() + ggtitle(TeX(\"COMPAS: race $\\\\perp$ sex rejected (p &lt; 0.001)\"))\n\ncowplot::plot_grid(c1, c2, ncol = 2L)\n\n\n\n\nFigure 2: Association of X and Z sets on the COMPAS dataset.\n\n\n\n\nFigure 2 shows the association of \\(X\\) and \\(Z_1\\), \\(Z_2\\). For testing the independence hypothesis \\(H_0\\), the data scientist runs:\n\nwilcox.test(data[data$race == \"White\", ]$age, data[data$race == \"Non-White\", ]$age)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  data[data$race == \"White\", ]$age and data[data$race == \"Non-White\", ]$age\nW = 7027020, p-value &lt; 2.2e-16\nalternative hypothesis: true location shift is not equal to 0\n\n\nIn fact, both of the hypotheses are rejected (\\(p\\)-values \\(&lt; 0.001\\)). However, possible confounders of this relationship are not measured in the corresponding dataset.\nSimilarly, the same data scientist is now trying to understand the correlation of the features in the Government Census dataset (we skip the code to avoid repetition). The protected attribute \\(X\\) is gender, and the demographic variables \\(Z_1\\), \\(Z_2\\) are age and race. The data scientist tests the independence of sex and age (\\(X \\perp\\!\\!\\!\\perp Z_1\\)), and sex and race (\\(X \\perp\\!\\!\\!\\perp Z_2\\)), and both hypotheses are rejected (p-values \\(&lt; 0.001\\)). Figure 3 shows the associations of the features. Again, possible confounders of this relationship are not measured in the corresponding dataset, meaning that the attribute \\(X\\) cannot be separated from the confounders \\(Z_1, Z_2\\) using any of the observed variables.\n\ndata &lt;- get(data(\"gov_census\", package = \"faircause\"))\ndata &lt;- as.data.frame(data[seq_len(20000), ])\n\nlevels(data$race) &lt;- c(\"Minority\", \"Minority\", \"Minority\", \"Minority\", \n                       \"Minority\", \"Minority\", \"Majority\")\n\nwilcox.test(data[data$sex == \"female\", ]$age, data[data$sex == \"male\", ]$age)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  data[data$sex == \"female\", ]$age and data[data$sex == \"male\", ]$age\nW = 54018487, p-value &lt; 2.2e-16\nalternative hypothesis: true location shift is not equal to 0\n\na1 &lt;- ggplot(data, aes(x = age, fill = sex)) +\n  geom_density(alpha = 0.4) + theme_bw() +\n  theme(\n    legend.position = c(0.2, 0.8),\n    legend.box.background = element_rect()\n  ) + ggtitle(TeX(\"Adult: age $\\\\perp$ sex rejected (p &lt; 0.001)\"))\n\na2 &lt;- ggplot(data, aes(x = sex, fill = race)) +\n  geom_bar(position = \"fill\") +\n  geom_text(aes(label = percent(round(after_stat(count)/tapply(after_stat(count), after_stat(x), sum)[after_stat(x)], 4))),\n            stat = \"count\", position = position_fill(0.5)) +\n  scale_y_continuous(labels = percent) + ylab(\"proportion\") +\n  theme_minimal() + ggtitle(TeX(\"Census: race $\\\\perp$ sex rejected (p &lt; 0.001)\"))\n\ncowplot::plot_grid(a1, a2, ncol = 2L)\n\n\n\n\nFigure 3: Association of X and Z sets on the UCI Adult dataset."
  },
  {
    "objectID": "pages/compas-t2.html",
    "href": "pages/compas-t2.html",
    "title": "Task 2: Fair Predictions on COMPAS",
    "section": "",
    "text": "In this vignette, we focus on Task 2 (Fair Prediction), and specifically focus on the limitation of the currently found methods in the literature to provide causally meaningful fair predictions."
  },
  {
    "objectID": "pages/compas-t2.html#fair-predictions-on-compas",
    "href": "pages/compas-t2.html#fair-predictions-on-compas",
    "title": "Task 2: Fair Predictions on COMPAS",
    "section": "Fair Predictions on COMPAS",
    "text": "Fair Predictions on COMPAS\nA team of data scientists from ProPublica have shown that the COMPAS dataset from Broward County contains a strong racial bias against minorities. They are now interested in producing fair predictions \\(\\widehat{Y}\\) on the dataset, to replace the biased predictions. They first load the COMPAS data:\n\n# load the data\ndat <- get(data(\"compas\", package = \"faircause\"))\nknitr::kable(head(dat), caption = \"COMPAS dataset.\")\n\n\nCOMPAS dataset.\n\n\n\n\n\n\n\n\n\n\n\n\n\nsex\nage\nrace\njuv_fel_count\njuv_misd_count\njuv_other_count\npriors_count\nc_charge_degree\ntwo_year_recid\n\n\n\n\nMale\n69\nNon-White\n0\n0\n0\n0\nF\n0\n\n\nMale\n34\nNon-White\n0\n0\n0\n0\nF\n1\n\n\nMale\n24\nNon-White\n0\n0\n1\n4\nF\n1\n\n\nMale\n23\nNon-White\n0\n1\n0\n1\nF\n0\n\n\nMale\n43\nNon-White\n0\n0\n0\n2\nF\n0\n\n\nMale\n44\nNon-White\n0\n0\n0\n0\nM\n0\n\n\n\n\n# load the metadata\nmdat <- get_metadata(\"compas\")\n\nTo produce fair predictions, they first experiment with four different classifiers.\n\n(i) Random Forest without fairness constraints\n\n# Method 1: Random Forest\norg_dat <- dat\norg_dat$two_year_recid <- ranger(two_year_recid ~ ., dat,\n                                 classification = TRUE)$predictions\n\n# Method 1: decompose Total Variation\norg_tvd <- fairness_cookbook(org_dat, mdat[[\"X\"]], mdat[[\"W\"]], mdat[[\"Z\"]],\n                             mdat[[\"Y\"]], mdat[[\"x0\"]], mdat[[\"x1\"]])\norg_plt <- autoplot(org_tvd, decompose = \"xspec\", dataaset = dataset) +\n  xlab(\"\") + ylab(\"\")\n\n\n\n(ii) Logistic regression trained with reweighing (Kamiran and Calders 2012)\n\nimport os\n\nexec(open(os.path.join(r.root, \"py\", \"reweighing_compas.py\")).read())\n\nWARNING:root:No module named 'tempeh': LawSchoolGPADataset will be unavailable. To install, run:\npip install 'aif360[LawSchoolGPA]'\nWARNING:root:No module named 'tensorflow': AdversarialDebiasing will be unavailable. To install, run:\npip install 'aif360[AdversarialDebiasing]'\nWARNING:root:No module named 'tensorflow': AdversarialDebiasing will be unavailable. To install, run:\npip install 'aif360[AdversarialDebiasing]'\n\nYhat_rew = reweigh_and_predict(r.dat, r.dat)\n\n\n# Method 2: Reweighing by Kamiran & Calders\nreweigh_dat <- dat\nreweigh_dat$two_year_recid <- as.vector(py$Yhat_rew)\n\nreweigh_tvd <- fairness_cookbook(\n  reweigh_dat, mdat[[\"X\"]], mdat[[\"W\"]], mdat[[\"Z\"]], mdat[[\"Y\"]], mdat[[\"x0\"]],\n  mdat[[\"x1\"]])\n\nrew_plt <- autoplot(reweigh_tvd, decompose = \"xspec\", dataset = dataset) +\n  xlab(\"\") + ylab(\"\")\n\n\n\n(iii) Fair Reductions approach of (Agarwal et al. 2018)\n\nexec(open(os.path.join(r.root, \"py\", \"reductions_compas.py\")).read())\nYhat_red = reduce_and_predict(r.dat, r.dat, 0.01)\n\n\n# Method 3: Reductions (Agarwal et. al.)\nsource_python(file.path(root, \"py\", paste0(\"reductions_\", dataset, \".py\")))\n\nred_dat <- dat\nred_dat$two_year_recid <- as.vector(py$Yhat_red)\nred_tvd <- fairness_cookbook(\n  red_dat, mdat[[\"X\"]], mdat[[\"W\"]], mdat[[\"Z\"]], mdat[[\"Y\"]], mdat[[\"x0\"]],\n  mdat[[\"x1\"]]\n)\n\nred_plt <- autoplot(red_tvd, decompose = \"xspec\", dataset = dataset) +\n  xlab(\"\") + ylab(\"\")\n\n\n\n(iv) Random Forest with reject-option post-processing (Kamiran, Karim, and Zhang 2012)\n\n# Method 4: reject-option classification\nrjo_prb <- ranger(two_year_recid ~ ., dat, probability = TRUE)$predictions[, 2]\nrjo_dat <- dat\nrjo_dat$two_year_recid <- RejectOption(rjo_prb, rjo_dat$race)\n\nrjo_tvd <- fairness_cookbook(rjo_dat, mdat[[\"X\"]], mdat[[\"W\"]], mdat[[\"Z\"]],\n                             mdat[[\"Y\"]], mdat[[\"x0\"]], mdat[[\"x1\"]])\nrjo_plt <- autoplot(rjo_tvd, decompose = \"xspec\", dataset = dataset) +\n  xlab(\"\") + ylab(\"\")\n\n\n\nAre the methods successful at eliminating discrimination?\nThe fair prediction algorithms used above are intended to set the TV measure to \\(0\\). After constructing these predictors, the ProPublica team made use of the Fairness Cookbook, to inspect the causal implications of the methods. Following the steps of the Fairness Cookbook, the team computes the TV measure, together with the appropriate measures of direct, indirect, and spurious discrimination. We can now visualize these decompositions in Figure 1.\n\n\n\n\n\nFigure 1: Fair Predictions on the COMPAS dataset.\n\n\n\n\nThe ProPublica team notes that all methods substantially reduce the \\(\\text{TV}_{x_0,x_1}(\\widehat{y})\\), however, the measures of direct, indirect, and, spurious effects are not necessarily reduced to \\(0\\), consistent with the Fair Prediction Theorem."
  },
  {
    "objectID": "pages/compas-t2.html#how-to-fix-the-issue",
    "href": "pages/compas-t2.html#how-to-fix-the-issue",
    "title": "Task 2: Fair Predictions on COMPAS",
    "section": "How to fix the issue?",
    "text": "How to fix the issue?\nTo produce causally meaningful fair predictions, we suggest using the fairadapt package (Plečko and Meinshausen 2020; Plečko, Bennett, and Meinshausen 2021). In particular, the package offers a way of removing discrimination which is based on the causal diagram. In this application, we are interested in constructing fair predictions that remove both the direct and the indirect effect. Firstly, we obtain the adjacency matrix representing the causal diagram associated with the COMPAS dataset:\n\nset.seed(2022)\n# load the causal diagram\nmats <- get_mat(dataset)\nadj.mat <- mats[[1L]]\ncfd.mat <- mats[[2L]]\n\ncausal_graph <- fairadapt::graphModel(adj.mat, cfd.mat)\nplot(causal_graph, size = 50)\n\n\n\n\nAfter loading the causal diagram, we perform fair data adaptation:\n\nfdp <- fairadapt::fairadapt(two_year_recid ~ ., prot.attr = \"race\",\n                            train.data = dat, adj.mat = adj.mat)\n\n# obtain the adapted data\nad_dat <- fairadapt:::adaptedData(fdp)\nad_dat$race <- dat$race\n\n# obtain predictions based on the adapted data\nadapt_oob <- ranger(two_year_recid ~ ., ad_dat,\n                    classification = TRUE)$predictions\nad_dat$two_year_recid <- adapt_oob\n\n# decompose the TV for the predictions\ndat.fairadapt <- dat\ndat.fairadapt$two_year_recid <- adapt_oob\n\nfairadapt_tvd <- fairness_cookbook(\n  ad_dat, mdat[[\"X\"]], mdat[[\"W\"]], mdat[[\"Z\"]],\n  mdat[[\"Y\"]], mdat[[\"x0\"]], mdat[[\"x1\"]]\n)\n\n# visualize the decomposition\nfairadapt_plt <- autoplot(fairadapt_tvd, decompose = \"xspec\", dataset = dataset) +\n  xlab(\"\") + ylab(\"\")\n\nfairadapt_plt\n\n\n\n\nFigure 2: Fair Data Adaptation on the COMPAS dataset.\n\n\n\n\nFigure 2 shows how the fairadapt package can be used to provide causally meaningful predictions that remove both direct and indirect effects."
  },
  {
    "objectID": "pages/compas-t1.html",
    "href": "pages/compas-t1.html",
    "title": "COMPAS - Task 1",
    "section": "",
    "text": "Courts in Broward County, Florida use machine learning to predict whether individuals released on parole are at high risk of re-offending within 2 years (\\(Y\\)). The algorithm is based on the demographic information \\(Z\\) (\\(Z_1\\) for gender, \\(Z_2\\) for age), race \\(X\\) (\\(x_0\\) denoting White, \\(x_1\\) Non-White), juvenile offense counts \\(J\\), prior offense count \\(P\\), and degree of charge \\(D\\).\nIn this vignette, we perform the task of bias detection on this dataset. We begin by loading and pre-processing the original data:\n\ndata <- read.csv(file.path(root, \"inst\", \"extdata\",\n                           \"compas-scores-two-years.csv\"))\ncol.keep <- which(\n  names(data) %in% c(\"age\", \"sex\", \"juv_fel_count\",\n                     \"juv_misd_count\", \"juv_other_count\", \"priors_count\",\n                     \"c_charge_degree\", \"race\", \"two_year_recid\", \"decile_score\")\n)\ndata <- data[, col.keep]\ndata$race <- factor(data$race)\nlevels(data$race) <- c(\"Minority\", \"Minority\", \"Majority\", \"Minority\",\n                       \"Minority\", \"Minority\")\ndata$race <- relevel(data$race, \"Majority\")\ncumsum(table(data$decile_score)) / sum(table(data$decile_score))\n\n        1         2         3         4         5         6         7         8 \n0.1996119 0.3300527 0.4336013 0.5401996 0.6345994 0.7234544 0.8055171 0.8764902 \n        9        10 \n0.9469088 1.0000000 \n\n# decile_score > 4 represents high risk (approximately)\ndata$northpointe <- as.integer(data$decile_score > 4)\ndata$decile_score <- NULL\nnames(data) <- gsub(\"_count\", \"\", names(data))\nnames(data)[which(names(data) == \"c_charge_degree\")] <- \"charge\"\n\nknitr::kable(head(data), caption = \"COMPAS dataset.\")\n\n\nCOMPAS dataset.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsex\nage\nrace\njuv_fel\njuv_misd\njuv_other\npriors\ncharge\ntwo_year_recid\nnorthpointe\n\n\n\n\nMale\n69\nMinority\n0\n0\n0\n0\nF\n0\n0\n\n\nMale\n34\nMinority\n0\n0\n0\n0\nF\n1\n0\n\n\nMale\n24\nMinority\n0\n0\n1\n4\nF\n1\n0\n\n\nMale\n23\nMinority\n0\n1\n0\n1\nF\n0\n1\n\n\nMale\n43\nMinority\n0\n0\n0\n2\nF\n0\n0\n\n\nMale\n44\nMinority\n0\n0\n0\n0\nM\n0\n0\n\n\n\n\n\nIn Causal Fairness Analysis, we are interested in decomposing the TV measure (also known as the parity gap), into its direct, indirect, and spurious components. We show the causal diagram associated with the data, and also a representation of how the target effects can be visualized as follows:\n\n\n\n\n\n\nFigure 1: COMPAS Causal Diagram\n\n\n\n\n\n\n\nFigure 2: Direct effect visualization.\n\n\n\n\n\n\n\n\n\nFigure 3: Indirect effect visualization.\n\n\n\n\n\n\n\nFigure 4: Confounded effect visualization.\n\n\n\n\n\nAfter obtaining the data, we then specify the Standard Fairness Model, and decompose the TV measure for the true outcome \\(Y\\):\n\nX <- \"race\"\nZ <- c(\"age\", \"sex\")\nW <- c(\"juv_fel\", \"juv_misd\", \"juv_other\", \"priors\", \"charge\")\nY <- c(\"two_year_recid\")\ntwo_year <- fairness_cookbook(data, X = X, W = W, Z = Z, Y = Y,\n                              x0 = \"Majority\", x1 = \"Minority\")\n\nautoplot(two_year, decompose = \"xspec\") + \n  ggtitle(TeX(\"$Y$ disparity decomposition COMPAS\"))\n\n\n\n\nCausal decomposition of the TV measure for two-year recidivism.\n\n\n\n\nHowever, we are also interested in the disparity for the predictor \\(\\widehat{Y}\\), so we can decompose the TV measure for the predictor, too:\n\nYhat <- \"northpointe\"\nnorth_decompose <- fairness_cookbook(data, X = X, W = W, Z = Z, Y = Yhat,\n                                     x0 = \"Majority\", x1 = \"Minority\")\nautoplot(north_decompose, decompose = \"xspec\") +\n  ggtitle(TeX(\"$\\\\widehat{Y}$ disparity decomposition COMPAS\"))\n\n\n\n\nCausal decomposition of the TV measure for Northpointe’s predictions.\n\n\n\n\nTo perform the complete analysis, we plot the two results side-by-side, and shade the areas depending on whether the associated measure is included in the business necessity set or not:\n\n\n\n\n\nCausal decompositions of the TV measure for the true and predicted outcomes visualized together."
  },
  {
    "objectID": "pages/compas-t1.html#compas-under-business-necessity",
    "href": "pages/compas-t1.html#compas-under-business-necessity",
    "title": "COMPAS - Task 1",
    "section": "COMPAS under Business Necessity",
    "text": "COMPAS under Business Necessity\nThe courts at Broward County, Florida, were using machine learning to predict whether individuals released on parole are at high risk of re-offending within 2 years. The algorithm is based on the demographic information \\(Z\\) (\\(Z_1\\) for gender, \\(Z_2\\) for age), race \\(X\\) (\\(x_0\\) denoting White, \\(x_1\\) Non-White), juvenile offense counts \\(J\\), prior offense count \\(P\\), and degree of charge \\(D\\).\n\n\n\n\n\nFigure 1: Causal diagram of the COMPAS dataset. Business Necessity variables highlighted in green.\n\n\n\n\nA causal analysis using the fairness_cookbook() revealed that:\n\ndata <- get(data(\"compas\", package = \"faircause\"))\nset.seed(2022)\nmdata <- SFM_proj(\"compas\")\nfc_compas <- fairness_cookbook(data, X = mdata$X, Z = mdata$Z, W = mdata$W,\n                               Y = mdata$Y, x0 = mdata$x0, x1 = mdata$x1)\n\nautoplot(fc_compas, decompose = \"xspec\", dataset = \"COMPAS\")\n\n\n\n\nFigure 2: Fairness Cookbook on the COMPAS dataset.\n\n\n\n\nThat is, the computed measures equal: \\[\n\\begin{align}\n  \\text{Ctf-IE}_{x_1, x_0}(y\\mid x_1) &= -5.7\\% \\pm 0.5\\%,\\\\\n  \\text{Ctf-SE}_{x_1, x_0}(y) &= -4.0\\% \\pm 0.9\\%,\n\\end{align}\n\\] and are also shown graphically in Figure 2, potentially indicating presence of disparate impact. Based on this information, a legal team of ProPublica filed a lawsuit to the district court, claiming discrimination w.r.t. the Non-White subpopulation based on the doctrine of disparate impact. After the court hearing, the judge rules that using the attributes age (\\(Z_2\\)), prior count (\\(P\\)), and charge degree (\\(D\\)) is not discriminatory, but using the attributes juvenile count (\\(J\\)) and gender (\\(Z_1\\)) is discriminatory. The causal diagram with a visualization of which variables are included in the business-necessity set is given in Figure 1. Data scientists at ProPublica need to consider how to proceed in the light of this new requirement for discounting the allowable attributes in the quantiative analysis.\nThe difficulty in this example is that the quantity Ctf-SE\\(_{x_1, x_0}(y)\\) measures the spurious discrimination between the attribute \\(X\\) and outcome \\(Y\\) as generated by both confounders \\(Z_1\\) and \\(Z_2\\). Since using the confounder \\(Z_2\\) is not considered discriminatory, but using the confounder \\(Z_1\\) is, the quantity Ctf-SE\\(_{x_1, x_0}(y)\\) needs to be refined such that the spurious variations based on the different confounders are disentangled. In particular, one might be interested in finding a decomposition of the spurious effect such that \\[\n\\begin{align}\n    \\text{Ctf-SE}_{x_1, x_0}(y) = \\underbrace{\\text{Ctf-SE}^{Z_1}_{x_1, x_0}(y)}_{\\text{gender variations}} + \\underbrace{\\text{Ctf-SE}^{Z_2}_{x_1, x_0}(y)}_{\\text{age variations}},\n\\end{align}\n\\] which would allow the data analyst to further distinguish the variations explained by each of the confounders. A similar challenge is present for the Ctf-IE\\(_{x_1, x_0}(y\\mid x_1)\\) measure, since it has contributions explained by juvenile offense counts \\(J\\), prior counts \\(P\\), and the charge degree \\(D\\). Therefore, we might be interested in decomposing the indirect effect into \\[\n\\begin{align}\n    \\text{Ctf-IE}_{x_1, x_0}(y \\mid x_1) =& \\underbrace{\\text{Ctf-IE}^J_{x_1, x_0}(y \\mid x_1)}_{\\text{juvenile count variations}} + \\underbrace{\\text{Ctf-IE}^P_{x_1, x_0}(y \\mid x_1)}_{\\text{prior count variations}} \\\\ &+ \\underbrace{\\text{Ctf-IE}^D_{x_1, x_0}(y \\mid x_1)}_{\\text{charge degree variations}}. \\nonumber\n\\end{align}\n\\] Again, such a decomposition would allow the data analyst to better understand the contribution of each of the mediators to the totality of the indirect effect. In situations when some mediating variables are in the business necessity set, while others are not, such a decomposition would allow for assessment of disparate impact claims.\n\nTowards the solution"
  },
  {
    "objectID": "pages/census-t1.html",
    "href": "pages/census-t1.html",
    "title": "Census Dataset Analysis",
    "section": "",
    "text": "The United States Census of 2018 collected broad information about the US Government employees, including demographic information \\(Z\\) (\\(Z_1\\) for age, \\(Z_2\\) for race, \\(Z_3\\) for nationality), gender \\(X\\) (\\(x_0\\) female, \\(x_1\\) male), marital and family status \\(M\\), education information \\(L\\), and work-related information \\(R\\).\nA data scientist loads the data and performs the following initial analysis:\n\ndata &lt;- get(data(\"gov_census\", package = \"faircause\"))\ndata &lt;- as.data.frame(data[seq_len(20000)])\nknitr::kable(head(data), caption = \"Census dataset.\")\n\n\nCensus dataset.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsex\nage\nrace\nhispanic_origin\ncitizenship\nnativity\nmarital\nfamily_size\nchildren\neducation_level\nenglish_level\nsalary\nhours_worked\nweeks_worked\noccupation\nindustry\neconomic_region\n\n\n\n\nmale\n64\nblack\nno\n1\nnative\nmarried\n2\n0\n20\n0\n43000\n56\n49\n13-1081\n928P\nSoutheast\n\n\nfemale\n54\nwhite\nno\n1\nnative\nmarried\n3\n1\n20\n0\n45000\n42\n49\n29-2061\n6231\nSoutheast\n\n\nmale\n38\nblack\nno\n1\nnative\nmarried\n3\n1\n24\n0\n99000\n50\n49\n25-1000\n611M1\nSoutheast\n\n\nfemale\n41\nasian\nno\n1\nnative\nmarried\n3\n1\n24\n0\n63000\n50\n49\n25-1000\n611M1\nSoutheast\n\n\nfemale\n40\nwhite\nno\n1\nnative\nmarried\n4\n2\n21\n0\n45200\n40\n49\n27-1010\n611M1\nSoutheast\n\n\nfemale\n46\nwhite\nno\n1\nnative\ndivorced\n3\n1\n18\n0\n28000\n40\n49\n43-6014\n6111\nSoutheast\n\n\n\n\nmean_sal &lt;- tapply(data$salary, data$sex, mean)\ntv &lt;- mean_sal[2] - mean_sal[1]\n\nTherefore, the data scientist observed that male employees on average earn $14000/year more than female employees, that is\n\n\n\\(E[y \\mid x_1] - E[y \\mid x_0] = 15054.\\)\n\n\nFollowing the Fairness Cookbook, the data scientist does the following:\nSFM projection: the SFM projection of the causal diagram \\(\\mathcal{G}\\) of this dataset is given by \\[\n\\Pi_{\\text{SFM}}(\\mathcal{G}) = \\langle X = \\lbrace X \\rbrace,  Z = \\lbrace Z_1, Z_2, Z_3 \\rbrace, W = \\lbrace M, L, R\\rbrace, Y = \\lbrace Y \\rbrace\\rangle.\n\\] She then inputs this SFM projection into the faircause R-package,\n\nset.seed(2022)\nmdata &lt;- SFM_proj(\"census\")\nmdata\n\n$X\n[1] \"sex\"\n\n$W\n[1] \"marital\"         \"family_size\"     \"children\"        \"education_level\"\n[5] \"english_level\"   \"hours_worked\"    \"weeks_worked\"    \"occupation\"     \n[9] \"industry\"       \n\n$Z\n[1] \"age\"             \"race\"            \"hispanic_origin\" \"citizenship\"    \n[5] \"nativity\"        \"economic_region\"\n\n$Y\n[1] \"salary\"\n\n$x0\n[1] \"male\"\n\n$x1\n[1] \"female\"\n\n$ylvl\n[1] NA\n\nfc_census &lt;- fairness_cookbook(\n  as.data.frame(data), X = mdata$X, Z = mdata$Z, W = mdata$W,\n  Y = mdata$Y, x0 = mdata$x0, x1 = mdata$x1\n)\n\nautoplot(fc_census, decompose = \"xspec\", dataset = \"Census\")\n\n\n\n\nFigure 1: Fairness Cookbook on the Census dataset.\n\n\n\n\nUsing these results, she considers the following:\nDisparate treatment: when considering disparate treatment, she computes \\(x\\text{-DE}_{x_0, x_1}(y \\mid x_0)\\) and its 95% confidence interval to be\n\n\n\n\\(x\\text{-DE}_{x_0, x_1}(y \\mid x_0) = -10891\\pm443.\\)\n\n\n\nThe hypothesis \\(H_0^{(x\\text{-DE})}\\) is thus rejected, providing evidence of disparate treatment of females.\nDisparate impact: when considering disparate impact, she notice that Ctf-SE, Ctf-IE and their respective 95% confidence intervals equal:\n\n\n\n\\(\\begin{align}x\\text{-IE}_{x_1, x_0}(y \\mid x_0) &= 5190\\pm342\\\\x\\text{-SE}_{x_1, x_0}(y) &= -1027\\pm435.\\end{align}\\)\n\n\n\nThe data scientist decides that the differences in salary explained by the spurious correlation of gender with age, race, and nationality are not considered discriminatory. Therefore, she tests the hypothesis \\[H_0^{(x\\text{-IE})}: x\\text{-IE}_{x_1, x_0}(y \\mid x_0) = 0,\\] which is rejected, indicating evidence of disparate impact on female employees of the government. The measures computed in this example are visualized in Figure 1."
  },
  {
    "objectID": "pages/python-use.html",
    "href": "pages/python-use.html",
    "title": "Causal Fairness Analysis - from Python",
    "section": "",
    "text": "import pandas as pd\nimport rpy2.robjects as ro\nfrom rpy2.robjects.packages import importr\nfrom rpy2.robjects import pandas2ri\nfrom rpy2.robjects.vectors import StrVector\nfrom rpy2.robjects.conversion import localconverter\ncompas_path = os.path.join(git_root, \"scripts\", \"fair-prediction\",\n                           \"compas-preprocessed.csv\")\n\ncompas = pd.read_csv(compas_path)\ncompas.drop(compas.columns[0], axis=1, inplace=True)\n\nwith localconverter(ro.default_converter + pandas2ri.converter):\n  r_compas = ro.conversion.py2rpy(compas)\n\nfaircause = importr('faircause')\n\nfc_compas = faircause.fairness_cookbook(\n  r_compas, X = \"race\",\n  W = StrVector([\"juv_fel\", \"juv_misd\", \"juv_other\", \"priors\", \"charge\"]),\n  Z = StrVector([\"age\", \"sex\"]),\n  Y = \"two_year_recid\",\n  x0 = 0,\n  x1 = 1\n)\n\nprint(ro.r.summary(fc_compas).rx2(\"measures\"))\n\n\n    WARNING: The R package \"reticulate\" only fixed recently\n    an issue that caused a segfault when used with rpy2:\n    https://github.com/rstudio/reticulate/pull/1188\n    Make sure that you use a version of that package that includes\n    the fix.\n              measure       value          sd\nctfde       ctfde -0.05267448 0.008506597\nctfie       ctfie  0.07872395 0.008864717\nctfse       ctfse  0.03724921 0.009534149\nett           ett -0.13139843 0.006831753\nexpse_x0 expse_x0  0.01684047 0.001207810\nexpse_x1 expse_x1 -0.03331364 0.002440835\nnde           nde -0.03042546 0.011378097\nnie           nie  0.08806808 0.007254499\nte             te -0.11849353 0.011704603\ntv             tv -0.16864764 0.012047903"
  },
  {
    "objectID": "pages/python-use.html#running-code",
    "href": "pages/python-use.html#running-code",
    "title": "Causal Fairness Analysis - from Python",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\ndat <- get(data(\"compas\", package = \"faircause\"))\nknitr::kable(head(dat), caption = \"COMPAS dataset.\")\n\n\nCOMPAS dataset.\n\n\n\n\n\n\n\n\n\n\n\n\n\nsex\nage\nrace\njuv_fel_count\njuv_misd_count\njuv_other_count\npriors_count\nc_charge_degree\ntwo_year_recid\n\n\n\n\nMale\n69\nNon-White\n0\n0\n0\n0\nF\n0\n\n\nMale\n34\nNon-White\n0\n0\n0\n0\nF\n1\n\n\nMale\n24\nNon-White\n0\n0\n1\n4\nF\n1\n\n\nMale\n23\nNon-White\n0\n1\n0\n1\nF\n0\n\n\nMale\n43\nNon-White\n0\n0\n0\n2\nF\n0\n\n\nMale\n44\nNon-White\n0\n0\n0\n0\nM\n0\n\n\n\n\n\n\nimport rpy2\nfrom rpy2.robjects.packages import importr\n# import R's \"base\" package\nfaircause = importr('faircause')\n\nfaircause.fairness_cookbook(\n  dat, X = \"race\", \n  W = [\"juv_fel_count\", \"juv_misd_count\", \"juv_other_count\", \"priors_count\",        \"c_charge_degree\"],\n  Z = [\"age\", \"sex\"],\n  Y = \"two_year_recid\",\n  x0 = \"White\",\n  x1 = \"Non-White\"\n)\n\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "pages/python-use.html#quarto",
    "href": "pages/python-use.html#quarto",
    "title": "Causal Fairness Analysis - from Python",
    "section": "Quarto",
    "text": "Quarto\nQuarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "pages/compas-t1-extended.html",
    "href": "pages/compas-t1-extended.html",
    "title": "COMPAS - Task 1",
    "section": "",
    "text": "In this vignette, we generalize the analysis of the Fairness Cookbook, to consider more refined settings described by an arbitrary causal diagram. The main motivation for doing so comes from the observation that when analyzing disparate impact, quantities such as Ctf-DE\\(_{x_0,x_1}(y\\mid x_0)\\), Ctf-IE\\(_{x_0,x_1}(y\\mid x_0)\\), and Ctf-SE\\(_{x_0,x_1}(y)\\) are insufficient to account for certain business necessity requirements. For concreteness, consider the following example."
  },
  {
    "objectID": "pages/compas-t1-extended.html#compas-under-business-necessity",
    "href": "pages/compas-t1-extended.html#compas-under-business-necessity",
    "title": "COMPAS - Task 1",
    "section": "COMPAS under Business Necessity",
    "text": "COMPAS under Business Necessity\nThe courts at Broward County, Florida, were using machine learning to predict whether individuals released on parole are at high risk of re-offending within 2 years. The algorithm is based on the demographic information \\(Z\\) (\\(Z_1\\) for gender, \\(Z_2\\) for age), race \\(X\\) (\\(x_0\\) denoting White, \\(x_1\\) Non-White), juvenile offense counts \\(J\\), prior offense count \\(P\\), and degree of charge \\(D\\).\n\n\n\n\n\nFigure 1: Causal diagram of the COMPAS dataset. Business Necessity variables highlighted in green.\n\n\n\n\nA causal analysis using the fairness_cookbook() revealed that:\n\ndata <- get(data(\"compas\", package = \"faircause\"))\nset.seed(2022)\nmdata <- SFM_proj(\"compas\")\nfc_compas <- fairness_cookbook(data, X = mdata$X, Z = mdata$Z, W = mdata$W,\n                               Y = mdata$Y, x0 = mdata$x0, x1 = mdata$x1)\n\nautoplot(fc_compas, decompose = \"xspec\", dataset = \"COMPAS\")\n\n\n\n\nFigure 2: Fairness Cookbook on the COMPAS dataset.\n\n\n\n\nThat is, the computed measures equal: \\[\n\\begin{align}\n  \\text{Ctf-IE}_{x_1, x_0}(y\\mid x_1) &= -5.7\\% \\pm 0.5\\%,\\\\\n  \\text{Ctf-SE}_{x_1, x_0}(y) &= -4.0\\% \\pm 0.9\\%,\n\\end{align}\n\\] and are also shown graphically in Figure 2, potentially indicating presence of disparate impact. Based on this information, a legal team of ProPublica filed a lawsuit to the district court, claiming discrimination w.r.t. the Non-White subpopulation based on the doctrine of disparate impact. After the court hearing, the judge rules that using the attributes age (\\(Z_2\\)), prior count (\\(P\\)), and charge degree (\\(D\\)) is not discriminatory, but using the attributes juvenile count (\\(J\\)) and gender (\\(Z_1\\)) is discriminatory. The causal diagram with a visualization of which variables are included in the business-necessity set is given in Figure 1. Data scientists at ProPublica need to consider how to proceed in the light of this new requirement for discounting the allowable attributes in the quantiative analysis.\nThe difficulty in this example is that the quantity Ctf-SE\\(_{x_1, x_0}(y)\\) measures the spurious discrimination between the attribute \\(X\\) and outcome \\(Y\\) as generated by both confounders \\(Z_1\\) and \\(Z_2\\). Since using the confounder \\(Z_2\\) is not considered discriminatory, but using the confounder \\(Z_1\\) is, the quantity Ctf-SE\\(_{x_1, x_0}(y)\\) needs to be refined such that the spurious variations based on the different confounders are disentangled. In particular, one might be interested in finding a decomposition of the spurious effect such that \\[\n\\begin{align}\n    \\text{Ctf-SE}_{x_1, x_0}(y) = \\underbrace{\\text{Ctf-SE}^{Z_1}_{x_1, x_0}(y)}_{\\text{gender variations}} + \\underbrace{\\text{Ctf-SE}^{Z_2}_{x_1, x_0}(y)}_{\\text{age variations}},\n\\end{align}\n\\] which would allow the data analyst to further distinguish the variations explained by each of the confounders. A similar challenge is present for the Ctf-IE\\(_{x_1, x_0}(y\\mid x_1)\\) measure, since it has contributions explained by juvenile offense counts \\(J\\), prior counts \\(P\\), and the charge degree \\(D\\). Therefore, we might be interested in decomposing the indirect effect into \\[\n\\begin{align}\n    \\text{Ctf-IE}_{x_1, x_0}(y \\mid x_1) =& \\underbrace{\\text{Ctf-IE}^J_{x_1, x_0}(y \\mid x_1)}_{\\text{juvenile count variations}} + \\underbrace{\\text{Ctf-IE}^P_{x_1, x_0}(y \\mid x_1)}_{\\text{prior count variations}} \\\\ &+ \\underbrace{\\text{Ctf-IE}^D_{x_1, x_0}(y \\mid x_1)}_{\\text{charge degree variations}}. \\nonumber\n\\end{align}\n\\] Again, such a decomposition would allow the data analyst to better understand the contribution of each of the mediators to the totality of the indirect effect. In situations when some mediating variables are in the business necessity set, while others are not, such a decomposition would allow for assessment of disparate impact claims.\n\nTowards the solution"
  },
  {
    "objectID": "pages/outcome-control.html",
    "href": "pages/outcome-control.html",
    "title": "Outcome Control – Cancer Surgery Example",
    "section": "",
    "text": "Consider the structural causal model (SCM) introduced in Ex. 5.10 of the paper (Plecko and Bareinboim 2022): \\[\\begin{align}\nX  &\\gets U_X \\label{eq:cancer-scm-1} \\\\\n                    W &\\gets \\begin{cases} \\sqrt{U_W} \\text{ if } X = x_0, \\\\\n                    1 - \\sqrt{1 - U_W} \\text{ if } X = x_1\n                    \\end{cases}  \\\\\n                D & \\gets f_D(X, W) \\\\\n                Y  &\\gets \\mathbb{1}(U_Y + \\frac{1}{3} WD - \\frac{1}{5} W &gt; 0.5). \\\\\n                    U_X &\\in \\{0,1\\}, P(U_X = 1) = 0.5, \\\\\n                    %U_Z&,\n                        U_W&, U_Y \\sim \\text{Unif}[0, 1], \\label{eq:cancer-scm-n}\n\\end{align}\\] We begin by generating \\(n = 5000\\) samples from the SCM:\n\nset.seed(2023)\nn &lt;- 5000\n\n# generate data from the SCM\nx &lt;- rbinom(n, 1, 0.5)\nuw &lt;- runif(n)\nw &lt;- ifelse(x, 1 - sqrt(1-uw), sqrt(uw)) \nuy &lt;- runif(n)\n\n# f_Y coefficients\nalph &lt;- 1 / 5\nbeta &lt;- 1 / 3\n\n# assuming a policy D s.t. P(d | w) = w.\nd &lt;- rbinom(n, 1, prob = w)\ny &lt;- as.integer(uy + beta * w *d - alph * w &gt; 0.5)\n\nlabels &lt;- c(\"Doomed\", \"Helped\", \"Safe\")\n\ndelta &lt;- beta * w\n\n# determine canonical type\ncanon &lt;- ifelse(uy &gt; 0.5 + w * alph, \"Safe\",\n                ifelse(uy &gt; 0.5 + w * alph - w * beta, \"Helped\", \"Doomed\"))\n\nThe latent variable \\(U_Y\\) determines which canonical type the individual belongs to. After generating the data, we visualize it from the oracle’s perspective, assuming access to \\(U_Y\\):\n\n\n\n\n\n\n\n\n\nFrom the oracle’s perspective, treating individuals in the green area is optimal. However, we next look at the perspective of the decision-maker:"
  },
  {
    "objectID": "pages/outcome-control.html#oracles-perspective",
    "href": "pages/outcome-control.html#oracles-perspective",
    "title": "Outcome Control – Cancer Surgery Example",
    "section": "",
    "text": "Consider the structural causal model (SCM) introduced in Ex. 5.10 of the paper (Plecko and Bareinboim 2022): \\[\\begin{align}\nX  &\\gets U_X \\label{eq:cancer-scm-1} \\\\\n                    W &\\gets \\begin{cases} \\sqrt{U_W} \\text{ if } X = x_0, \\\\\n                    1 - \\sqrt{1 - U_W} \\text{ if } X = x_1\n                    \\end{cases}  \\\\\n                D & \\gets f_D(X, W) \\\\\n                Y  &\\gets \\mathbb{1}(U_Y + \\frac{1}{3} WD - \\frac{1}{5} W &gt; 0.5). \\\\\n                    U_X &\\in \\{0,1\\}, P(U_X = 1) = 0.5, \\\\\n                    %U_Z&,\n                        U_W&, U_Y \\sim \\text{Unif}[0, 1], \\label{eq:cancer-scm-n}\n\\end{align}\\] We begin by generating \\(n = 5000\\) samples from the SCM:\n\nset.seed(2023)\nn &lt;- 5000\n\n# generate data from the SCM\nx &lt;- rbinom(n, 1, 0.5)\nuw &lt;- runif(n)\nw &lt;- ifelse(x, 1 - sqrt(1-uw), sqrt(uw)) \nuy &lt;- runif(n)\n\n# f_Y coefficients\nalph &lt;- 1 / 5\nbeta &lt;- 1 / 3\n\n# assuming a policy D s.t. P(d | w) = w.\nd &lt;- rbinom(n, 1, prob = w)\ny &lt;- as.integer(uy + beta * w *d - alph * w &gt; 0.5)\n\nlabels &lt;- c(\"Doomed\", \"Helped\", \"Safe\")\n\ndelta &lt;- beta * w\n\n# determine canonical type\ncanon &lt;- ifelse(uy &gt; 0.5 + w * alph, \"Safe\",\n                ifelse(uy &gt; 0.5 + w * alph - w * beta, \"Helped\", \"Doomed\"))\n\nThe latent variable \\(U_Y\\) determines which canonical type the individual belongs to. After generating the data, we visualize it from the oracle’s perspective, assuming access to \\(U_Y\\):\n\n\n\n\n\n\n\n\n\nFrom the oracle’s perspective, treating individuals in the green area is optimal. However, we next look at the perspective of the decision-maker:"
  },
  {
    "objectID": "pages/outcome-control.html#decision-makers-perspective",
    "href": "pages/outcome-control.html#decision-makers-perspective",
    "title": "Outcome Control – Cancer Surgery Example",
    "section": "Decision-maker’s Perspective",
    "text": "Decision-maker’s Perspective\nWe next plot the decision-maker’s perspective, which is based on estimating the benefit \\(E[Y_{d_1} - Y_{d_0} \\mid x, z, w]\\). We first estimate the benefit from the data.\n\ndf &lt;- data.frame(x, w, d, y)\n\n# fit a logistic regression model\nlogreg &lt;- glm(y ~ x + w * d, data = df, family = \"binomial\")\n\n\n# compute the potential outcomes\ndf_d0 &lt;- df_d1 &lt;- df\ndf_d0$d &lt;- 0\ndf_d1$d &lt;- 1\npy_d0 &lt;- predict(logreg, df_d0, type = \"response\")\npy_d1 &lt;- predict(logreg, df_d1, type = \"response\")\n\n# compute the benefit\ndf$delta_hat &lt;- delta_hat &lt;- py_d1 - py_d0\n\nAfter estimating \\(\\Delta\\), we visualize the data based on the obtained estimates:\n\n\n\n\n\n\n\n\n\nTherefore, when looking at the benefit, it is clearly higher for the \\(X = x_0\\) group. This explains why the decision-maker may naturally decide to treat more individuals in the \\(X = x_0\\) group."
  },
  {
    "objectID": "pages/outcome-control.html#computing-the-disparity-in-treatment-allocation",
    "href": "pages/outcome-control.html#computing-the-disparity-in-treatment-allocation",
    "title": "Outcome Control – Cancer Surgery Example",
    "section": "Computing the Disparity in Treatment Allocation",
    "text": "Computing the Disparity in Treatment Allocation\nWe look at the policy \\(D^*\\) obtained from Algorithm 5.3: \\[\\begin{align}\n  D^* = \\mathbb{1}(\\Delta &gt; \\frac{1}{6}).\n\\end{align}\\]\n\n# construct the decision policy \nd_star &lt;- as.integer(delta_hat &gt; quantile(delta_hat, 0.5))\n# look at resource allocation in each group\ntapply(d_star, x, mean)\n\n        0         1 \n0.7513683 0.2366912 \n\n\nTherefore, we obtained that \\[\\begin{align}\n  P(d^* \\mid x_1) - P(d^* \\mid x_0) \\approx -51.5\\%.\n\\end{align}\\] The true population value (as shown in the paper) is -50%."
  },
  {
    "objectID": "pages/outcome-control.html#decomposing-the-disparity",
    "href": "pages/outcome-control.html#decomposing-the-disparity",
    "title": "Outcome Control – Cancer Surgery Example",
    "section": "Decomposing the Disparity",
    "text": "Decomposing the Disparity\nWe next look at decomposing the disparity using Algorithm 5.4. We first use the faircause package to decompose the disparity in resource allocation:\n\n# implement the new policy\ndf$d &lt;- d_star\n\n# apply the fairness cookbook\nfc_d &lt;- fairness_cookbook(df, X = \"x\", Z = NULL, W = \"w\", Y = \"d\",\n                          x0 = 0, x1 = 1, nboot1 = 5, model = \"linear\")\n\nautoplot(fc_d, signed = FALSE) + ggtitle(TeX(\"$P(d^* | x_1) - P(d^* | x_0)$ decomposition\")) +\n  scale_x_discrete(labels = c(\"TV\", \"DE\", \"IE\", \"SE\"))\n\n\n\n\n\n\n\n\nTherefore, as explained in the paper, the disparity is driven by the indirect effect. We can also decompose the disparity in the estimated benefit \\(E[\\Delta \\mid x_1] - E[\\Delta \\mid x_0]\\):\n\n# apply the fairness cookbook\nfc_delta &lt;- fairness_cookbook(df, X = \"x\", Z = NULL, W = \"w\", Y = \"delta_hat\",\n                              x0 = 0, x1 = 1, nboot1 = 5, model = \"linear\")\n\nautoplot(fc_delta, signed = FALSE) +\n  ggtitle(TeX(\"$E[\\\\Delta | x_1] - E[\\\\Delta | x_0]$ decomposition\")) + \n  scale_x_discrete(labels = c(\"TV\", \"DE\", \"IE\", \"SE\"))"
  },
  {
    "objectID": "pages/outcome-control.html#constructing-the-causally-fair-dcf-policy",
    "href": "pages/outcome-control.html#constructing-the-causally-fair-dcf-policy",
    "title": "Outcome Control – Cancer Surgery Example",
    "section": "Constructing the causally fair \\(D^{CF}\\) policy",
    "text": "Constructing the causally fair \\(D^{CF}\\) policy\nThe clinicians first want to construct the causally fair policy. For doing so, they first construct the counterfactual values of the illness severity. In particular, they assume that the relative order of the values remains the same under the counterfactual change of the protected attribute.\n\n# get order statistics for the X = x_1 group\nord_x1 &lt;- order(w[x == 1])\n\n# compute the sample quantile\nquant_x1 &lt;- ord_x1 / length(ord_x1)\n\n# initialize the counterfactual values\nw_cf &lt;- rep(0, n)\n\n# for X = x_0, values remain the same by consistency axiom\nw_cf[x == 0] &lt;- w[x == 0]\n\n# for X = x_1, match to the corresponding quantile in the X = x_0 distribution\nw_cf[x == 1] &lt;- vapply(quant_x1, function(q) quantile(w[x == 0], q), numeric(1L))\n\n# compute the counterfactual benefit\ndelta_cf &lt;- w_cf / 3\n\nb &lt;- mean(d_star)\nfcf &lt;- ecdf(delta_cf)\nfcf.inv &lt;- inverse(fcf, lower = 0, upper = 1)\nd_cf &lt;- as.integer(delta_cf &gt; fcf.inv(1 - b))\ntapply(d_cf, x, mean)\n\n  0   1 \n0.5 0.5 \n\n\nTherefore, we see that \\[\\begin{align}\n  P(d^{CF} \\mid x_1) - P(d^{CF} \\mid x_0) \\approx 0\\%.\n\\end{align}\\]\nThe true population value of the disparity for \\(D^{CF}\\), discussed in the paper, also equals \\(0\\%\\)."
  },
  {
    "objectID": "pages/outcome-control.html#constructing-the-causally-fair-dut-policy",
    "href": "pages/outcome-control.html#constructing-the-causally-fair-dut-policy",
    "title": "Outcome Control – Cancer Surgery Example",
    "section": "Constructing the causally fair \\(D^{UT}\\) policy",
    "text": "Constructing the causally fair \\(D^{UT}\\) policy\nFinally, to perform the utilitarian approach, we have from the disparity in \\(D^{CF}\\) that: \\[\\begin{align}\n  M = 0.\n\\end{align}\\] Therefore, following Algorithm 5.5 with method = UT, we compute:\n\n# compute \\epsilon, l\nepsilon &lt;- mean(d_star[x == 1]) - mean(d_star[x == 0]) - 0 # -0 since M = 0!\nl &lt;- sum(x == 1) / sum(x == 0)\n\n# compute inverse function of \\Delta \\mid x_0\nf0 &lt;- ecdf(delta_hat[x == 0])\nf0.inv &lt;- inverse(f0, lower = 0, upper = 1)\n\n# compute \\delta^{x_0}\ndelta_x0 &lt;- f0.inv(1 - (mean(delta_hat[x == 0] &gt; quantile(delta_hat, 0.5)) + \n                          epsilon * l / (1 + l)))\n\n# compute inverse function of \\Delta \\mid x_0\nf1 &lt;- ecdf(delta_hat[x == 1])\nf1.inv &lt;- inverse(f1, lower = 0, upper = 1)\n\n# get the budget\nb &lt;- mean(d_star)\n\n# compute \\delta^{x_1}\ndelta_x1 &lt;-  f1.inv(1 - (b / mean(x == 1) - 1 / l * mean(delta_hat[x == 0] &gt;= delta_x0)))\n\nd_ut &lt;- rep(0, length(d))\nd_ut[x == 0] &lt;- delta_hat[x == 0] &gt; delta_x0\nd_ut[x == 1] &lt;- delta_hat[x == 1] &gt; delta_x1\n\n# compute the allocated proportions\ntapply(d_ut, x, mean)\n\n        0         1 \n0.4996091 0.5004095 \n\n\nIn particular, we have showed that \\[\\begin{align}\n  P(d^{UT} \\mid x_1) - P(d^{UT} \\mid x_0) \\approx 0\\%.\n\\end{align}\\] The true population value also equals \\(0\\%\\).\nIn general, the policies \\(D^{CF}\\) and \\(D^{UT}\\) may pick different individuals for treatment. The conditions under which the policies \\(D^{CF}\\) and \\(D^{UT}\\) are the same are discussed in Appendix D of the paper. In the above example, these conditions are satisfied, and \\(D^{CF}\\), \\(D^{UT}\\) select the same group of individuals."
  },
  {
    "objectID": "pages/ci-tests.html#standard-fairness-model",
    "href": "pages/ci-tests.html#standard-fairness-model",
    "title": "Conditional Independence Tests - Census & COMPAS",
    "section": "",
    "text": "The framework of Causal Fairness Analysis (Plecko and Bareinboim 2022) introduces the Standard Fairness Model (SFM), which is a type of causal diagram with slightly fewer assumptions than the typically used diagrams in the literature (in particular, SFM is a type of clustered diagram (see Anand et al. 2021)). In particular, the SFM can be seen in Figure 1.\n\n\n\n \n\n\n\n\n\nFigure 1: Standard Fairness Model\n\n\n\n\n \n\n\n\nWe in particular focus on the bidirected arrow between nodes \\(X\\) and \\(Z\\). Such an arrow in particular allows for the possibility that there are variations between \\(X\\) and \\(Z\\) that can be left unexplained in the model, or that unmeasured confounders may exist, or that we are dealing with selection bias. Practically speaking, assuming that no bidirected arrows exist is a strong assumption that does not hold in many settings. For instance, consider the widely recognized phenomenon in the fairness literature known as redlining (Zenou and Boccard 2000; Hernandez 2009). In some practical settings, the location where loan applicants live may correlate with their race. Applications might be rejected based on the zip code, disproportionately affecting certain minority groups in the real world.\nIt has been reported in the literature that correlation between gender and location, or religious and location may possibly exist, and therefore, should be acknowledged through modeling. For instance, the one-child policy affecting mainly urban areas in China had visible effects in terms of shifting the gender ratio towards males (Hesketh, Lu, and Xing 2005; Ding and Hesketh 2006). Beyond race or gender, religious segregation is also a recognized phenomenon in some urban areas (Brimicombe 2007). Again, while we make no claim that location affects race (or religion), or vice-versa, the bidirected arrows give a degree of modeling flexibility that allows for the encoding of such co-variations. Still, this is without making any commitment to whatever historical processes and other complex dynamics that took place and generated such imbalance in the first place. To corroborate this point, consider the following practical investigation.\nA data scientist is trying to understand the correlation between the features in the COMPAS dataset. The protected attribute \\(X\\) is race, and the demographic variables \\(Z_1\\), \\(Z_2\\) are age and sex. The data scientist tests two hypotheses, namely: \\[\\begin{align}\n    H^{(1)}_0: X \\perp\\!\\!\\!\\perp Z_1,\\\\\n    H^{(2)}_0: X \\perp\\!\\!\\!\\perp Z_2.\n\\end{align}\\] To do so, the data scientist first loads the COMPAS dataset from the faircause package:\n\ndata &lt;- get(data(\"compas\", package = \"faircause\"))\nknitr::kable(head(data), caption = \"COMPAS dataset.\")\n\n\nCOMPAS dataset.\n\n\n\n\n\n\n\n\n\n\n\n\n\nsex\nage\nrace\njuv_fel_count\njuv_misd_count\njuv_other_count\npriors_count\nc_charge_degree\ntwo_year_recid\n\n\n\n\nMale\n69\nNon-White\n0\n0\n0\n0\nF\n0\n\n\nMale\n34\nNon-White\n0\n0\n0\n0\nF\n1\n\n\nMale\n24\nNon-White\n0\n0\n1\n4\nF\n1\n\n\nMale\n23\nNon-White\n0\n1\n0\n1\nF\n0\n\n\nMale\n43\nNon-White\n0\n0\n0\n2\nF\n0\n\n\nMale\n44\nNon-White\n0\n0\n0\n0\nM\n0\n\n\n\n\n\nAfter loading the data, the data scientist visualizes the correlations of the variables:\n\nc1 &lt;- ggplot(data, aes(x = age, fill = race)) +\n  geom_density(alpha = 0.4) + theme_bw() +\n  theme(\n    legend.position = c(0.8, 0.8),\n    legend.box.background = element_rect()\n  ) + ggtitle(TeX(\"COMPAS: age $\\\\perp$ race rejected (p &lt; 0.001)\"))\n\nc2 &lt;- ggplot(data, aes(x = race, fill = sex)) +\n  geom_bar(position = \"fill\") +\n  geom_text(\n    aes(label = percent(round(after_stat(count)/tapply(after_stat(count), after_stat(x), sum)[after_stat(x)], 4))),\n            stat = \"count\", position = position_fill(0.5)\n  ) +\n  scale_y_continuous(labels = percent) + ylab(\"proportion\") +\n  theme_minimal() + ggtitle(TeX(\"COMPAS: race $\\\\perp$ sex rejected (p &lt; 0.001)\"))\n\ncowplot::plot_grid(c1, c2, ncol = 2L)\n\n\n\n\nFigure 2: Association of X and Z sets on the COMPAS dataset.\n\n\n\n\nFigure 2 shows the association of \\(X\\) and \\(Z_1\\), \\(Z_2\\). For testing the independence hypothesis \\(H_0\\), the data scientist runs:\n\nwilcox.test(data[data$race == \"White\", ]$age, data[data$race == \"Non-White\", ]$age)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  data[data$race == \"White\", ]$age and data[data$race == \"Non-White\", ]$age\nW = 7027020, p-value &lt; 2.2e-16\nalternative hypothesis: true location shift is not equal to 0\n\n\nIn fact, both of the hypotheses are rejected (\\(p\\)-values \\(&lt; 0.001\\)). However, possible confounders of this relationship are not measured in the corresponding dataset.\nSimilarly, the same data scientist is now trying to understand the correlation of the features in the Government Census dataset (we skip the code to avoid repetition). The protected attribute \\(X\\) is gender, and the demographic variables \\(Z_1\\), \\(Z_2\\) are age and race. The data scientist tests the independence of sex and age (\\(X \\perp\\!\\!\\!\\perp Z_1\\)), and sex and race (\\(X \\perp\\!\\!\\!\\perp Z_2\\)), and both hypotheses are rejected (p-values \\(&lt; 0.001\\)). Figure 3 shows the associations of the features. Again, possible confounders of this relationship are not measured in the corresponding dataset, meaning that the attribute \\(X\\) cannot be separated from the confounders \\(Z_1, Z_2\\) using any of the observed variables.\n\ndata &lt;- get(data(\"gov_census\", package = \"faircause\"))\ndata &lt;- as.data.frame(data[seq_len(20000), ])\n\nlevels(data$race) &lt;- c(\"Minority\", \"Minority\", \"Minority\", \"Minority\", \n                       \"Minority\", \"Minority\", \"Majority\")\n\nwilcox.test(data[data$sex == \"female\", ]$age, data[data$sex == \"male\", ]$age)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  data[data$sex == \"female\", ]$age and data[data$sex == \"male\", ]$age\nW = 54018487, p-value &lt; 2.2e-16\nalternative hypothesis: true location shift is not equal to 0\n\na1 &lt;- ggplot(data, aes(x = age, fill = sex)) +\n  geom_density(alpha = 0.4) + theme_bw() +\n  theme(\n    legend.position = c(0.2, 0.8),\n    legend.box.background = element_rect()\n  ) + ggtitle(TeX(\"Adult: age $\\\\perp$ sex rejected (p &lt; 0.001)\"))\n\na2 &lt;- ggplot(data, aes(x = sex, fill = race)) +\n  geom_bar(position = \"fill\") +\n  geom_text(aes(label = percent(round(after_stat(count)/tapply(after_stat(count), after_stat(x), sum)[after_stat(x)], 4))),\n            stat = \"count\", position = position_fill(0.5)) +\n  scale_y_continuous(labels = percent) + ylab(\"proportion\") +\n  theme_minimal() + ggtitle(TeX(\"Census: race $\\\\perp$ sex rejected (p &lt; 0.001)\"))\n\ncowplot::plot_grid(a1, a2, ncol = 2L)\n\n\n\n\nFigure 3: Association of X and Z sets on the UCI Adult dataset."
  },
  {
    "objectID": "pages/t1-census.html",
    "href": "pages/t1-census.html",
    "title": "Census Dataset Analysis",
    "section": "",
    "text": "The United States Census of 2018 collected broad information about the US Government employees, including demographic information \\(Z\\) (\\(Z_1\\) for age, \\(Z_2\\) for race, \\(Z_3\\) for nationality), gender \\(X\\) (\\(x_0\\) female, \\(x_1\\) male), marital and family status \\(M\\), education information \\(L\\), and work-related information \\(R\\).\nA data scientist loads the data and performs the following initial analysis:\n\ndata &lt;- get(data(\"gov_census\", package = \"faircause\"))\ndata &lt;- as.data.frame(data[seq_len(20000), ])\nknitr::kable(head(data), caption = \"Census dataset.\")\n\n\nCensus dataset.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsex\nage\nrace\nhispanic_origin\ncitizenship\nnativity\nmarital\nfamily_size\nchildren\neducation_level\nenglish_level\nsalary\nhours_worked\nweeks_worked\noccupation\nindustry\neconomic_region\n\n\n\n\nmale\n64\nblack\nno\n1\nnative\nmarried\n2\n0\n20\n0\n43000\n56\n49\n13-1081\n928P\nSoutheast\n\n\nfemale\n54\nwhite\nno\n1\nnative\nmarried\n3\n1\n20\n0\n45000\n42\n49\n29-2061\n6231\nSoutheast\n\n\nmale\n38\nblack\nno\n1\nnative\nmarried\n3\n1\n24\n0\n99000\n50\n49\n25-1000\n611M1\nSoutheast\n\n\nfemale\n41\nasian\nno\n1\nnative\nmarried\n3\n1\n24\n0\n63000\n50\n49\n25-1000\n611M1\nSoutheast\n\n\nfemale\n40\nwhite\nno\n1\nnative\nmarried\n4\n2\n21\n0\n45200\n40\n49\n27-1010\n611M1\nSoutheast\n\n\nfemale\n46\nwhite\nno\n1\nnative\ndivorced\n3\n1\n18\n0\n28000\n40\n49\n43-6014\n6111\nSoutheast\n\n\n\n\nmean_sal &lt;- tapply(data$salary, data$sex, mean)\ntv &lt;- mean_sal[2] - mean_sal[1]\n\nTherefore, the data scientist observed that male employees on average earn $14000/year more than female employees, that is\n\n\n\\(E[y \\mid x_1] - E[y \\mid x_0] = 15054.\\)\n\n\nFollowing the Fairness Cookbook, the data scientist does the following:\nSFM projection: the SFM projection of the causal diagram \\(\\mathcal{G}\\) of this dataset is given by \\[\n\\Pi_{\\text{SFM}}(\\mathcal{G}) = \\langle X = \\lbrace X \\rbrace,  Z = \\lbrace Z_1, Z_2, Z_3 \\rbrace, W = \\lbrace M, L, R\\rbrace, Y = \\lbrace Y \\rbrace\\rangle.\n\\] She then inputs this SFM projection into the faircause R-package,\n\nset.seed(2022)\nmdata &lt;- SFM_proj(\"census\")\nmdata\n\n$X\n[1] \"sex\"\n\n$W\n[1] \"marital\"         \"family_size\"     \"children\"        \"education_level\"\n[5] \"english_level\"   \"hours_worked\"    \"weeks_worked\"    \"occupation\"     \n[9] \"industry\"       \n\n$Z\n[1] \"age\"             \"race\"            \"hispanic_origin\" \"citizenship\"    \n[5] \"nativity\"        \"economic_region\"\n\n$Y\n[1] \"salary\"\n\n$x0\n[1] \"male\"\n\n$x1\n[1] \"female\"\n\n$ylvl\n[1] NA\n\nfc_census &lt;- fairness_cookbook(\n  as.data.frame(data), X = mdata$X, Z = mdata$Z, W = mdata$W,\n  Y = mdata$Y, x0 = mdata$x0, x1 = mdata$x1\n)\n\nautoplot(fc_census, decompose = \"xspec\", dataset = \"Census\")\n\n\n\n\nFigure 1: Fairness Cookbook on the Census dataset.\n\n\n\n\nUsing these results, she considers the following:\nDisparate treatment: when considering disparate treatment, she computes \\(x\\text{-DE}_{x_0, x_1}(y \\mid x_0)\\) and its 95% confidence interval to be\n\n\n\n\\(x\\text{-DE}_{x_0, x_1}(y \\mid x_0) = -10891\\pm443.\\)\n\n\n\nThe hypothesis \\(H_0^{(x\\text{-DE})}\\) is thus rejected, providing evidence of disparate treatment of females.\nDisparate impact: when considering disparate impact, she notice that Ctf-SE, Ctf-IE and their respective 95% confidence intervals equal:\n\n\n\n\\(\\begin{align}x\\text{-IE}_{x_1, x_0}(y \\mid x_0) &= 5190\\pm342\\\\x\\text{-SE}_{x_1, x_0}(y) &= -1027\\pm435.\\end{align}\\)\n\n\n\nThe data scientist decides that the differences in salary explained by the spurious correlation of gender with age, race, and nationality are not considered discriminatory. Therefore, she tests the hypothesis \\[H_0^{(x\\text{-IE})}: x\\text{-IE}_{x_1, x_0}(y \\mid x_0) = 0,\\] which is rejected, indicating evidence of disparate impact on female employees of the government. The measures computed in this example are visualized in Figure 1."
  },
  {
    "objectID": "pages/t1-admissions-over-time.html",
    "href": "pages/t1-admissions-over-time.html",
    "title": "College Admissions: Task 1 over Time",
    "section": "",
    "text": "In this vignette, we analyze a synthetic College Admission dataset, to demonstrate how the task of bias detection could be performed over time, in a university. In particular, we have the following story.\nA university in the United States admits applicants every year. The university is interested in quantifying discrimination in the admission process and track it over time, between 2010 and 2020. The data generating process changes over time, and can be described as follows. Let \\(X\\) denote gender (\\(x_0\\) female, \\(x_1\\) male). Let \\(Z\\) be the age at time of application (\\(Z = 0\\) under 20 years, \\(Z = 1\\) over 20 years) and let \\(W\\) denote the department of application (\\(W = 0\\) for arts&humanities, \\(W = 1\\) for sciences). Finally, let \\(Y\\) denote the admission decision (\\(Y = 0\\) rejection, \\(Y = 1\\) acceptance). The application process changes over time and is given by \\[\n\\begin{empheq}[left ={\\mathcal{F}(t), P(U): \\empheqlbrace}]{align}\n                X & \\gets \\mathbb{1} ( U_X &lt; 0.5 + 0.1U_{XZ})\\label{eq:fcb2-x}\\\\\n                Z & \\gets \\mathbb{1} (U_Z &lt; 0.5 + \\kappa(t) U_{XZ})  \\\\\n                W & \\gets \\mathbb{1} (U_W &lt; 0.5 + \\lambda(t) U_{XZ})  \\\\\n                Y & \\gets \\mathbb{1} (U_Y &lt; 0.1 + \\alpha(t)X + \\beta(t)W + 0.1Z). \\label{eq:fcb2-y} \\\\\n                \\nonumber\\\\\n                    U_{XZ} &\\in \\{0,1\\}, P(U_{XZ} = 1) = 0.5, \\\\\n                    U_X &, U_Z, U_W, U_Y \\sim \\text{Unif}[0, 1].       \n\\end{empheq}\n\\] The coefficients \\(\\kappa(t), \\lambda(t), \\alpha(t), \\beta(t)\\) change every year, and obey the following dynamics: \\[\n\\begin{align}\n    \\kappa(t+1) &= 0.9\\kappa(t) \\\\\n    \\lambda(t+1) &= \\lambda(t) (1 - \\beta(t)) \\\\\n    \\beta(t+1) &= \\beta(t) (1 - \\lambda(t)) f(t), f(t) \\sim \\text{Unif}[0.8, 1.2] \\\\\n    \\alpha(t+1) &= 0.8\\alpha(t).\n\\end{align}\n\\] The equations can be interpreted as follows. The coefficient \\(\\kappa(t)\\) decreases over time, meaning that the overall age gap between the groups decreases. The coefficient \\(\\lambda(t)\\) decreases compared to the previous year, by an amount dependent on \\(\\beta(t)\\). In words, the rate of application to arts&humanities departments decreases if these departments have lower overall admission rates (i.e., students are less likely to apply to departments that are hard to get into). Further, \\(\\alpha(t)\\), which represents gender bias, decreases over time. Finally, \\(\\beta(t)\\) represent the increase in the probability of admission when applying to a science department. Its value depends on the value from the previous year, multiplied by \\((1 - \\lambda(t))\\) and the random variable \\(f(t)\\). Multiplication by the former factor describes the mechanism in which the benefit of applying to a science department decreases if a larger proportion of students apply for it. The latter factor describes a random variation over time which describes how well (in relative terms) the science departments are funded, and can be seen as depending on research and market dynamics in the sciences.\nWe now create functions for generating synethetic data as described above:\n\ncol_adm &lt;- function(n, cfs, kap = cfs[[\"kap\"]], lam = cfs[[\"lam\"]],\n                    alf = cfs[[\"alf\"]], bet = cfs[[\"bet\"]]) {\n\n  u_xz &lt;- rbinom(n, size = 1, prob = 0.5)\n  x &lt;- rbinom(n, size = 1, prob = 0.5 + 0.1 * u_xz)\n  z &lt;- rbinom(n, size = 1, prob = 0.5 + kap * u_xz)\n  d &lt;- rbinom(n, size = 1, prob = 0.5 + lam * x)\n  y &lt;- rbinom(n, size = 1, prob = 0.1 + alf * x + bet * d + 0.1 * z)\n\n  data.frame(x, z, d, y)\n\n}\n\ncfs_nxt &lt;- function(cfs, kap = cfs[[\"kap\"]], lam = cfs[[\"lam\"]],\n                    alf = cfs[[\"alf\"]], bet = cfs[[\"bet\"]]) {\n\n  kapt &lt;- kap * 0.9\n  lamt &lt;- lam * (1 - bet)\n  bett &lt;- bet * (1 - lam) * runif(1, 0.8, 1.2)\n  alft &lt;- alf * 0.8\n\n  list(kap = kapt, lam = lamt, alf = alft, bet = bett)\n\n}\n\nThe head data scientist at the university decides to use the Fairness Cookbook for performing bias quantification. The SFM projection of the causal diagram \\(\\mathcal{G}\\) of the dataset is given by \\[\n\\begin{equation}\n        \\Pi_{\\text{SFM}}(\\mathcal{G}) = \\langle X = \\lbrace X \\rbrace,  Z = \\lbrace Z \\rbrace, W = \\lbrace W\\rbrace, Y = \\lbrace Y \\rbrace\\rangle.\n    \\end{equation}\n\\] After that, the analyst estimates the quantities \\[\n\\begin{align}\n    x\\text{-DE}^{\\text{sym}}_{x}(y \\mid x_0), x\\text{-DE}^{\\text{sym}}_{x}(y \\mid x_0), \\text{ and } x\\text{-SE}_{x_1, x_0}(y) \\;\\;\\; \\forall t \\in \\{2010, \\dots, 2020\\}.\n\\end{align}\n\\]\nThe following code generates the data, obtains the ground truth causal fairness measures, and also performs the fairness_cookbook() at each step.\n\ncol_adm_tim &lt;- function(n, cfs) {\n\n  dat &lt;- bqn &lt;- gtr &lt;- list()\n\n  for (t in seq_len(10)) {\n\n    # generate data for given year\n    dat[[t]] &lt;- col_adm(n, cfs)\n\n    # compute decomposition\n    bqn[[t]] &lt;- fairness_cookbook(dat[[t]], X = \"x\", Z = \"z\", W = \"d\", Y = \"y\",\n                                  x0 = 0, x1 = 1)\n\n    # get ground truth\n    gtr[[t]] &lt;- c(cfs$alf, -(cfs$bet * cfs$lam),\n                  0.1 * ( (0.125 + 0.2 * (0.5 + cfs$kap))/(0.45) -\n                             (0.125 + 0.4 * (0.5 + cfs$kap))/(0.55) ))\n\n    # update the coefficients for each year\n    cfs &lt;- cfs_nxt(cfs)\n\n  }\n\n  list(dat = dat, bqn = bqn, gtr = gtr)\n\n}\n\ncfs &lt;- list(kap = 0.3, lam = 0.2, alf = 0.1, bet = 0.3)\n\nset.seed(22)\nres &lt;- col_adm_tim(n = 5000L, cfs = cfs)\n\notp &lt;- c()\nfor (t in seq_along(res[[\"bqn\"]])) {\n\n  x &lt;- summary(res[[\"bqn\"]][[t]])$measures\n  x_tr &lt;- res[[\"gtr\"]][[t]]\n  add_row &lt;- data.frame(\n    Spurious = x[x$measure == \"ctfse\", ]$value,\n    Indirect = x[x$measure == \"ctfie\", ]$value,\n    Direct = x[x$measure == \"ctfde\", ]$value,\n    Spurious_True = x_tr[3],\n    Indirect_True = x_tr[2],\n    Direct_True = x_tr[1],\n    Year = 2010+t\n  )\n\n  otp &lt;- rbind(otp, add_row)\n\n}\n\ndf &lt;- reshape2::melt(otp, id.vars = \"Year\", variable.name = \"Measure\")\ndf$Meas &lt;- gsub(\"_.*\", \"\", df$Measure)\ndf$whc &lt;- ifelse(grepl(\"_\", df$Measure), \"True (population)\",\n                 \"Estimated (from sample)\")\n\nggplot(df, aes(x = Year, y = value, color = Meas, linetype = whc)) +\n  geom_line() + geom_point() + theme_bw() +\n  ggtitle(\"Bias Quantification Over Time - College Admissions\") +\n  ylab(\"Value\") + scale_y_continuous(labels = scales::percent,\n                                     limits = c(-0.1, 0.15)) +\n  scale_x_continuous(breaks = seq(2010, 2020)) +\n  theme(\n    legend.position = \"bottom\",\n    legend.box.background = element_rect(),\n    legend.box = \"horizontal\"\n  ) + scale_linetype_discrete(name = \"Quantity\") +\n  scale_color_discrete(name = \"Effect\")\n\n\n\n\nFigure 1: Tracking bias over time in the synthetic College Admissions dataset from Example 5.2, between years 2010 and 2020. Both the estimated values from simulated samples (solid line) and the true population values (dashed lines) are shown, for direct (red), indirect (green), and spurious (blue) effects.\n\n\n\n\nThe temporal dynamics of the estimated measures of discrimination (together with the ground truth values obtained from the SCM \\(\\mathcal{M}_t\\)) are shown graphically in Figure 1."
  },
  {
    "objectID": "pages/t1-admissions-over-time.html#college-admissions-bias-detection-over-time",
    "href": "pages/t1-admissions-over-time.html#college-admissions-bias-detection-over-time",
    "title": "College Admissions: Task 1 over Time",
    "section": "",
    "text": "In this vignette, we analyze a synthetic College Admission dataset, to demonstrate how the task of bias detection could be performed over time, in a university. In particular, we have the following story.\nA university in the United States admits applicants every year. The university is interested in quantifying discrimination in the admission process and track it over time, between 2010 and 2020. The data generating process changes over time, and can be described as follows. Let \\(X\\) denote gender (\\(x_0\\) female, \\(x_1\\) male). Let \\(Z\\) be the age at time of application (\\(Z = 0\\) under 20 years, \\(Z = 1\\) over 20 years) and let \\(W\\) denote the department of application (\\(W = 0\\) for arts&humanities, \\(W = 1\\) for sciences). Finally, let \\(Y\\) denote the admission decision (\\(Y = 0\\) rejection, \\(Y = 1\\) acceptance). The application process changes over time and is given by \\[\n\\begin{empheq}[left ={\\mathcal{F}(t), P(U): \\empheqlbrace}]{align}\n                X & \\gets \\mathbb{1} ( U_X &lt; 0.5 + 0.1U_{XZ})\\label{eq:fcb2-x}\\\\\n                Z & \\gets \\mathbb{1} (U_Z &lt; 0.5 + \\kappa(t) U_{XZ})  \\\\\n                W & \\gets \\mathbb{1} (U_W &lt; 0.5 + \\lambda(t) U_{XZ})  \\\\\n                Y & \\gets \\mathbb{1} (U_Y &lt; 0.1 + \\alpha(t)X + \\beta(t)W + 0.1Z). \\label{eq:fcb2-y} \\\\\n                \\nonumber\\\\\n                    U_{XZ} &\\in \\{0,1\\}, P(U_{XZ} = 1) = 0.5, \\\\\n                    U_X &, U_Z, U_W, U_Y \\sim \\text{Unif}[0, 1].       \n\\end{empheq}\n\\] The coefficients \\(\\kappa(t), \\lambda(t), \\alpha(t), \\beta(t)\\) change every year, and obey the following dynamics: \\[\n\\begin{align}\n    \\kappa(t+1) &= 0.9\\kappa(t) \\\\\n    \\lambda(t+1) &= \\lambda(t) (1 - \\beta(t)) \\\\\n    \\beta(t+1) &= \\beta(t) (1 - \\lambda(t)) f(t), f(t) \\sim \\text{Unif}[0.8, 1.2] \\\\\n    \\alpha(t+1) &= 0.8\\alpha(t).\n\\end{align}\n\\] The equations can be interpreted as follows. The coefficient \\(\\kappa(t)\\) decreases over time, meaning that the overall age gap between the groups decreases. The coefficient \\(\\lambda(t)\\) decreases compared to the previous year, by an amount dependent on \\(\\beta(t)\\). In words, the rate of application to arts&humanities departments decreases if these departments have lower overall admission rates (i.e., students are less likely to apply to departments that are hard to get into). Further, \\(\\alpha(t)\\), which represents gender bias, decreases over time. Finally, \\(\\beta(t)\\) represent the increase in the probability of admission when applying to a science department. Its value depends on the value from the previous year, multiplied by \\((1 - \\lambda(t))\\) and the random variable \\(f(t)\\). Multiplication by the former factor describes the mechanism in which the benefit of applying to a science department decreases if a larger proportion of students apply for it. The latter factor describes a random variation over time which describes how well (in relative terms) the science departments are funded, and can be seen as depending on research and market dynamics in the sciences.\nWe now create functions for generating synethetic data as described above:\n\ncol_adm &lt;- function(n, cfs, kap = cfs[[\"kap\"]], lam = cfs[[\"lam\"]],\n                    alf = cfs[[\"alf\"]], bet = cfs[[\"bet\"]]) {\n\n  u_xz &lt;- rbinom(n, size = 1, prob = 0.5)\n  x &lt;- rbinom(n, size = 1, prob = 0.5 + 0.1 * u_xz)\n  z &lt;- rbinom(n, size = 1, prob = 0.5 + kap * u_xz)\n  d &lt;- rbinom(n, size = 1, prob = 0.5 + lam * x)\n  y &lt;- rbinom(n, size = 1, prob = 0.1 + alf * x + bet * d + 0.1 * z)\n\n  data.frame(x, z, d, y)\n\n}\n\ncfs_nxt &lt;- function(cfs, kap = cfs[[\"kap\"]], lam = cfs[[\"lam\"]],\n                    alf = cfs[[\"alf\"]], bet = cfs[[\"bet\"]]) {\n\n  kapt &lt;- kap * 0.9\n  lamt &lt;- lam * (1 - bet)\n  bett &lt;- bet * (1 - lam) * runif(1, 0.8, 1.2)\n  alft &lt;- alf * 0.8\n\n  list(kap = kapt, lam = lamt, alf = alft, bet = bett)\n\n}\n\nThe head data scientist at the university decides to use the Fairness Cookbook for performing bias quantification. The SFM projection of the causal diagram \\(\\mathcal{G}\\) of the dataset is given by \\[\n\\begin{equation}\n        \\Pi_{\\text{SFM}}(\\mathcal{G}) = \\langle X = \\lbrace X \\rbrace,  Z = \\lbrace Z \\rbrace, W = \\lbrace W\\rbrace, Y = \\lbrace Y \\rbrace\\rangle.\n    \\end{equation}\n\\] After that, the analyst estimates the quantities \\[\n\\begin{align}\n    x\\text{-DE}^{\\text{sym}}_{x}(y \\mid x_0), x\\text{-DE}^{\\text{sym}}_{x}(y \\mid x_0), \\text{ and } x\\text{-SE}_{x_1, x_0}(y) \\;\\;\\; \\forall t \\in \\{2010, \\dots, 2020\\}.\n\\end{align}\n\\]\nThe following code generates the data, obtains the ground truth causal fairness measures, and also performs the fairness_cookbook() at each step.\n\ncol_adm_tim &lt;- function(n, cfs) {\n\n  dat &lt;- bqn &lt;- gtr &lt;- list()\n\n  for (t in seq_len(10)) {\n\n    # generate data for given year\n    dat[[t]] &lt;- col_adm(n, cfs)\n\n    # compute decomposition\n    bqn[[t]] &lt;- fairness_cookbook(dat[[t]], X = \"x\", Z = \"z\", W = \"d\", Y = \"y\",\n                                  x0 = 0, x1 = 1)\n\n    # get ground truth\n    gtr[[t]] &lt;- c(cfs$alf, -(cfs$bet * cfs$lam),\n                  0.1 * ( (0.125 + 0.2 * (0.5 + cfs$kap))/(0.45) -\n                             (0.125 + 0.4 * (0.5 + cfs$kap))/(0.55) ))\n\n    # update the coefficients for each year\n    cfs &lt;- cfs_nxt(cfs)\n\n  }\n\n  list(dat = dat, bqn = bqn, gtr = gtr)\n\n}\n\ncfs &lt;- list(kap = 0.3, lam = 0.2, alf = 0.1, bet = 0.3)\n\nset.seed(22)\nres &lt;- col_adm_tim(n = 5000L, cfs = cfs)\n\notp &lt;- c()\nfor (t in seq_along(res[[\"bqn\"]])) {\n\n  x &lt;- summary(res[[\"bqn\"]][[t]])$measures\n  x_tr &lt;- res[[\"gtr\"]][[t]]\n  add_row &lt;- data.frame(\n    Spurious = x[x$measure == \"ctfse\", ]$value,\n    Indirect = x[x$measure == \"ctfie\", ]$value,\n    Direct = x[x$measure == \"ctfde\", ]$value,\n    Spurious_True = x_tr[3],\n    Indirect_True = x_tr[2],\n    Direct_True = x_tr[1],\n    Year = 2010+t\n  )\n\n  otp &lt;- rbind(otp, add_row)\n\n}\n\ndf &lt;- reshape2::melt(otp, id.vars = \"Year\", variable.name = \"Measure\")\ndf$Meas &lt;- gsub(\"_.*\", \"\", df$Measure)\ndf$whc &lt;- ifelse(grepl(\"_\", df$Measure), \"True (population)\",\n                 \"Estimated (from sample)\")\n\nggplot(df, aes(x = Year, y = value, color = Meas, linetype = whc)) +\n  geom_line() + geom_point() + theme_bw() +\n  ggtitle(\"Bias Quantification Over Time - College Admissions\") +\n  ylab(\"Value\") + scale_y_continuous(labels = scales::percent,\n                                     limits = c(-0.1, 0.15)) +\n  scale_x_continuous(breaks = seq(2010, 2020)) +\n  theme(\n    legend.position = \"bottom\",\n    legend.box.background = element_rect(),\n    legend.box = \"horizontal\"\n  ) + scale_linetype_discrete(name = \"Quantity\") +\n  scale_color_discrete(name = \"Effect\")\n\n\n\n\nFigure 1: Tracking bias over time in the synthetic College Admissions dataset from Example 5.2, between years 2010 and 2020. Both the estimated values from simulated samples (solid line) and the true population values (dashed lines) are shown, for direct (red), indirect (green), and spurious (blue) effects.\n\n\n\n\nThe temporal dynamics of the estimated measures of discrimination (together with the ground truth values obtained from the SCM \\(\\mathcal{M}_t\\)) are shown graphically in Figure 1."
  },
  {
    "objectID": "pages/t1-compas-y-yhat.html",
    "href": "pages/t1-compas-y-yhat.html",
    "title": "COMPAS - Task 1",
    "section": "",
    "text": "Courts in Broward County, Florida use machine learning to predict whether individuals released on parole are at high risk of re-offending within 2 years (\\(Y\\)). The algorithm is based on the demographic information \\(Z\\) (\\(Z_1\\) for gender, \\(Z_2\\) for age), race \\(X\\) (\\(x_0\\) denoting White, \\(x_1\\) Non-White), juvenile offense counts \\(J\\), prior offense count \\(P\\), and degree of charge \\(D\\).\nIn this vignette, we perform the task of bias detection on this dataset. We begin by loading and pre-processing the original data:\n\ndata &lt;- read.csv(file.path(root, \"inst\", \"extdata\",\n                           \"compas-scores-two-years.csv\"))\ncol.keep &lt;- which(\n  names(data) %in% c(\"age\", \"sex\", \"juv_fel_count\",\n                     \"juv_misd_count\", \"juv_other_count\", \"priors_count\",\n                     \"c_charge_degree\", \"race\", \"two_year_recid\", \"decile_score\")\n)\ndata &lt;- data[, col.keep]\ndata$race &lt;- factor(data$race)\nlevels(data$race) &lt;- c(\"Minority\", \"Minority\", \"Majority\", \"Minority\",\n                       \"Minority\", \"Minority\")\ndata$race &lt;- relevel(data$race, \"Majority\")\ncumsum(table(data$decile_score)) / sum(table(data$decile_score))\n\n        1         2         3         4         5         6         7         8 \n0.1996119 0.3300527 0.4336013 0.5401996 0.6345994 0.7234544 0.8055171 0.8764902 \n        9        10 \n0.9469088 1.0000000 \n\n# decile_score &gt; 4 represents high risk (approximately)\ndata$northpointe &lt;- as.integer(data$decile_score &gt; 4)\ndata$decile_score &lt;- NULL\nnames(data) &lt;- gsub(\"_count\", \"\", names(data))\nnames(data)[which(names(data) == \"c_charge_degree\")] &lt;- \"charge\"\n\nknitr::kable(head(data), caption = \"COMPAS dataset.\")\n\n\nCOMPAS dataset.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsex\nage\nrace\njuv_fel\njuv_misd\njuv_other\npriors\ncharge\ntwo_year_recid\nnorthpointe\n\n\n\n\nMale\n69\nMinority\n0\n0\n0\n0\nF\n0\n0\n\n\nMale\n34\nMinority\n0\n0\n0\n0\nF\n1\n0\n\n\nMale\n24\nMinority\n0\n0\n1\n4\nF\n1\n0\n\n\nMale\n23\nMinority\n0\n1\n0\n1\nF\n0\n1\n\n\nMale\n43\nMinority\n0\n0\n0\n2\nF\n0\n0\n\n\nMale\n44\nMinority\n0\n0\n0\n0\nM\n0\n0\n\n\n\n\n\nIn Causal Fairness Analysis, we are interested in decomposing the TV measure (also known as the parity gap), into its direct, indirect, and spurious components. We show the causal diagram associated with the data, and also a representation of how the target effects can be visualized as follows:\n\n\n\n\n\n\nFigure 1: COMPAS Causal Diagram\n\n\n\n\n\n\n\nFigure 2: Direct effect visualization.\n\n\n\n\n\n\n\n\n\nFigure 3: Indirect effect visualization.\n\n\n\n\n\n\n\nFigure 4: Confounded effect visualization.\n\n\n\n\n\nAfter obtaining the data, we then specify the Standard Fairness Model, and decompose the TV measure for the true outcome \\(Y\\):\n\nX &lt;- \"race\"\nZ &lt;- c(\"age\", \"sex\")\nW &lt;- c(\"juv_fel\", \"juv_misd\", \"juv_other\", \"priors\", \"charge\")\nY &lt;- c(\"two_year_recid\")\ntwo_year &lt;- fairness_cookbook(data, X = X, W = W, Z = Z, Y = Y,\n                              x0 = \"Majority\", x1 = \"Minority\")\n\nautoplot(two_year, decompose = \"xspec\") + \n  ggtitle(TeX(\"$Y$ disparity decomposition COMPAS\"))\n\n\n\n\nCausal decomposition of the TV measure for two-year recidivism.\n\n\n\n\nHowever, we are also interested in the disparity for the predictor \\(\\widehat{Y}\\), so we can decompose the TV measure for the predictor, too:\n\nYhat &lt;- \"northpointe\"\nnorth_decompose &lt;- fairness_cookbook(data, X = X, W = W, Z = Z, Y = Yhat,\n                                     x0 = \"Majority\", x1 = \"Minority\")\nautoplot(north_decompose, decompose = \"xspec\") +\n  ggtitle(TeX(\"$\\\\widehat{Y}$ disparity decomposition COMPAS\"))\n\n\n\n\nCausal decomposition of the TV measure for Northpointe’s predictions.\n\n\n\n\nTo perform the complete analysis, we plot the two results side-by-side, and shade the areas depending on whether the associated measure is included in the business necessity set or not:\n\n\n\n\n\nCausal decompositions of the TV measure for the true and predicted outcomes visualized together."
  },
  {
    "objectID": "pages/t1-compas-beyond-sfm.html",
    "href": "pages/t1-compas-beyond-sfm.html",
    "title": "COMPAS - Task 1",
    "section": "",
    "text": "In this vignette, we generalize the analysis of the Fairness Cookbook, to consider more refined settings described by an arbitrary causal diagram. The main motivation for doing so comes from the observation that when analyzing disparate impact, quantities such as Ctf-DE\\(_{x_0,x_1}(y\\mid x_0)\\), Ctf-IE\\(_{x_0,x_1}(y\\mid x_0)\\), and Ctf-SE\\(_{x_0,x_1}(y)\\) are insufficient to account for certain business necessity requirements. For concreteness, consider the following example."
  },
  {
    "objectID": "pages/t1-compas-beyond-sfm.html#compas-under-business-necessity",
    "href": "pages/t1-compas-beyond-sfm.html#compas-under-business-necessity",
    "title": "COMPAS - Task 1",
    "section": "COMPAS under Business Necessity",
    "text": "COMPAS under Business Necessity\nThe courts at Broward County, Florida, were using machine learning to predict whether individuals released on parole are at high risk of re-offending within 2 years. The algorithm is based on the demographic information \\(Z\\) (\\(Z_1\\) for gender, \\(Z_2\\) for age), race \\(X\\) (\\(x_0\\) denoting White, \\(x_1\\) Non-White), juvenile offense counts \\(J\\), prior offense count \\(P\\), and degree of charge \\(D\\).\n\n\n\n\n\nFigure 1: Causal diagram of the COMPAS dataset. Business Necessity variables highlighted in green.\n\n\n\n\nA causal analysis using the fairness_cookbook() revealed that:\n\ndata &lt;- get(data(\"compas\", package = \"faircause\"))\nset.seed(2022)\nmdata &lt;- SFM_proj(\"compas\")\nfc_compas &lt;- fairness_cookbook(data, X = mdata$X, Z = mdata$Z, W = mdata$W,\n                               Y = mdata$Y, x0 = mdata$x0, x1 = mdata$x1)\n\nautoplot(fc_compas, decompose = \"xspec\", dataset = \"COMPAS\")\n\n\n\n\nFigure 2: Fairness Cookbook on the COMPAS dataset.\n\n\n\n\nThat is, the computed measures equal: \\[\n\\begin{align}\n  \\text{Ctf-IE}_{x_1, x_0}(y\\mid x_1) &= -5.7\\% \\pm 0.5\\%,\\\\\n  \\text{Ctf-SE}_{x_1, x_0}(y) &= -4.0\\% \\pm 0.9\\%,\n\\end{align}\n\\] and are also shown graphically in Figure 2, potentially indicating presence of disparate impact. Based on this information, a legal team of ProPublica filed a lawsuit to the district court, claiming discrimination w.r.t. the Non-White subpopulation based on the doctrine of disparate impact. After the court hearing, the judge rules that using the attributes age (\\(Z_2\\)), prior count (\\(P\\)), and charge degree (\\(D\\)) is not discriminatory, but using the attributes juvenile count (\\(J\\)) and gender (\\(Z_1\\)) is discriminatory. The causal diagram with a visualization of which variables are included in the business-necessity set is given in Figure 1. Data scientists at ProPublica need to consider how to proceed in the light of this new requirement for discounting the allowable attributes in the quantiative analysis.\nThe difficulty in this example is that the quantity Ctf-SE\\(_{x_1, x_0}(y)\\) measures the spurious discrimination between the attribute \\(X\\) and outcome \\(Y\\) as generated by both confounders \\(Z_1\\) and \\(Z_2\\). Since using the confounder \\(Z_2\\) is not considered discriminatory, but using the confounder \\(Z_1\\) is, the quantity Ctf-SE\\(_{x_1, x_0}(y)\\) needs to be refined such that the spurious variations based on the different confounders are disentangled. In particular, one might be interested in finding a decomposition of the spurious effect such that \\[\n\\begin{align}\n    \\text{Ctf-SE}_{x_1, x_0}(y) = \\underbrace{\\text{Ctf-SE}^{Z_1}_{x_1, x_0}(y)}_{\\text{gender variations}} + \\underbrace{\\text{Ctf-SE}^{Z_2}_{x_1, x_0}(y)}_{\\text{age variations}},\n\\end{align}\n\\] which would allow the data analyst to further distinguish the variations explained by each of the confounders. A similar challenge is present for the Ctf-IE\\(_{x_1, x_0}(y\\mid x_1)\\) measure, since it has contributions explained by juvenile offense counts \\(J\\), prior counts \\(P\\), and the charge degree \\(D\\). Therefore, we might be interested in decomposing the indirect effect into \\[\n\\begin{align}\n    \\text{Ctf-IE}_{x_1, x_0}(y \\mid x_1) =& \\underbrace{\\text{Ctf-IE}^J_{x_1, x_0}(y \\mid x_1)}_{\\text{juvenile count variations}} + \\underbrace{\\text{Ctf-IE}^P_{x_1, x_0}(y \\mid x_1)}_{\\text{prior count variations}} \\\\ &+ \\underbrace{\\text{Ctf-IE}^D_{x_1, x_0}(y \\mid x_1)}_{\\text{charge degree variations}}. \\nonumber\n\\end{align}\n\\] Again, such a decomposition would allow the data analyst to better understand the contribution of each of the mediators to the totality of the indirect effect. In situations when some mediating variables are in the business necessity set, while others are not, such a decomposition would allow for assessment of disparate impact claims.\n\nTowards the solution"
  },
  {
    "objectID": "pages/t3-surgeries.html",
    "href": "pages/t3-surgeries.html",
    "title": "Outcome Control – Cancer Surgery Example",
    "section": "",
    "text": "Consider the structural causal model (SCM) introduced in Ex. 5.10 of the paper (Plečko and Bareinboim 2024): \\[\\begin{align}\nX  &\\gets U_X \\label{eq:cancer-scm-1} \\\\\n                    W &\\gets \\begin{cases} \\sqrt{U_W} \\text{ if } X = x_0, \\\\\n                    1 - \\sqrt{1 - U_W} \\text{ if } X = x_1\n                    \\end{cases}  \\\\\n                D & \\gets f_D(X, W) \\\\\n                Y  &\\gets \\mathbb{1}(U_Y + \\frac{1}{3} WD - \\frac{1}{5} W &gt; 0.5). \\\\\n                    U_X &\\in \\{0,1\\}, P(U_X = 1) = 0.5, \\\\\n                    %U_Z&,\n                        U_W&, U_Y \\sim \\text{Unif}[0, 1], \\label{eq:cancer-scm-n}\n\\end{align}\\] We begin by generating \\(n = 5000\\) samples from the SCM:\n\nset.seed(2023)\nn &lt;- 5000\n\n# generate data from the SCM\nx &lt;- rbinom(n, 1, 0.5)\nuw &lt;- runif(n)\nw &lt;- ifelse(x, 1 - sqrt(1-uw), sqrt(uw)) \nuy &lt;- runif(n)\n\n# f_Y coefficients\nalph &lt;- 1 / 5\nbeta &lt;- 1 / 3\n\n# assuming a policy D s.t. P(d | w) = w.\nd &lt;- rbinom(n, 1, prob = w)\ny &lt;- as.integer(uy + beta * w *d - alph * w &gt; 0.5)\n\nlabels &lt;- c(\"Doomed\", \"Helped\", \"Safe\")\n\ndelta &lt;- beta * w\n\n# determine canonical type\ncanon &lt;- ifelse(uy &gt; 0.5 + w * alph, \"Safe\",\n                ifelse(uy &gt; 0.5 + w * alph - w * beta, \"Helped\", \"Doomed\"))\n\nThe latent variable \\(U_Y\\) determines which canonical type the individual belongs to. After generating the data, we visualize it from the oracle’s perspective, assuming access to \\(U_Y\\):\n\n\n\n\n\n\n\n\n\nFrom the oracle’s perspective, treating individuals in the green area is optimal. However, we next look at the perspective of the decision-maker:"
  },
  {
    "objectID": "pages/t3-surgeries.html#oracles-perspective",
    "href": "pages/t3-surgeries.html#oracles-perspective",
    "title": "Outcome Control – Cancer Surgery Example",
    "section": "",
    "text": "Consider the structural causal model (SCM) introduced in Ex. 5.10 of the paper (Plečko and Bareinboim 2024): \\[\\begin{align}\nX  &\\gets U_X \\label{eq:cancer-scm-1} \\\\\n                    W &\\gets \\begin{cases} \\sqrt{U_W} \\text{ if } X = x_0, \\\\\n                    1 - \\sqrt{1 - U_W} \\text{ if } X = x_1\n                    \\end{cases}  \\\\\n                D & \\gets f_D(X, W) \\\\\n                Y  &\\gets \\mathbb{1}(U_Y + \\frac{1}{3} WD - \\frac{1}{5} W &gt; 0.5). \\\\\n                    U_X &\\in \\{0,1\\}, P(U_X = 1) = 0.5, \\\\\n                    %U_Z&,\n                        U_W&, U_Y \\sim \\text{Unif}[0, 1], \\label{eq:cancer-scm-n}\n\\end{align}\\] We begin by generating \\(n = 5000\\) samples from the SCM:\n\nset.seed(2023)\nn &lt;- 5000\n\n# generate data from the SCM\nx &lt;- rbinom(n, 1, 0.5)\nuw &lt;- runif(n)\nw &lt;- ifelse(x, 1 - sqrt(1-uw), sqrt(uw)) \nuy &lt;- runif(n)\n\n# f_Y coefficients\nalph &lt;- 1 / 5\nbeta &lt;- 1 / 3\n\n# assuming a policy D s.t. P(d | w) = w.\nd &lt;- rbinom(n, 1, prob = w)\ny &lt;- as.integer(uy + beta * w *d - alph * w &gt; 0.5)\n\nlabels &lt;- c(\"Doomed\", \"Helped\", \"Safe\")\n\ndelta &lt;- beta * w\n\n# determine canonical type\ncanon &lt;- ifelse(uy &gt; 0.5 + w * alph, \"Safe\",\n                ifelse(uy &gt; 0.5 + w * alph - w * beta, \"Helped\", \"Doomed\"))\n\nThe latent variable \\(U_Y\\) determines which canonical type the individual belongs to. After generating the data, we visualize it from the oracle’s perspective, assuming access to \\(U_Y\\):\n\n\n\n\n\n\n\n\n\nFrom the oracle’s perspective, treating individuals in the green area is optimal. However, we next look at the perspective of the decision-maker:"
  },
  {
    "objectID": "pages/t3-surgeries.html#decision-makers-perspective",
    "href": "pages/t3-surgeries.html#decision-makers-perspective",
    "title": "Outcome Control – Cancer Surgery Example",
    "section": "Decision-maker’s Perspective",
    "text": "Decision-maker’s Perspective\nWe next plot the decision-maker’s perspective, which is based on estimating the benefit \\(E[Y_{d_1} - Y_{d_0} \\mid x, z, w]\\). We first estimate the benefit from the data.\n\ndf &lt;- data.frame(x, w, d, y)\n\n# fit a logistic regression model\nlogreg &lt;- glm(y ~ x + w * d, data = df, family = \"binomial\")\n\n\n# compute the potential outcomes\ndf_d0 &lt;- df_d1 &lt;- df\ndf_d0$d &lt;- 0\ndf_d1$d &lt;- 1\npy_d0 &lt;- predict(logreg, df_d0, type = \"response\")\npy_d1 &lt;- predict(logreg, df_d1, type = \"response\")\n\n# compute the benefit\ndf$delta_hat &lt;- delta_hat &lt;- py_d1 - py_d0\n\nAfter estimating \\(\\Delta\\), we visualize the data based on the obtained estimates:\n\n\n\n\n\n\n\n\n\nTherefore, when looking at the benefit, it is clearly higher for the \\(X = x_0\\) group. This explains why the decision-maker may naturally decide to treat more individuals in the \\(X = x_0\\) group."
  },
  {
    "objectID": "pages/t3-surgeries.html#computing-the-disparity-in-treatment-allocation",
    "href": "pages/t3-surgeries.html#computing-the-disparity-in-treatment-allocation",
    "title": "Outcome Control – Cancer Surgery Example",
    "section": "Computing the Disparity in Treatment Allocation",
    "text": "Computing the Disparity in Treatment Allocation\nWe look at the policy \\(D^*\\) obtained from Algorithm 5.3: \\[\\begin{align}\n  D^* = \\mathbb{1}(\\Delta &gt; \\frac{1}{6}).\n\\end{align}\\]\n\n# construct the decision policy \nd_star &lt;- as.integer(delta_hat &gt; quantile(delta_hat, 0.5))\n# look at resource allocation in each group\ntapply(d_star, x, mean)\n\n        0         1 \n0.7513683 0.2366912 \n\n\nTherefore, we obtained that \\[\\begin{align}\n  P(d^* \\mid x_1) - P(d^* \\mid x_0) \\approx -51.5\\%.\n\\end{align}\\] The true population value (as shown in the paper) is -50%."
  },
  {
    "objectID": "pages/t3-surgeries.html#decomposing-the-disparity",
    "href": "pages/t3-surgeries.html#decomposing-the-disparity",
    "title": "Outcome Control – Cancer Surgery Example",
    "section": "Decomposing the Disparity",
    "text": "Decomposing the Disparity\nWe next look at decomposing the disparity using Algorithm 5.4. We first use the faircause package to decompose the disparity in resource allocation:\n\n# implement the new policy\ndf$d &lt;- d_star\n\n# apply the fairness cookbook\nfc_d &lt;- fairness_cookbook(df, X = \"x\", Z = NULL, W = \"w\", Y = \"d\",\n                          x0 = 0, x1 = 1, nboot1 = 5, model = \"linear\")\n\nautoplot(fc_d, signed = FALSE) + ggtitle(TeX(\"$P(d^* | x_1) - P(d^* | x_0)$ decomposition\")) +\n  scale_x_discrete(labels = c(\"TV\", \"DE\", \"IE\", \"SE\"))\n\n\n\n\n\n\n\n\nTherefore, as explained in the paper, the disparity is driven by the indirect effect. We can also decompose the disparity in the estimated benefit \\(E[\\Delta \\mid x_1] - E[\\Delta \\mid x_0]\\):\n\n# apply the fairness cookbook\nfc_delta &lt;- fairness_cookbook(df, X = \"x\", Z = NULL, W = \"w\", Y = \"delta_hat\",\n                              x0 = 0, x1 = 1, nboot1 = 5, model = \"linear\")\n\nautoplot(fc_delta, signed = FALSE) +\n  ggtitle(TeX(\"$E[\\\\Delta | x_1] - E[\\\\Delta | x_0]$ decomposition\")) + \n  scale_x_discrete(labels = c(\"TV\", \"DE\", \"IE\", \"SE\"))"
  },
  {
    "objectID": "pages/t3-surgeries.html#constructing-the-causally-fair-dcf-policy",
    "href": "pages/t3-surgeries.html#constructing-the-causally-fair-dcf-policy",
    "title": "Outcome Control – Cancer Surgery Example",
    "section": "Constructing the causally fair \\(D^{CF}\\) policy",
    "text": "Constructing the causally fair \\(D^{CF}\\) policy\nThe clinicians first want to construct the causally fair policy. For doing so, they first construct the counterfactual values of the illness severity. In particular, they assume that the relative order of the values remains the same under the counterfactual change of the protected attribute.\n\n# get order statistics for the X = x_1 group\nord_x1 &lt;- order(w[x == 1])\n\n# compute the sample quantile\nquant_x1 &lt;- ord_x1 / length(ord_x1)\n\n# initialize the counterfactual values\nw_cf &lt;- rep(0, n)\n\n# for X = x_0, values remain the same by consistency axiom\nw_cf[x == 0] &lt;- w[x == 0]\n\n# for X = x_1, match to the corresponding quantile in the X = x_0 distribution\nw_cf[x == 1] &lt;- vapply(quant_x1, function(q) quantile(w[x == 0], q), numeric(1L))\n\n# compute the counterfactual benefit\ndelta_cf &lt;- w_cf / 3\n\nb &lt;- mean(d_star)\nfcf &lt;- ecdf(delta_cf)\nfcf.inv &lt;- inverse(fcf, lower = 0, upper = 1)\nd_cf &lt;- as.integer(delta_cf &gt; fcf.inv(1 - b))\ntapply(d_cf, x, mean)\n\n  0   1 \n0.5 0.5 \n\n\nTherefore, we see that \\[\\begin{align}\n  P(d^{CF} \\mid x_1) - P(d^{CF} \\mid x_0) \\approx 0\\%.\n\\end{align}\\]\nThe true population value of the disparity for \\(D^{CF}\\), discussed in the paper, also equals \\(0\\%\\)."
  },
  {
    "objectID": "pages/t3-surgeries.html#constructing-the-causally-fair-dut-policy",
    "href": "pages/t3-surgeries.html#constructing-the-causally-fair-dut-policy",
    "title": "Outcome Control – Cancer Surgery Example",
    "section": "Constructing the causally fair \\(D^{UT}\\) policy",
    "text": "Constructing the causally fair \\(D^{UT}\\) policy\nFinally, to perform the utilitarian approach, we have from the disparity in \\(D^{CF}\\) that: \\[\\begin{align}\n  M = 0.\n\\end{align}\\] Therefore, following Algorithm 5.5 with method = UT, we compute:\n\n# compute \\epsilon, l\nepsilon &lt;- mean(d_star[x == 1]) - mean(d_star[x == 0]) - 0 # -0 since M = 0!\nl &lt;- sum(x == 1) / sum(x == 0)\n\n# compute inverse function of \\Delta \\mid x_0\nf0 &lt;- ecdf(delta_hat[x == 0])\nf0.inv &lt;- inverse(f0, lower = 0, upper = 1)\n\n# compute \\delta^{x_0}\ndelta_x0 &lt;- f0.inv(1 - (mean(delta_hat[x == 0] &gt; quantile(delta_hat, 0.5)) + \n                          epsilon * l / (1 + l)))\n\n# compute inverse function of \\Delta \\mid x_0\nf1 &lt;- ecdf(delta_hat[x == 1])\nf1.inv &lt;- inverse(f1, lower = 0, upper = 1)\n\n# get the budget\nb &lt;- mean(d_star)\n\n# compute \\delta^{x_1}\ndelta_x1 &lt;-  f1.inv(1 - (b / mean(x == 1) - 1 / l * mean(delta_hat[x == 0] &gt;= delta_x0)))\n\nd_ut &lt;- rep(0, length(d))\nd_ut[x == 0] &lt;- delta_hat[x == 0] &gt; delta_x0\nd_ut[x == 1] &lt;- delta_hat[x == 1] &gt; delta_x1\n\n# compute the allocated proportions\ntapply(d_ut, x, mean)\n\n        0         1 \n0.4996091 0.5004095 \n\n\nIn particular, we have showed that \\[\\begin{align}\n  P(d^{UT} \\mid x_1) - P(d^{UT} \\mid x_0) \\approx 0\\%.\n\\end{align}\\] The true population value also equals \\(0\\%\\).\nIn general, the policies \\(D^{CF}\\) and \\(D^{UT}\\) may pick different individuals for treatment. The conditions under which the policies \\(D^{CF}\\) and \\(D^{UT}\\) are the same are discussed in Appendix D of the paper. In the above example, these conditions are satisfied, and \\(D^{CF}\\), \\(D^{UT}\\) select the same group of individuals."
  },
  {
    "objectID": "pages/t2-compas-fpt-inthe-wild.html",
    "href": "pages/t2-compas-fpt-inthe-wild.html",
    "title": "Task 2: Fair Predictions on COMPAS",
    "section": "",
    "text": "In this vignette, we focus on Task 2 (Fair Prediction), and specifically focus on the limitation of the currently found methods in the literature to provide causally meaningful fair predictions."
  },
  {
    "objectID": "pages/t2-compas-fpt-inthe-wild.html#fair-predictions-on-compas",
    "href": "pages/t2-compas-fpt-inthe-wild.html#fair-predictions-on-compas",
    "title": "Task 2: Fair Predictions on COMPAS",
    "section": "Fair Predictions on COMPAS",
    "text": "Fair Predictions on COMPAS\nA team of data scientists from ProPublica have shown that the COMPAS dataset from Broward County contains a strong racial bias against minorities. They are now interested in producing fair predictions \\(\\widehat{Y}\\) on the dataset, to replace the biased predictions. They first load the COMPAS data:\n\n# load the data\ndat &lt;- get(data(\"compas\", package = \"faircause\"))\nknitr::kable(head(dat), caption = \"COMPAS dataset.\")\n\n\nCOMPAS dataset.\n\n\n\n\n\n\n\n\n\n\n\n\n\nsex\nage\nrace\njuv_fel_count\njuv_misd_count\njuv_other_count\npriors_count\nc_charge_degree\ntwo_year_recid\n\n\n\n\nMale\n69\nNon-White\n0\n0\n0\n0\nF\n0\n\n\nMale\n34\nNon-White\n0\n0\n0\n0\nF\n1\n\n\nMale\n24\nNon-White\n0\n0\n1\n4\nF\n1\n\n\nMale\n23\nNon-White\n0\n1\n0\n1\nF\n0\n\n\nMale\n43\nNon-White\n0\n0\n0\n2\nF\n0\n\n\nMale\n44\nNon-White\n0\n0\n0\n0\nM\n0\n\n\n\n\n# load the SFM projection\nmdat &lt;- SFM_proj(\"compas\")\n\nTo produce fair predictions, they first experiment with four different classifiers.\n\n(i) Random Forest without fairness constraints\n\n# Method 1: Random Forest\norg_dat &lt;- dat\norg_dat$two_year_recid &lt;- ranger(two_year_recid ~ ., dat,\n                                 classification = TRUE)$predictions\n\n# Method 1: decompose Total Variation\norg_tvd &lt;- fairness_cookbook(org_dat, mdat[[\"X\"]], mdat[[\"Z\"]], mdat[[\"W\"]],\n                             mdat[[\"Y\"]], mdat[[\"x0\"]], mdat[[\"x1\"]])\norg_plt &lt;- autoplot(org_tvd, decompose = \"xspec\", dataaset = dataset) +\n  xlab(\"\") + ylab(\"\")\n\n\n\n(ii) Logistic regression trained with reweighing (Kamiran and Calders 2012)\n\nimport os\n\nexec(open(os.path.join(r.root, \"py\", \"reweighing_compas.py\")).read())\n\nWARNING:root:\n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n&lt;https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8&gt;\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n&lt;https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air&gt;\n: LawSchoolGPADataset will be unavailable. To install, run:\npip install 'aif360[LawSchoolGPA]'\nWARNING:root:No module named 'tensorflow': AdversarialDebiasing will be unavailable. To install, run:\npip install 'aif360[AdversarialDebiasing]'\nWARNING:root:No module named 'tensorflow': AdversarialDebiasing will be unavailable. To install, run:\npip install 'aif360[AdversarialDebiasing]'\n\nYhat_rew = reweigh_and_predict(r.dat, r.dat)\n\n\n# Method 2: Reweighing by Kamiran & Calders\nreweigh_dat &lt;- dat\nreweigh_dat$two_year_recid &lt;- as.vector(py$Yhat_rew)\n\nreweigh_tvd &lt;- fairness_cookbook(\n  reweigh_dat, mdat[[\"X\"]], mdat[[\"Z\"]], mdat[[\"W\"]], mdat[[\"Y\"]], mdat[[\"x0\"]],\n  mdat[[\"x1\"]])\n\nrew_plt &lt;- autoplot(reweigh_tvd, decompose = \"xspec\", dataset = dataset) +\n  xlab(\"\") + ylab(\"\")\n\n\n\n(iii) Fair Reductions approach of (Agarwal et al. 2018)\n\nexec(open(os.path.join(r.root, \"py\", \"reductions_compas.py\")).read())\nYhat_red = reduce_and_predict(r.dat, r.dat, 0.01)\n\n\n# Method 3: Reductions (Agarwal et. al.)\nsource_python(file.path(root, \"py\", paste0(\"reductions_\", dataset, \".py\")))\n\nred_dat &lt;- dat\nred_dat$two_year_recid &lt;- as.vector(py$Yhat_red)\nred_tvd &lt;- fairness_cookbook(\n  red_dat, mdat[[\"X\"]], mdat[[\"W\"]], mdat[[\"Z\"]], mdat[[\"Y\"]], mdat[[\"x0\"]],\n  mdat[[\"x1\"]]\n)\n\nred_plt &lt;- autoplot(red_tvd, decompose = \"xspec\", dataset = dataset) +\n  xlab(\"\") + ylab(\"\")\n\n\n\n(iv) Random Forest with reject-option post-processing (Kamiran, Karim, and Zhang 2012)\n\n# Method 4: reject-option classification\nrjo_prb &lt;- ranger(two_year_recid ~ ., dat, probability = TRUE)$predictions[, 2]\nrjo_dat &lt;- dat\nrjo_dat$two_year_recid &lt;- RejectOption(rjo_prb, rjo_dat$race)\n\nrjo_tvd &lt;- fairness_cookbook(rjo_dat, mdat[[\"X\"]], mdat[[\"W\"]], mdat[[\"Z\"]],\n                             mdat[[\"Y\"]], mdat[[\"x0\"]], mdat[[\"x1\"]])\nrjo_plt &lt;- autoplot(rjo_tvd, decompose = \"xspec\", dataset = dataset) +\n  xlab(\"\") + ylab(\"\")\n\n\n\nAre the methods successful at eliminating discrimination?\nThe fair prediction algorithms used above are intended to set the TV measure to \\(0\\). After constructing these predictors, the ProPublica team made use of the Fairness Cookbook, to inspect the causal implications of the methods. Following the steps of the Fairness Cookbook, the team computes the TV measure, together with the appropriate measures of direct, indirect, and spurious discrimination. We can now visualize these decompositions in Figure 1.\n\n\n\n\n\nFigure 1: Fair Predictions on the COMPAS dataset.\n\n\n\n\nThe ProPublica team notes that all methods substantially reduce the \\(\\text{TV}_{x_0,x_1}(\\widehat{y})\\), however, the measures of direct, indirect, and, spurious effects are not necessarily reduced to \\(0\\), consistent with the Fair Prediction Theorem."
  },
  {
    "objectID": "pages/t2-compas-fpt-inthe-wild.html#how-to-fix-the-issue",
    "href": "pages/t2-compas-fpt-inthe-wild.html#how-to-fix-the-issue",
    "title": "Task 2: Fair Predictions on COMPAS",
    "section": "How to fix the issue?",
    "text": "How to fix the issue?\nTo produce causally meaningful fair predictions, we suggest using the fairadapt package (Plečko and Meinshausen 2020; Plečko, Bennett, and Meinshausen 2021). In particular, the package offers a way of removing discrimination which is based on the causal diagram. In this application, we are interested in constructing fair predictions that remove both the direct and the indirect effect. Firstly, we obtain the adjacency matrix representing the causal diagram associated with the COMPAS dataset:\n\nset.seed(2022)\n# load the causal diagram\nmats &lt;- get_mat(dataset)\nadj.mat &lt;- mats[[1L]]\ncfd.mat &lt;- mats[[2L]]\n\ncausal_graph &lt;- fairadapt::graphModel(adj.mat, cfd.mat)\nlayout_matrix &lt;- matrix(c(\n  1, 2,  # age\n  -1, 2,  # sex\n  -2, -2,  # juv_fel_count\n  -1, -2, # juv_misd_count\n  0, -2, # juv_other_count\n  1, -2,  # priors_count\n  2, -2,  # c_charge_degree\n  -3, 0,  # race\n  3, 0    # two_year_recid\n), ncol = 2, byrow = TRUE)\n\nplot(causal_graph, layout=layout_matrix, vertex.size = 30)\n\n\n\n\nAfter loading the causal diagram, we perform fair data adaptation:\n\nfdp &lt;- fairadapt::fairadapt(two_year_recid ~ ., prot.attr = \"race\",\n                            train.data = dat, adj.mat = adj.mat)\n\n# obtain the adapted data\nad_dat &lt;- fairadapt:::adaptedData(fdp)\nad_dat$race &lt;- dat$race\n\n# obtain predictions based on the adapted data\nadapt_oob &lt;- ranger(two_year_recid ~ ., ad_dat,\n                    classification = TRUE)$predictions\nad_dat$two_year_recid &lt;- adapt_oob\n\n# decompose the TV for the predictions\ndat.fairadapt &lt;- dat\ndat.fairadapt$two_year_recid &lt;- adapt_oob\n\nfairadapt_tvd &lt;- fairness_cookbook(\n  ad_dat, mdat[[\"X\"]], mdat[[\"W\"]], mdat[[\"Z\"]],\n  mdat[[\"Y\"]], mdat[[\"x0\"]], mdat[[\"x1\"]]\n)\n\n# visualize the decomposition\nfairadapt_plt &lt;- autoplot(fairadapt_tvd, decompose = \"xspec\", dataset = dataset) +\n  xlab(\"\") + ylab(\"\")\n\nfairadapt_plt\n\n\n\n\nFigure 2: Fair Data Adaptation on the COMPAS dataset.\n\n\n\n\nFigure 2 shows how the fairadapt package can be used to provide causally meaningful predictions that remove both direct and indirect effects."
  },
  {
    "objectID": "pages/t3-mimic-respirators.html",
    "href": "pages/t3-mimic-respirators.html",
    "title": "Task 3: Respirator Allocation on MIMIC-IV",
    "section": "",
    "text": "We begin by loading and inspecting the data from MIMIC-IV:\n\nsrc &lt;- \"miiv\"\ndata &lt;- as.data.frame(load_resp_data(src))\nknitr::kable(head(data), caption = \"MIMIC-IV Respiration data.\")\n\n\nMIMIC-IV Respiration data.\n\n\n\n\n\n\n\n\n\n\n\n\n\nstay_id\no2prior\nsofa\nresp\npo2\nsex\nage\no2post\nrespirator\n\n\n\n\n30000646\n96.16667\n2\n17.66667\n71\n1\n43\n96.50000\n0\n\n\n30001336\n97.00000\n1\n26.66667\n71\n1\n77\n96.00000\n0\n\n\n30001396\n87.66667\n2\n25.66667\n76\n1\n40\n80.66667\n0\n\n\n30001446\n99.33333\n11\n19.66667\n98\n1\n56\n99.00000\n0\n\n\n30001471\n94.00000\n1\n15.66667\n98\n1\n86\n92.16667\n0\n\n\n30001555\n94.33333\n9\n16.00000\n98\n0\n33\n96.66667\n0\n\n\n\n\n\nWe consider the cohort of all patients in the database admitted to the ICU. Patients who are mechanically ventilated immediately upon entering the ICU are subsequently removed. By focusing on the time window of the first 48 hours from admission to ICU, for each patient we determine the earliest time of mechanical ventilation, labeled \\(t_{MV}\\). Since mechanical ventilation is used to stabilize the respiratory profile of patients, for each patient we determine the average oxygen saturation in the three-hour period \\([t_{MV}-3, t_{MV}]\\) prior to mechanical ventilation, labeled O\\(_2\\)-pre. We also determine the oxygen saturation in the three-hour period following ventilation \\([t_{MV}, t_{MV}+3]\\), labeled O\\(_2\\)-post. For controls (patient not ventilated at any point in the first 48 hours), we take the reference point as 12 hours after ICU admission, and calculate the values in three hours before and after this time. Patients’ respiratory stability, which represents the outcome of interest \\(Y\\), is measured as follows:\n\\[\\begin{align}\n    Y := \\begin{cases}\n    0 \\text{ if O}_2\\text{-post} \\geq 97, \\\\\n    -(\\text{O}_2\\text{-post}-97)^2 \\text{ otherwise}.\n        \\end{cases}\n\\end{align}\\]\nValues of oxygen saturation above 97 are considered stable, and the larger the distance from this stability value, the higher the risk for the patient. We also collect other important patient characteristics before intervention that are the key predictors of outcome, including the SOFA score, respiratory rate, and partial oxygen pressure (PaO\\(_2\\))."
  },
  {
    "objectID": "pages/t3-mimic-respirators.html#inspecting-the-respiration-data-from-mimic-iv-dataset",
    "href": "pages/t3-mimic-respirators.html#inspecting-the-respiration-data-from-mimic-iv-dataset",
    "title": "Task 3: Respirator Allocation on MIMIC-IV",
    "section": "",
    "text": "We begin by loading and inspecting the data from MIMIC-IV:\n\nsrc &lt;- \"miiv\"\ndata &lt;- as.data.frame(load_resp_data(src))\nknitr::kable(head(data), caption = \"MIMIC-IV Respiration data.\")\n\n\nMIMIC-IV Respiration data.\n\n\n\n\n\n\n\n\n\n\n\n\n\nstay_id\no2prior\nsofa\nresp\npo2\nsex\nage\no2post\nrespirator\n\n\n\n\n30000646\n96.16667\n2\n17.66667\n71\n1\n43\n96.50000\n0\n\n\n30001336\n97.00000\n1\n26.66667\n71\n1\n77\n96.00000\n0\n\n\n30001396\n87.66667\n2\n25.66667\n76\n1\n40\n80.66667\n0\n\n\n30001446\n99.33333\n11\n19.66667\n98\n1\n56\n99.00000\n0\n\n\n30001471\n94.00000\n1\n15.66667\n98\n1\n86\n92.16667\n0\n\n\n30001555\n94.33333\n9\n16.00000\n98\n0\n33\n96.66667\n0\n\n\n\n\n\nWe consider the cohort of all patients in the database admitted to the ICU. Patients who are mechanically ventilated immediately upon entering the ICU are subsequently removed. By focusing on the time window of the first 48 hours from admission to ICU, for each patient we determine the earliest time of mechanical ventilation, labeled \\(t_{MV}\\). Since mechanical ventilation is used to stabilize the respiratory profile of patients, for each patient we determine the average oxygen saturation in the three-hour period \\([t_{MV}-3, t_{MV}]\\) prior to mechanical ventilation, labeled O\\(_2\\)-pre. We also determine the oxygen saturation in the three-hour period following ventilation \\([t_{MV}, t_{MV}+3]\\), labeled O\\(_2\\)-post. For controls (patient not ventilated at any point in the first 48 hours), we take the reference point as 12 hours after ICU admission, and calculate the values in three hours before and after this time. Patients’ respiratory stability, which represents the outcome of interest \\(Y\\), is measured as follows:\n\\[\\begin{align}\n    Y := \\begin{cases}\n    0 \\text{ if O}_2\\text{-post} \\geq 97, \\\\\n    -(\\text{O}_2\\text{-post}-97)^2 \\text{ otherwise}.\n        \\end{cases}\n\\end{align}\\]\nValues of oxygen saturation above 97 are considered stable, and the larger the distance from this stability value, the higher the risk for the patient. We also collect other important patient characteristics before intervention that are the key predictors of outcome, including the SOFA score, respiratory rate, and partial oxygen pressure (PaO\\(_2\\))."
  },
  {
    "objectID": "pages/t3-mimic-respirators.html#constructing-the-sfm",
    "href": "pages/t3-mimic-respirators.html#constructing-the-sfm",
    "title": "Task 3: Respirator Allocation on MIMIC-IV",
    "section": "Constructing the SFM",
    "text": "Constructing the SFM\nWe next construct the Standard Fairness Model, with also a decision \\(D\\):\n\n# constructing the SFM\nX &lt;- \"sex\"\nZ &lt;- \"age\"\nW &lt;- c(\"sofa\", \"po2\", \"o2prior\")\nD &lt;- \"respirator\"\nY &lt;- \"o2post\""
  },
  {
    "objectID": "pages/t3-mimic-respirators.html#obtaining-fair-decisions",
    "href": "pages/t3-mimic-respirators.html#obtaining-fair-decisions",
    "title": "Task 3: Respirator Allocation on MIMIC-IV",
    "section": "Obtaining Fair Decisions",
    "text": "Obtaining Fair Decisions\nFair Decisions can be obtained by passing the data, SFM, and any transform functions of the potential outcome \\(Y_{d}\\) (po_transform argument) to the fair_decisions() function:\n\n# fit the outcome control model\nresp_oc &lt;- fair_decisions(\n  data, X = X, Z = Z, W = W, Y = Y, D = D, x0 = 0, x1 = 1,\n  po_transform = function(x) ifelse(x &lt; 97, -(x-97)^2, 0), po_diff_sign = 1\n)\n\nNote that in faircause 0.2.0 the columns of the input data to functions fair_decisions() and fair_predictions() need to be either numeric or integer. Use one-hot encoding wherever appropriate.\nWhen constructing fair decisions, xgboost is first used to estimate the conditional average treatment effect (CATE) of \\(D\\) on \\(Y\\), labeled \\(\\Delta\\), also referred to as benefit:\n\\[\\begin{align}\n\\Delta = E[Y_{d_1} - Y_{d_0} \\mid x, z, w].\n\\end{align}\\]\nIn the above, resp_oc is S3-class object of type fair_decision. We can use the autoplot() generic to analyze various important aspects of the decision making process:\n\n# inspect the decomposition of D\nautoplot(resp_oc, type = \"decision\")\n\n\n\n\n\n\n\n\n\n# inspect the decomposition of Delta\nautoplot(resp_oc, type = \"delta\")\n\n\n\n\n\n\n\n\n\n# inspect benefit fairness\nautoplot(resp_oc, type = \"benefit_fairness\")\n\n\n\n\n\n\n\n\nFurther generics can be applied to fair_decision objects, such as predict(), which allows us to get estimates of the benefit \\(\\Delta\\) on some test data, and also find the optimal policy that satisfies benefit fairness:\n\n# make predictions on test\ntest_data &lt;- data\nfair_dec &lt;- predict(resp_oc, newdata = test_data, budget = 0.2)\n\n# inspect the estimate benefit \\Delta\nhead(fair_dec$delta, n = 100L)\n\n  [1] 8.017566e-01 3.724638e-01 5.191853e+01 0.000000e+00 6.212983e+00\n  [6] 4.203570e+00 1.008966e+01 1.269065e-01 0.000000e+00 3.286853e-01\n [11] 1.251144e+00 5.440242e+00 2.653410e+00 0.000000e+00 3.287728e-01\n [16] 6.071516e-01 4.757139e-01 0.000000e+00 1.529186e+00 2.971644e-01\n [21] 8.352732e-01 3.049420e-01 0.000000e+00 1.316464e-01 9.029436e-01\n [26] 2.170649e-02 2.222031e+00 6.038357e+00 1.162410e+00 9.170604e+00\n [31] 1.005439e+01 1.413266e+01 3.665277e-01 0.000000e+00 0.000000e+00\n [36] 3.129165e+00 4.450426e+00 1.166478e-01 0.000000e+00 2.907858e-01\n [41] 7.480980e-04 0.000000e+00 1.167838e+00 1.495915e+00 0.000000e+00\n [46] 0.000000e+00 0.000000e+00 8.373803e-01 1.013286e+01 0.000000e+00\n [51] 6.888062e-06 9.781933e-01 5.623761e+00 4.914779e+00 1.095073e+01\n [56] 2.950802e-05 1.074799e+01 2.525700e-01 4.529369e+00 2.937705e+01\n [61] 5.786662e-04 0.000000e+00 0.000000e+00 4.605885e+00 4.780608e+00\n [66] 6.791864e+00 1.930003e+01 0.000000e+00 5.194264e+00 2.610412e+00\n [71] 0.000000e+00 0.000000e+00 4.935268e-03 1.658654e+00 8.200450e-01\n [76] 9.657683e-02 6.761817e+00 1.841890e+00 2.859673e+00 0.000000e+00\n [81] 9.752755e-02 0.000000e+00 4.548055e-01 1.569106e+00 1.290448e+00\n [86] 2.937638e-01 1.533631e+01 5.638602e+00 1.379101e+01 0.000000e+00\n [91] 0.000000e+00 0.000000e+00 6.255346e+00 9.792346e-02 0.000000e+00\n [96] 1.116764e+00 9.761857e+00 2.054724e+00 3.707430e-01 4.080177e+00\n\n# inspect the allocated decisions\nhead(fair_dec$decision, n = 100L)\n\n  [1] 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0 0\n [38] 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0\n [75] 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0"
  },
  {
    "objectID": "index.html#want-to-learn-more-about-causal-fairness-analysis",
    "href": "index.html#want-to-learn-more-about-causal-fairness-analysis",
    "title": "Causal Fairness Analysis - Software Tools",
    "section": "Want to learn more about Causal Fairness Analysis?",
    "text": "Want to learn more about Causal Fairness Analysis?\nFor those interested in learning more about CFA, we suggest the following resources:\n\n\nReading the Causal Fairness Analysis paper, found here,\n\n\nFollow the series of lectures on CFA which were part of the COMSW-4775 course at Columbia Computer Science,\n\n\nCheck our ICML 2022 Tutorial.\n\n\nCheck the vignettes on this webpage which demonstrate how to perform Causal Fairness Analysis in practice."
  },
  {
    "objectID": "pages/t2-compas-neural-inproc.html",
    "href": "pages/t2-compas-neural-inproc.html",
    "title": "Task 2: Neural In-Processing on COMPAS",
    "section": "",
    "text": "We first inspect the COMPAS dataset.\n\nknitr::kable(head(data), caption = \"COMPAS dataset.\")\n\n\nCOMPAS dataset.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsex\nage\nrace\njuv_fel\njuv_misd\njuv_other\npriors\ncharge\ntwo_year_recid\nnorthpointe\n\n\n\n\n0\n69\n1\n0\n0\n0\n0\n1\n0\n0\n\n\n0\n34\n1\n0\n0\n0\n0\n1\n1\n0\n\n\n0\n24\n1\n0\n0\n1\n4\n1\n1\n0\n\n\n0\n23\n1\n0\n1\n0\n1\n1\n0\n1\n\n\n0\n43\n1\n0\n0\n0\n2\n1\n0\n0\n\n\n0\n44\n1\n0\n0\n0\n0\n1\n0\n0"
  },
  {
    "objectID": "pages/t2-compas-neural-inproc.html#inspecting-the-compas-dataset",
    "href": "pages/t2-compas-neural-inproc.html#inspecting-the-compas-dataset",
    "title": "Task 2: Neural In-Processing on COMPAS",
    "section": "",
    "text": "We first inspect the COMPAS dataset.\n\nknitr::kable(head(data), caption = \"COMPAS dataset.\")\n\n\nCOMPAS dataset.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsex\nage\nrace\njuv_fel\njuv_misd\njuv_other\npriors\ncharge\ntwo_year_recid\nnorthpointe\n\n\n\n\n0\n69\n1\n0\n0\n0\n0\n1\n0\n0\n\n\n0\n34\n1\n0\n0\n0\n0\n1\n1\n0\n\n\n0\n24\n1\n0\n0\n1\n4\n1\n1\n0\n\n\n0\n23\n1\n0\n1\n0\n1\n1\n0\n1\n\n\n0\n43\n1\n0\n0\n0\n2\n1\n0\n0\n\n\n0\n44\n1\n0\n0\n0\n0\n1\n0\n0"
  },
  {
    "objectID": "pages/t2-compas-neural-inproc.html#constructing-the-sfm",
    "href": "pages/t2-compas-neural-inproc.html#constructing-the-sfm",
    "title": "Task 2: Neural In-Processing on COMPAS",
    "section": "Constructing the SFM",
    "text": "Constructing the SFM\nWe next construct the Standard Fairness Model:\n\n# constructing the SFM\nX &lt;- \"race\"\nZ &lt;- c(\"age\", \"sex\")\nW &lt;- c(\"juv_fel\", \"juv_misd\", \"juv_other\", \"priors\", \"charge\")\nY &lt;- c(\"two_year_recid\")"
  },
  {
    "objectID": "pages/t2-compas-neural-inproc.html#obtaining-fair-predictions",
    "href": "pages/t2-compas-neural-inproc.html#obtaining-fair-predictions",
    "title": "Task 2: Neural In-Processing on COMPAS",
    "section": "Obtaining Fair Predictions",
    "text": "Obtaining Fair Predictions\nFair Predictions can be obtained by passing the data, SFM, and the choice of the business necessity set (BN) to the fair_predictions() function:\n\n# build our own predictor and decompose the true one\nfair_pred &lt;- fair_predictions(data, X, Z, W, Y, x0 = 0, x1 = 1,\n                              BN = c(\"IE\"))\n\nNote that in faircause 0.2.0 the columns of the input data to functions fair_predictions() and fair_decisions() need to be either numeric or integer. Use one-hot encoding wherever appropriate.\nWhen fitting the fair predictor, pytorch is used under the hood to optimize the function of the form: \\[\n\\begin{align}\nL(y, \\hat{y}) + \\lambda \\Big( | \\text{NDE}_{x_0, x_1}(\\hat{y}) - \\eta_1 | + | \\text{NIE}_{x_1, x_0}(\\hat{y}) - \\eta_2 | + | \\text{Exp-SE}_{x_1}(\\hat{y}) - \\eta_3 | + | \\text{Exp-SE}_{x_0}(\\hat{y}) - \\eta_4 | \\Big)\n\\end{align}\n\\] where the values \\(\\eta_1, \\eta_2, \\eta_3, \\eta_4\\) are either 0 if effect is not in the business necessity set, or equal the effect estimate for the true outcome \\(Y\\) (if the effect is BN). Multiple values of the tuning parameter \\(\\lambda\\) are used at the same time (this choice can also be adjusted using the lmbd_seq parameter, to which an arbitrary sequence of values can be passed). For binary classification the loss \\(L(y, \\hat{y})\\) is the cross-entropy, whereas for regression the loss is the mean squared error (MSE).\nIn the above, fair_pred is S3-class object of type fair_prediction. We can use the predict() function to obtain its predictions on new data.\n\npreds &lt;- predict(fair_pred, data)\n# appending the in-sample predictions to the dataset\ndata$faircause_preds &lt;- preds$predictions[[5]]\n\nFurther generics can be applied to fair_prediction objects, such as autoplot(), which allows us to visualize the loss function and the causal fairness measures on held-out evaluation data:\n\ncowplot::plot_grid(\n  autoplot(fair_pred, type = \"causal\"),\n  autoplot(fair_pred, type = \"accuracy\"), ncol = 1L\n)"
  },
  {
    "objectID": "pages/t2-compas-neural-inproc.html#decomposing-the-disparity-on-evaluation-data",
    "href": "pages/t2-compas-neural-inproc.html#decomposing-the-disparity-on-evaluation-data",
    "title": "Task 2: Neural In-Processing on COMPAS",
    "section": "Decomposing the Disparity on Evaluation Data",
    "text": "Decomposing the Disparity on Evaluation Data\nThe function fair_prediction() takes an eval_prop argument which determines the proportion of data that is used as an evaluation fold (for early stopping when fitting the neural network). We now extract this fold of the data, and decompose the disparity manually:\n\n# decompose the predictions on the eval set\ntrain_idx &lt;- seq_len(round(0.75 * nrow(data)))\nfaircause_decomp &lt;- fairness_cookbook(data[-train_idx,], X = X, W = W, Z = Z,\n                                      Y = \"faircause_preds\",\n                                      x0 = 0, x1 = 1, nboot1 = 20, nboot2=100)\n\nFinally, we can plot the two decompositions, of the true outcome \\(Y\\) and the newly constructed causally fair predictor \\(\\widehat{Y}^{FC}\\), together:"
  },
  {
    "objectID": "pages/t1-icu-mortality.html",
    "href": "pages/t1-icu-mortality.html",
    "title": "Task 1: MIMIC-IV Hospitality Mortality after ICU Admission",
    "section": "",
    "text": "We begin by loading and inspecting the required data from MIMIC-IV:\n\n# load the mortality data\npatient_ids &lt;- id_col(load_concepts(\"adm_episode\", \"miiv\", \n                                    verbose = FALSE)[adm_episode == 1])\ndat &lt;- load_concepts(c(\"acu_24\", \"diag\", \"age\", \"sex\", \"charlson\",\n                       \"lact_24\", \"pafi_24\", \"ast_24\",\n                       \"race\", \"death\"), \"miiv\", patient_ids = patient_ids,\n                     verbose = FALSE)\ndat &lt;- dat[race %in% c(\"Caucasian\", \"African American\")]\ndat[, c(index_var(dat)) := NULL]\nimp_lst &lt;- list(\n  age = 65,\n  acu_24 = 0,\n  charlson = 0,\n  lact_24 = 1,\n  ast_24 = 20,\n  pafi_24 = 500,\n  death = FALSE\n)\n\nfor (i in seq_len(ncol(dat))) {\n\n  var &lt;- names(dat)[i]\n  if (any(is.na(dat[[var]])) & !is.null(imp_lst[[var]]))\n    dat[is.na(get(var)), c(var) := imp_lst[[var]]]\n}\n\nknitr::kable(head(dat), caption = \"MIMIC-IV Mortality data.\")\n\n\nMIMIC-IV Mortality data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstay_id\nacu_24\ndiag\nage\nsex\ncharlson\nlact_24\npafi_24\nast_24\nrace\ndeath\n\n\n\n\n30000153\n3\nTRAUM\n61\nMale\n0\n1.7\n430.0000\n20\nCaucasian\nFALSE\n\n\n30001396\n3\nMED\n40\nMale\n1\n1.8\n361.9048\n23\nCaucasian\nFALSE\n\n\n30001446\n11\nMED\n56\nMale\n1\n1.7\n466.6667\n114\nCaucasian\nFALSE\n\n\n30001656\n3\nNMED\n68\nFemale\n0\n1.0\n500.0000\n20\nCaucasian\nFALSE\n\n\n30001947\n2\nSURG\n43\nMale\n1\n0.7\n535.0000\n36\nCaucasian\nFALSE\n\n\n30002415\n4\nCSURG\n72\nFemale\n0\n1.0\n240.0000\n20\nCaucasian\nFALSE\n\n\n\n\n\nWe consider the cohort of all patients in the database admitted to the ICU (and we only consider the first admission of each patient). We also load information on the SOFA score, admission diagnosis, age, sex, Charlson comorbidity index, worst values of lactate, PaO2/FiO2, and AST in first 24 hours. We further load the race information (protected attribute \\(X\\)) and the death indicator (outcome \\(Y\\)). We want to investigate the disparities in outcome between \"Caucasian\" and \"African American\" groups."
  },
  {
    "objectID": "pages/t1-icu-mortality.html#inspecting-the-data-from-mimic-iv-dataset",
    "href": "pages/t1-icu-mortality.html#inspecting-the-data-from-mimic-iv-dataset",
    "title": "Task 1: MIMIC-IV Hospitality Mortality after ICU Admission",
    "section": "",
    "text": "We begin by loading and inspecting the required data from MIMIC-IV:\n\n# load the mortality data\npatient_ids &lt;- id_col(load_concepts(\"adm_episode\", \"miiv\", \n                                    verbose = FALSE)[adm_episode == 1])\ndat &lt;- load_concepts(c(\"acu_24\", \"diag\", \"age\", \"sex\", \"charlson\",\n                       \"lact_24\", \"pafi_24\", \"ast_24\",\n                       \"race\", \"death\"), \"miiv\", patient_ids = patient_ids,\n                     verbose = FALSE)\ndat &lt;- dat[race %in% c(\"Caucasian\", \"African American\")]\ndat[, c(index_var(dat)) := NULL]\nimp_lst &lt;- list(\n  age = 65,\n  acu_24 = 0,\n  charlson = 0,\n  lact_24 = 1,\n  ast_24 = 20,\n  pafi_24 = 500,\n  death = FALSE\n)\n\nfor (i in seq_len(ncol(dat))) {\n\n  var &lt;- names(dat)[i]\n  if (any(is.na(dat[[var]])) & !is.null(imp_lst[[var]]))\n    dat[is.na(get(var)), c(var) := imp_lst[[var]]]\n}\n\nknitr::kable(head(dat), caption = \"MIMIC-IV Mortality data.\")\n\n\nMIMIC-IV Mortality data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstay_id\nacu_24\ndiag\nage\nsex\ncharlson\nlact_24\npafi_24\nast_24\nrace\ndeath\n\n\n\n\n30000153\n3\nTRAUM\n61\nMale\n0\n1.7\n430.0000\n20\nCaucasian\nFALSE\n\n\n30001396\n3\nMED\n40\nMale\n1\n1.8\n361.9048\n23\nCaucasian\nFALSE\n\n\n30001446\n11\nMED\n56\nMale\n1\n1.7\n466.6667\n114\nCaucasian\nFALSE\n\n\n30001656\n3\nNMED\n68\nFemale\n0\n1.0\n500.0000\n20\nCaucasian\nFALSE\n\n\n30001947\n2\nSURG\n43\nMale\n1\n0.7\n535.0000\n36\nCaucasian\nFALSE\n\n\n30002415\n4\nCSURG\n72\nFemale\n0\n1.0\n240.0000\n20\nCaucasian\nFALSE\n\n\n\n\n\nWe consider the cohort of all patients in the database admitted to the ICU (and we only consider the first admission of each patient). We also load information on the SOFA score, admission diagnosis, age, sex, Charlson comorbidity index, worst values of lactate, PaO2/FiO2, and AST in first 24 hours. We further load the race information (protected attribute \\(X\\)) and the death indicator (outcome \\(Y\\)). We want to investigate the disparities in outcome between \"Caucasian\" and \"African American\" groups."
  },
  {
    "objectID": "pages/t1-icu-mortality.html#constructing-the-sfm",
    "href": "pages/t1-icu-mortality.html#constructing-the-sfm",
    "title": "Task 1: MIMIC-IV Hospitality Mortality after ICU Admission",
    "section": "Constructing the SFM",
    "text": "Constructing the SFM\nWe next construct the Standard Fairness Model, with also a decision \\(D\\):\n\n# constructing the SFM\nX &lt;- \"race\"\nZ &lt;- c(\"age\", \"sex\")\nW &lt;- c(\"acu_24\", \"diag\", \"charlson\", \"lact_24\", \"pafi_24\", \"ast_24\")\nY &lt;- \"death\""
  },
  {
    "objectID": "pages/t1-icu-mortality.html#decomposing-the-disparity",
    "href": "pages/t1-icu-mortality.html#decomposing-the-disparity",
    "title": "Task 1: MIMIC-IV Hospitality Mortality after ICU Admission",
    "section": "Decomposing the Disparity",
    "text": "Decomposing the Disparity\n\nfcb &lt;- fairness_cookbook(\n  data = dat, X = X, Z = Z, W = W, Y = Y,\n  x1 = \"Caucasian\", x0 = \"African American\"\n)\n\n2.10843373493976% of extreme P(x | z) or P(x | z, w) probabilities.\nEstimates likely biased.\n\n\nWe can then inspect the decomposition by calling autoplot() on the fcb object which is an S3 class of type faircause:\n\nautoplot(fcb) +\n  labs(title=\"Causal decomposition of mortality difference: MIMIC-IV race effect\",\n       y=\"Mortality difference (%)\") +\n  theme_minimal() +\n  scale_x_discrete(labels = c(\"Total Variation\", \"Direct\", \"Indirect\",\n                                \"Confounded\"), name = \"Pathway\") +\n  scale_fill_discrete(labels = c(\"Total Variation\", \"Direct\", \"Indirect\",\n                                 \"Confounded\"), name = \"Pathway\")"
  },
  {
    "objectID": "pages/t1-icu-mortality.html#zooming-in-on-the-spurious-effect",
    "href": "pages/t1-icu-mortality.html#zooming-in-on-the-spurious-effect",
    "title": "Task 1: MIMIC-IV Hospitality Mortality after ICU Admission",
    "section": "Zooming-in on the Spurious Effect",
    "text": "Zooming-in on the Spurious Effect\nTo better understand the spurious effect, we plot the density of the age distributions between groups:"
  }
]