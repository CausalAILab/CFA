[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Causal Fairness Analysis - Software Tools",
    "section": "",
    "text": "This page contains a sequence of vignettes associated with the Causal Fairness Analysis paper (Plecko and Bareinboim 2022). In particular, here you can find all the code used to reproduce the results of the paper, formatted in a vignette-style fashion. There are also additional examples of analyses, which are not presented in the manuscript. In particular, you can find the following:\n\nA: Vignettes directly reproducing the content of the manuscript\n\nCensus dataset analysis from Example 5.1 of (Plecko and Bareinboim 2022, sec. 5.1),\nCollege Admissions Bias Quantification over Time from Example 5.2 of (Plecko and Bareinboim 2022, sec. 5.1),\nAnalysis of fair predictions on the COMPAS dataset from Example 5.3 of (Plecko and Bareinboim 2022, sec. 5.2),\nCOMPAS dataset analysis from Example 6.1 of (Plecko and Bareinboim 2022, chap. 6),\nConditional independence tests on COMPAS & Census datasets from Figure 4.7 of (Plecko and Bareinboim 2022, sec. 4.5).\n\n\n\nB: Vignettes not directly reproducing the content of the manuscript:\n\nTV & EO analysis on COMPAS dataset,\nFair predictions using Fair Data Adaptation.\n\nTo learn more about Causal Fairness Analysis, visit https://fairness.causalai.net/ where you can access a 2-hour Tutorial on the topic, which was presented in the International Conference on Machine Learning (ICML) 2022.\n\n\n\n\n\nReferences\n\nPlecko, Drago, and Elias Bareinboim. 2022. “Causal Fairness Analysis.” arXiv Preprint arXiv:2207.11385."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "pages/shai-challenge.html",
    "href": "pages/shai-challenge.html",
    "title": "COMPAS Analysis - A Causal Approach",
    "section": "",
    "text": "Across United States, courts are using algorithms to predict which of the defendants are likely to recidivate and re-offend. As is becoming apparent in the literature on fair machine learning, algorithms do not have any ethical or moral values a priori, and they are capable of learning, or even amplifying the existing societal bias. If left unattended, such algorithms could lead to the perpetuation of unfairness, raising a serious concern about the impact of AI on socially important questions in the long term.\nIn their seminal work from 2016 (Larson et al. 2016), the team from ProPublica (an independent, non-profit newsroom), analyzed data from a commercial tool called Correctional Offender Management Profiling for Alternative Sanctions (COMPAS), which was developed by Northpointe, Inc. (the company is today known as Equivant). The analysis was performed on a large number of individuals from Broward County, Florida, and compared the recidivism predictions produced by COMPAS with those that actually occurred in practice. The findings of their analysis sent alarm bells ringing to everyone concerned about issues of racial injustice. In particular, the ProPublica team demonstrated, among other things, that:\n\nblack defendants who did not recidivate over a two-year period were nearly twice as likely to be classified as higher risk compared to their white counterparts,\nwhite defendants who did recidivate over a two-year period were labeled as low risk twice as often as black re-offenders.\n\nThe above mentioned disparities observed by ProPublica are a starting point of an important investigation. What their analysis did not investigate is the causal explanation of how the disparities arose. In particular, using the language of causal models, we provide such an explanation as follows. For the observed disparity between the groups, we quantify how much of the disparity can be explained by:\n\nthe direct causal effect of race on the outcome (not explained by other features),\nthe indirect causal effect of race on previous offenses and degree of charge, which in turn influence the outcome,\nthe confounded effect of race, which is associated with age and sex in the dataset, both of which are predictive of outcome.\n\nWe remark here that our causal analysis should be thought of manipulating the ``signals” of race, or its perception, rather than race itself, which is an immutable characteristic of every individual (Weinberger 2022; Greiner and Rubin 2011).\nThe three steps of our analysis are the following: A. we first look at the causal explanation of the two-year recidivism rates \\(Y\\), which will help us understand the causal effects of race in the real world, A. we then look at the causal explanation of the Northpointe’s predictions, which will help us understand the causal effects of the predicted world, A. finally, we look at the causal explanation of the Northpointe’s predictions when subsetting to only the group of individuals who did not recidivate over a two-year period; such an analysis will help us understand how Northpointe’s predictions causally treat individuals who do not re-offend.\nWe now introduce the methodology used in our analysis, which is implemented in the faircause R-package. The package can be installed using:\n\ndevtools::install_github(\"dplecko/CFA\")"
  },
  {
    "objectID": "pages/shai-challenge.html#a-explaining-disparity-in-recidivism",
    "href": "pages/shai-challenge.html#a-explaining-disparity-in-recidivism",
    "title": "COMPAS Analysis - A Causal Approach",
    "section": "A: Explaining disparity in recidivism",
    "text": "A: Explaining disparity in recidivism\nAs our starting point, we first compute the disparity between groups in two-year recidivism:\n\ntapply(data$two_year_recid, data$race, mean)\n\nMajority Minority \n0.393643 0.480042 \n\n\nBased on this information, we can compute that: \\[\\begin{equation}\n  \\text{PG}_{x_0, x_1}(y) = 48\\% - 39.5\\% = 8.5\\%.\n\\end{equation}\\] We then apply the fairness_cookbook() functionality from the R-package, and choose two_year_recid (\\(Y\\)) as the outcome of interest:\n\nX <- \"race\"\nZ <- c(\"age\", \"sex\")\nW <- c(\"juv_fel\", \"juv_misd\", \"juv_other\", \"priors\", \"charge\")\nY <- c(\"two_year_recid\")\ntwo_year <- fairness_cookbook(data, X = X, W = W, Z = Z, Y = Y, \n                              x0 = \"Majority\", x1 = \"Minority\")\ntwo_year\n\nfaircause object:\n\nAttribute:        race \nOutcome:          two_year_recid \nConfounders:      age,sex \nMediators:        juv_fel,juv_misd,juv_other,priors,charge \n\n\nAfter performing the decomposition we can visualize how the decomposition, as shown in Figure 5.\n\n\n\n\n\nFigure 5: Causal decomposition of the parity gap for two-year recidivism.\n\n\n\n\n\n\n\n\n\n\n\n\nTherefore, we have the first crucial result: \\[\n\\begin{equation}\n  \\text{PG}_{x_0, x_1}(y) = \\underbrace{1.5\\%\\;(\\pm 1.8\\%)}_{\\text{race effect}} +  \\underbrace{4\\%\\;(\\pm 0.8\\%)}_{\\text{prior counts effect}} +  \\underbrace{3\\%\\;(\\pm 1.4\\%)}_{\\text{age/sex effect}}.\n\\end{equation}\n\\] Important to note is that race does not have a statistically significant direct effect on outcome."
  },
  {
    "objectID": "pages/shai-challenge.html#b-explaining-disparity-in-northpointes-recidivism-predictions",
    "href": "pages/shai-challenge.html#b-explaining-disparity-in-northpointes-recidivism-predictions",
    "title": "COMPAS Analysis - A Causal Approach",
    "section": "B: Explaining disparity in Northpointe’s recidivism predictions",
    "text": "B: Explaining disparity in Northpointe’s recidivism predictions\nOur next step is to analyze the causal decomposition of the Northpointe’s predictions.\n\ntapply(data$northpointe, data$race, mean)\n\n Majority  Minority \n0.3480033 0.5174370 \n\n\nTherefore, we can compute that: \\[\n\\begin{equation}\n  \\text{PG}_{x_0, x_1}(\\hat{y}) = 52\\% - 35\\% = 17\\%.\n\\end{equation}\n\\] Firstly, we notice that the disparity in the predicted outcome is larger than in the true outcome. But crucially, the question is how this disparity in predicted outcome arises from a causal point of view. We again apply the fairness_cookbook(), this time choosing northpointe (\\(\\hat{Y}\\)) as the outcome of interest:\n\nYhat <- \"northpointe\"\nnorth_decompose <- fairness_cookbook(data, X = X, W = W, Z = Z, Y = Yhat, \n                                     x0 = \"Majority\", x1 = \"Minority\")\nautoplot(north_decompose, decompose = \"xspec\", signed = FALSE) +\n  ggtitle(TeX(\"$PG_{x_0, x_1}(\\\\hat{y})$ decomposition\"))\n\n\n\n\nFigure 6: Causal decomposition of the parity gap for Northpointe’s recidivism prediction.\n\n\n\n\nIn particular, we obtain that: \\[\n\\begin{equation}\n  \\text{PG}_{x_0, x_1}(\\hat{y}) = \\underbrace{5.5\\% \\;(\\pm 1.75\\%)}_{\\text{race effect}} +  \\underbrace{7.5\\% \\;(\\pm 1.2\\%)}_{\\text{prior counts effect}} +  \\underbrace{3\\%\\;(\\pm 1.5\\%)}_{\\text{age/sex effect}}.\n\\end{equation}\n\\] The decomposition is also visualized in Figure 6. Crucially, we find that race does have a statistically significant direct effect on Northpointe’s predictions. Furthermore, race by itself explains one third of the observed disparity."
  },
  {
    "objectID": "pages/shai-challenge.html#c-explaining-disparity-in-northpointes-recidivism-predictions-for-those-who-did-not-recidivate",
    "href": "pages/shai-challenge.html#c-explaining-disparity-in-northpointes-recidivism-predictions-for-those-who-did-not-recidivate",
    "title": "COMPAS Analysis - A Causal Approach",
    "section": "C: Explaining disparity in Northpointe’s recidivism predictions for those who did not recidivate",
    "text": "C: Explaining disparity in Northpointe’s recidivism predictions for those who did not recidivate\nIn the final step, we again perform a decomposition of a disparity, but this time within the group of individuals who did not recidivate. Similarly as before, we can distinguish the direct, indirect, and spurious effects in this disparity. However, the quantity we are decomposing is not the parity gap, but the error rate related to the equality of opportunity criterion, i.e., \\[\n\\begin{equation}\n\\text{ER}_{x_0, x_1}(\\hat{y} | y = 0) = P(\\hat{Y} = 1 \\mid X = x_1, Y = 0) - P(\\hat{Y} = 1 \\mid X = x_0, Y = 0).\n\\end{equation}\n\\] In words, we compare the proportion of minority individuals who are flagged as high risk, but did not recidivate, to the proportion of majority individuals who are flagged as high risk but did not recidivate. We can compute this as:\n\nno_recid <- data$two_year_recid == 0\ntapply(data$northpointe[no_recid], data$race[no_recid], mean)\n\n Majority  Minority \n0.2345430 0.3769697 \n\n\nTherefore, we obtained that: \\[\n\\begin{equation}\n\\text{ER}_{x_0, x_1}(\\hat{y} \\mid y = 0) = 38\\% - 23\\% = 15\\%.\n\\end{equation}\n\\] In words, from defendants who do no recidivate, the minority group defendants are 15% more likely to be labeled as high risk. Once again, we can obtain a causal decomposition, but this time of the \\(\\text{ER}_{x_0, x_1}(\\hat{y} \\mid y = 0)\\) measures. For this purpose, we use the fairness_cookbook_eo() functionality:\n\neo_decompose <- fairness_cookbook_eo(data, X = X, W = W, Z = Z, Y = Y,\n                            Yhat = Yhat, x0 = \"Majority\", x1 = \"Minority\",\n                            ylvl = 0)\nautoplot(eo_decompose, decompose = \"xspec\", signed = FALSE, eo = TRUE)\n\n\n\n\nFigure 7: Causal decomposition of the error rate for the group of non-recidivists.\n\n\n\n\nThe visualization of the decomposition is shown in Figure 7 and can be written mathematically as: \\[\\begin{equation}\n  \\text{ER}_{x_0, x_1}(\\hat{y} \\mid y = 0) = \\underbrace{6.8\\% \\;(\\pm 2\\%)}_{\\text{race effect}} +  \\underbrace{5\\% \\;(\\pm 1.3\\%)}_{\\text{prior counts effect}} +  \\underbrace{3\\%\\;(\\pm 2\\%)}_{\\text{age/sex effect}}.\n\\end{equation}\\] We conclude that there is a direct effect of race on the prediction even within the group of individuals who do not recedivate. Race explains more than one third of the observed disparity in the group."
  },
  {
    "objectID": "pages/shai-challenge.html#key-findings",
    "href": "pages/shai-challenge.html#key-findings",
    "title": "COMPAS Analysis - A Causal Approach",
    "section": "Key Findings",
    "text": "Key Findings\nThe analysis presented in this submission consists of three steps, with each of the steps demonstrating a specific causal explanation. The first step of the analysis demonstrates that the direct effect of race is not statistically significant when analyzing the true recidivism rates. In stark contrast, the second analysis shows that for the predictions produced by Northpointe, the direct effect of race is statistically significant, explaining almost a third of the overall racial disparity. Finally, in the third analysis, we show that the direct effect of race on Northpointe’s predictions was significant even in the group of individuals who did not re-offend.\nTherefore, the absence of the direct effect in the true, observed outcome, and its presence in the predictions for both the overall dataset and the group of non-recidivists, indicates that the predictions produced by Northpointe are strongly racially prejudiced, and constitute a serious mistreatment of the racial minority groups."
  },
  {
    "objectID": "pages/shai-challenge.html#legal-implications",
    "href": "pages/shai-challenge.html#legal-implications",
    "title": "COMPAS Analysis - A Causal Approach",
    "section": "Legal Implications",
    "text": "Legal Implications\nWe quickly discuss some of the possible legal implications of the findings summarized above. Firstly, direct discrimination is explicitly considered in some provisions of the US legal system. For example, under the Title VII of the Civil Rights Act of 1964 (Act 1964a), disparate treatment of individuals based on race is strictly prohibited. Another example is the Fair Housing Act (Act 1964b), which also prohibits direct racial discrimination.\nInterestingly, recent works in legal scholarship have started to turn to interpreting the disparate treatment doctrine in causal language (Plecko and Bareinboim 2022), in particular looking at the direct effect of the protected attribute. The analysis proposed in this work also gives a causal explanation of how the overall disparity came about, and follows from a more broad framework for causal fairness analysis, which was designed with the intention of interpreting the legal doctrines of disparate treatment and disparate impact.\nHowever, the doctrines of disparate treatment and disparate impact would likely not be explicitly considered in a legal proceeding on the COMPAS tool. Nonetheless, the absence of a direct effect of race on the true observed outcome (Experiment A) and its presence in Northpointe’s predictions (Experiments B, C) indicate a clear issue and provide prima facie evidence of intentional racial discrimination, which is prohibited by the Equal Protection clause of the 14th Amendment of the United States Constitution.\nIn conclusion, our causal analysis of the COMPAS tool explains the disparities observed in the data, and demonstrates a clear direct effect of race on predictions of recidivism. As discussed above, such findings likely constitute a basis for legal action against a discriminatory practice from the software provider that produced the predictions."
  },
  {
    "objectID": "pages/college-admissions-t1.html",
    "href": "pages/college-admissions-t1.html",
    "title": "College Admissions: Task 1 over Time",
    "section": "",
    "text": "In this vignette, we analyze a synthetic College Admission dataset, to demonstrate how the task of bias detection could be performed over time, in a university. In particular, we have the following story.\nA university in the United States admits applicants every year. The university is interested in quantifying discrimination in the admission process and track it over time, between 2010 and 2020. The data generating process changes over time, and can be described as follows. Let \\(X\\) denote gender (\\(x_0\\) female, \\(x_1\\) male). Let \\(Z\\) be the age at time of application (\\(Z = 0\\) under 20 years, \\(Z = 1\\) over 20 years) and let \\(W\\) denote the department of application (\\(W = 0\\) for arts&humanities, \\(W = 1\\) for sciences). Finally, let \\(Y\\) denote the admission decision (\\(Y = 0\\) rejection, \\(Y = 1\\) acceptance). The application process changes over time and is given by \\[\n\\begin{empheq}[left ={\\mathcal{F}(t), P(U): \\empheqlbrace}]{align}\n                X & \\gets \\mathbb{1} ( U_X < 0.5 + 0.1U_{XZ})\\label{eq:fcb2-x}\\\\\n                Z & \\gets \\mathbb{1} (U_Z < 0.5 + \\kappa(t) U_{XZ})  \\\\\n                W & \\gets \\mathbb{1} (U_W < 0.5 + \\lambda(t) U_{XZ})  \\\\\n                Y & \\gets \\mathbb{1} (U_Y < 0.1 + \\alpha(t)X + \\beta(t)W + 0.1Z). \\label{eq:fcb2-y} \\\\\n                \\nonumber\\\\\n                    U_{XZ} &\\in \\{0,1\\}, P(U_{XZ} = 1) = 0.5, \\\\\n                    U_X &, U_Z, U_W, U_Y \\sim \\text{Unif}[0, 1].       \n\\end{empheq}\n\\] The coefficients \\(\\kappa(t), \\lambda(t), \\alpha(t), \\beta(t)\\) change every year, and obey the following dynamics: \\[\n\\begin{align}\n    \\kappa(t+1) &= 0.9\\kappa(t) \\\\\n    \\lambda(t+1) &= \\lambda(t) (1 - \\beta(t)) \\\\\n    \\beta(t+1) &= \\beta(t) (1 - \\lambda(t)) f(t), f(t) \\sim \\text{Unif}[0.8, 1.2] \\\\\n    \\alpha(t+1) &= 0.8\\alpha(t).\n\\end{align}\n\\] The equations can be interpreted as follows. The coefficient \\(\\kappa(t)\\) decreases over time, meaning that the overall age gap between the groups decreases. The coefficient \\(\\lambda(t)\\) decreases compared to the previous year, by an amount dependent on \\(\\beta(t)\\). In words, the rate of application to arts&humanities departments decreases if these departments have lower overall admission rates (i.e., students are less likely to apply to departments that are hard to get into). Further, \\(\\alpha(t)\\), which represents gender bias, decreases over time. Finally, \\(\\beta(t)\\) represent the increase in the probability of admission when applying to a science department. Its value depends on the value from the previous year, multiplied by \\((1 - \\lambda(t))\\) and the random variable \\(f(t)\\). Multiplication by the former factor describes the mechanism in which the benefit of applying to a science department decreases if a larger proportion of students apply for it. The latter factor describes a random variation over time which describes how well (in relative terms) the science departments are funded, and can be seen as depending on research and market dynamics in the sciences.\nWe now create functions for generating synethetic data as described above:\n\ncol_adm <- function(n, cfs, kap = cfs[[\"kap\"]], lam = cfs[[\"lam\"]],\n                    alf = cfs[[\"alf\"]], bet = cfs[[\"bet\"]]) {\n\n  u_xz <- rbinom(n, size = 1, prob = 0.5)\n  x <- rbinom(n, size = 1, prob = 0.5 + 0.1 * u_xz)\n  z <- rbinom(n, size = 1, prob = 0.5 + kap * u_xz)\n  d <- rbinom(n, size = 1, prob = 0.5 + lam * x)\n  y <- rbinom(n, size = 1, prob = 0.1 + alf * x + bet * d + 0.1 * z)\n\n  data.frame(x, z, d, y)\n\n}\n\ncfs_nxt <- function(cfs, kap = cfs[[\"kap\"]], lam = cfs[[\"lam\"]],\n                    alf = cfs[[\"alf\"]], bet = cfs[[\"bet\"]]) {\n\n  kapt <- kap * 0.9\n  lamt <- lam * (1 - bet)\n  bett <- bet * (1 - lam) * runif(1, 0.8, 1.2)\n  alft <- alf * 0.8\n\n  list(kap = kapt, lam = lamt, alf = alft, bet = bett)\n\n}\n\nThe head data scientist at the university decides to use the Fairness Cookbook for performing bias quantification. The SFM projection of the causal diagram \\(\\mathcal{G}\\) of the dataset is given by \\[\n\\begin{equation}\n        \\Pi_{\\text{SFM}}(\\mathcal{G}) = \\langle X = \\lbrace X \\rbrace,  Z = \\lbrace Z \\rbrace, W = \\lbrace W\\rbrace, Y = \\lbrace Y \\rbrace\\rangle.\n    \\end{equation}\n\\] After that, the analyst estimates the quantities \\[\n\\begin{align}\n    x\\text{-DE}^{\\text{sym}}_{x}(y \\mid x_0), x\\text{-DE}^{\\text{sym}}_{x}(y \\mid x_0), \\text{ and } x\\text{-SE}_{x_1, x_0}(y) \\;\\;\\; \\forall t \\in \\{2010, \\dots, 2020\\}.\n\\end{align}\n\\]\nThe following code generates the data, obtains the ground truth causal fairness measures, and also performs the fairness_cookbook() at each step.\n\ncol_adm_tim <- function(n, cfs) {\n\n  dat <- bqn <- gtr <- list()\n\n  for (t in seq_len(10)) {\n\n    # generate data for given year\n    dat[[t]] <- col_adm(n, cfs)\n\n    # compute decomposition\n    bqn[[t]] <- fairness_cookbook(dat[[t]], X = \"x\", Z = \"z\", W = \"d\", Y = \"y\",\n                                  x0 = 0, x1 = 1)\n\n    # get ground truth\n    gtr[[t]] <- c(cfs$alf, -(cfs$bet * cfs$lam),\n                  0.1 * ( (0.125 + 0.2 * (0.5 + cfs$kap))/(0.45) -\n                             (0.125 + 0.4 * (0.5 + cfs$kap))/(0.55) ))\n\n    # update the coefficients for each year\n    cfs <- cfs_nxt(cfs)\n\n  }\n\n  list(dat = dat, bqn = bqn, gtr = gtr)\n\n}\n\ncfs <- list(kap = 0.3, lam = 0.2, alf = 0.1, bet = 0.3)\n\nset.seed(22)\nres <- col_adm_tim(n = 5000L, cfs = cfs)\n\notp <- c()\nfor (t in seq_along(res[[\"bqn\"]])) {\n\n  x <- res[[\"bqn\"]][[t]]\n  x_tr <- res[[\"gtr\"]][[t]]\n  add_row <- data.frame(Spurious = x$measures$CtfSE[1],\n                        Indirect = x$measures$CtfIE[1],\n                        Direct = x$measures$CtfDE[1],\n                        Spurious_True = x_tr[3],\n                        Indirect_True = x_tr[2],\n                        Direct_True = x_tr[1],\n                        Year = 2010+t)\n\n  otp <- rbind(otp, add_row)\n\n}\n\ndf <- reshape2::melt(otp, id.vars = \"Year\", variable.name = \"Measure\")\ndf$Meas <- gsub(\"_.*\", \"\", df$Measure)\ndf$whc <- ifelse(grepl(\"_\", df$Measure), \"True (population)\",\n                 \"Estimated (from sample)\")\n\nggplot(df, aes(x = Year, y = value, color = Meas, linetype = whc)) +\n  geom_line() + geom_point() + theme_bw() +\n  ggtitle(\"Bias Quantification Over Time - College Admissions\") +\n  ylab(\"Value\") + scale_y_continuous(labels = scales::percent,\n                                     limits = c(-0.1, 0.15)) +\n  scale_x_continuous(breaks = seq(2010, 2020)) +\n  theme(\n    legend.position = \"bottom\",\n    legend.box.background = element_rect(),\n    legend.box = \"horizontal\"\n  ) + scale_linetype_discrete(name = \"Quantity\") +\n  scale_color_discrete(name = \"Effect\")\n\n\n\n\nFigure 1: Tracking bias over time in the synthetic College Admissions dataset from Example 5.2, between years 2010 and 2020. Both the estimated values from simulated samples (solid line) and the true population values (dashed lines) are shown, for direct (red), indirect (green), and spurious (blue) effects.\n\n\n\n\nThe temporal dynamics of the estimated measures of discrimination (together with the ground truth values obtained from the SCM \\(\\mathcal{M}_t\\)) are shown graphically in Figure 1."
  },
  {
    "objectID": "pages/ci-tests.html",
    "href": "pages/ci-tests.html",
    "title": "Conditional Independence Tests - Census & COMPAS",
    "section": "",
    "text": "References\n\nAnand, Tara, Adele Ribeiro, Jin Tian, and Elias Bareinboim. 2021. “Effect Identification in Causal Diagrams with Clustered Variables.”\n\n\nBrimicombe, Allan J. 2007. “Ethnicity, Religion, and Residential Segregation in London: Evidence from a Computational Typology of Minority Communities.” Environment and Planning B: Planning and Design 34 (5): 884–904.\n\n\nDing, Qu Jian, and Therese Hesketh. 2006. “Family Size, Fertility Preferences, and Sex Ratio in China in the Era of the One Child Family Policy: Results from National Family Planning and Reproductive Health Survey.” British Medical Journal Publishing Group.\n\n\nHernandez, Jesus. 2009. “Redlining Revisited: Mortgage Lending Patterns in Sacramento 1930–2004.” International Journal of Urban and Regional Research 33 (2): 291–313.\n\n\nHesketh, Therese, Li Lu, and Zhu Wei Xing. 2005. “The Effect of China’s One-Child Family Policy After 25 Years.” New England Journal of Medicine. Mass Medical Soc.\n\n\nPlecko, Drago, and Elias Bareinboim. 2022. “Causal Fairness Analysis.” arXiv Preprint arXiv:2207.11385.\n\n\nZenou, Yves, and Nicolas Boccard. 2000. “Racial Discrimination and Redlining in Cities.” Journal of Urban Economics 48 (2): 260–85."
  },
  {
    "objectID": "pages/compas-t2.html",
    "href": "pages/compas-t2.html",
    "title": "Task 2: Fair Predictions on COMPAS",
    "section": "",
    "text": "In this vignette, we focus on Task 2 (Fair Prediction), and specifically focus on the limitation of the currently found methods in the literature to provide causally meaningful fair predictions."
  },
  {
    "objectID": "pages/compas-t2.html#fair-predictions-on-compas",
    "href": "pages/compas-t2.html#fair-predictions-on-compas",
    "title": "Task 2: Fair Predictions on COMPAS",
    "section": "Fair Predictions on COMPAS",
    "text": "Fair Predictions on COMPAS\nA team of data scientists from ProPublica have shown that the COMPAS dataset from Broward County contains a strong racial bias against minorities. They are now interested in producing fair predictions \\(\\widehat{Y}\\) on the dataset, to replace the biased predictions. They first load the COMPAS data:\n\n# load the data\ndat <- get(data(\"compas\", package = \"faircause\"))\nknitr::kable(head(dat), caption = \"COMPAS dataset.\")\n\n\nCOMPAS dataset.\n\n\n\n\n\n\n\n\n\n\n\n\n\nsex\nage\nrace\njuv_fel_count\njuv_misd_count\njuv_other_count\npriors_count\nc_charge_degree\ntwo_year_recid\n\n\n\n\nMale\n69\nNon-White\n0\n0\n0\n0\nF\n0\n\n\nMale\n34\nNon-White\n0\n0\n0\n0\nF\n1\n\n\nMale\n24\nNon-White\n0\n0\n1\n4\nF\n1\n\n\nMale\n23\nNon-White\n0\n1\n0\n1\nF\n0\n\n\nMale\n43\nNon-White\n0\n0\n0\n2\nF\n0\n\n\nMale\n44\nNon-White\n0\n0\n0\n0\nM\n0\n\n\n\n\n# load the metadata\nmdat <- get_metadata(\"compas\")\n\nTo produce fair predictions, they first experiment with four different classifiers.\n\n(i) Random Forest without fairness constraints\n\n# Method 1: Random Forest\norg_dat <- dat\norg_dat$two_year_recid <- ranger(two_year_recid ~ ., dat,\n                                 classification = TRUE)$predictions\n\n# Method 1: decompose Total Variation\norg_tvd <- fairness_cookbook(org_dat, mdat[[\"X\"]], mdat[[\"W\"]], mdat[[\"Z\"]],\n                             mdat[[\"Y\"]], mdat[[\"x0\"]], mdat[[\"x1\"]])\norg_plt <- autoplot(org_tvd, decompose = \"xspec\", dataaset = dataset) +\n  xlab(\"\") + ylab(\"\")\n\n\n\n(ii) Logistic regression trained with reweighing (Kamiran and Calders 2012)\n\nimport os\n\nexec(open(os.path.join(r.root, \"py\", \"reweighing_compas.py\")).read())\n\nWARNING:root:No module named 'tempeh': LawSchoolGPADataset will be unavailable. To install, run:\npip install 'aif360[LawSchoolGPA]'\nWARNING:root:No module named 'tensorflow': AdversarialDebiasing will be unavailable. To install, run:\npip install 'aif360[AdversarialDebiasing]'\nWARNING:root:No module named 'tensorflow': AdversarialDebiasing will be unavailable. To install, run:\npip install 'aif360[AdversarialDebiasing]'\n\nYhat_rew = reweigh_and_predict(r.dat, r.dat)\n\n\n# Method 2: Reweighing by Kamiran & Calders\nreweigh_dat <- dat\nreweigh_dat$two_year_recid <- as.vector(py$Yhat_rew)\n\nreweigh_tvd <- fairness_cookbook(\n  reweigh_dat, mdat[[\"X\"]], mdat[[\"W\"]], mdat[[\"Z\"]], mdat[[\"Y\"]], mdat[[\"x0\"]],\n  mdat[[\"x1\"]])\n\nrew_plt <- autoplot(reweigh_tvd, decompose = \"xspec\", dataset = dataset) +\n  xlab(\"\") + ylab(\"\")\n\n\n\n(iii) Fair Reductions approach of (Agarwal et al. 2018)\n\nexec(open(os.path.join(r.root, \"py\", \"reductions_compas.py\")).read())\nYhat_red = reduce_and_predict(r.dat, r.dat, 0.01)\n\n\n# Method 3: Reductions (Agarwal et. al.)\nsource_python(file.path(root, \"py\", paste0(\"reductions_\", dataset, \".py\")))\n\nred_dat <- dat\nred_dat$two_year_recid <- as.vector(py$Yhat_red)\nred_tvd <- fairness_cookbook(\n  red_dat, mdat[[\"X\"]], mdat[[\"W\"]], mdat[[\"Z\"]], mdat[[\"Y\"]], mdat[[\"x0\"]],\n  mdat[[\"x1\"]]\n)\n\nred_plt <- autoplot(red_tvd, decompose = \"xspec\", dataset = dataset) +\n  xlab(\"\") + ylab(\"\")\n\n\n\n(iv) Random Forest with reject-option post-processing (Kamiran, Karim, and Zhang 2012)\n\n# Method 4: reject-option classification\nrjo_prb <- ranger(two_year_recid ~ ., dat, probability = TRUE)$predictions[, 2]\nrjo_dat <- dat\nrjo_dat$two_year_recid <- RejectOption(rjo_prb, rjo_dat$race)\n\nrjo_tvd <- fairness_cookbook(rjo_dat, mdat[[\"X\"]], mdat[[\"W\"]], mdat[[\"Z\"]],\n                             mdat[[\"Y\"]], mdat[[\"x0\"]], mdat[[\"x1\"]])\nrjo_plt <- autoplot(rjo_tvd, decompose = \"xspec\", dataset = dataset) +\n  xlab(\"\") + ylab(\"\")\n\n\n\nAre the methods successful at eliminating discrimination?\nThe fair prediction algorithms used above are intended to set the TV measure to \\(0\\). After constructing these predictors, the ProPublica team made use of the Fairness Cookbook, to inspect the causal implications of the methods. Following the steps of the Fairness Cookbook, the team computes the TV measure, together with the appropriate measures of direct, indirect, and spurious discrimination. We can now visualize these decompositions in Figure 1.\n\n\n\n\n\nFigure 1: Fair Predictions on the COMPAS dataset.\n\n\n\n\nThe ProPublica team notes that all methods substantially reduce the \\(\\text{TV}_{x_0,x_1}(\\widehat{y})\\), however, the measures of direct, indirect, and, spurious effects are not necessarily reduced to \\(0\\), consistent with the Fair Prediction Theorem."
  },
  {
    "objectID": "pages/compas-t2.html#how-to-fix-the-issue",
    "href": "pages/compas-t2.html#how-to-fix-the-issue",
    "title": "Task 2: Fair Predictions on COMPAS",
    "section": "How to fix the issue?",
    "text": "How to fix the issue?\nTo produce causally meaningful fair predictions, we suggest using the fairadapt package (Plečko and Meinshausen 2020; Plečko, Bennett, and Meinshausen 2021). In particular, the package offers a way of removing discrimination which is based on the causal diagram. In this application, we are interested in constructing fair predictions that remove both the direct and the indirect effect. Firstly, we obtain the adjacency matrix representing the causal diagram associated with the COMPAS dataset:\n\nset.seed(2022)\n# load the causal diagram\nmats <- get_mat(dataset)\nadj.mat <- mats[[1L]]\ncfd.mat <- mats[[2L]]\n\ncausal_graph <- fairadapt::graphModel(adj.mat, cfd.mat)\nplot(causal_graph, size = 50)\n\n\n\n\nAfter loading the causal diagram, we perform fair data adaptation:\n\nfdp <- fairadapt::fairadapt(two_year_recid ~ ., prot.attr = \"race\",\n                            train.data = dat, adj.mat = adj.mat)\n\n# obtain the adapted data\nad_dat <- fairadapt:::adaptedData(fdp)\nad_dat$race <- dat$race\n\n# obtain predictions based on the adapted data\nadapt_oob <- ranger(two_year_recid ~ ., ad_dat,\n                    classification = TRUE)$predictions\nad_dat$two_year_recid <- adapt_oob\n\n# decompose the TV for the predictions\ndat.fairadapt <- dat\ndat.fairadapt$two_year_recid <- adapt_oob\n\nfairadapt_tvd <- fairness_cookbook(\n  ad_dat, mdat[[\"X\"]], mdat[[\"W\"]], mdat[[\"Z\"]],\n  mdat[[\"Y\"]], mdat[[\"x0\"]], mdat[[\"x1\"]]\n)\n\n# visualize the decomposition\nfairadapt_plt <- autoplot(fairadapt_tvd, decompose = \"xspec\", dataset = dataset) +\n  xlab(\"\") + ylab(\"\")\n\nfairadapt_plt\n\n\n\n\nFigure 2: Fair Data Adaptation on the COMPAS dataset.\n\n\n\n\nFigure 2 shows how the fairadapt package can be used to provide causally meaningful predictions that remove both direct and indirect effects."
  },
  {
    "objectID": "pages/compas-t1.html",
    "href": "pages/compas-t1.html",
    "title": "COMPAS - Task 1",
    "section": "",
    "text": "Courts in Broward County, Florida use machine learning to predict whether individuals released on parole are at high risk of re-offending within 2 years (\\(Y\\)). The algorithm is based on the demographic information \\(Z\\) (\\(Z_1\\) for gender, \\(Z_2\\) for age), race \\(X\\) (\\(x_0\\) denoting White, \\(x_1\\) Non-White), juvenile offense counts \\(J\\), prior offense count \\(P\\), and degree of charge \\(D\\).\nIn this vignette, we perform the task of bias detection on this dataset. We begin by loading and pre-processing the original data:\n\ndata <- read.csv(file.path(root, \"inst\", \"extdata\",\n                           \"compas-scores-two-years.csv\"))\ncol.keep <- which(\n  names(data) %in% c(\"age\", \"sex\", \"juv_fel_count\",\n                     \"juv_misd_count\", \"juv_other_count\", \"priors_count\",\n                     \"c_charge_degree\", \"race\", \"two_year_recid\", \"decile_score\")\n)\ndata <- data[, col.keep]\ndata$race <- factor(data$race)\nlevels(data$race) <- c(\"Minority\", \"Minority\", \"Majority\", \"Minority\",\n                       \"Minority\", \"Minority\")\ndata$race <- relevel(data$race, \"Majority\")\ncumsum(table(data$decile_score)) / sum(table(data$decile_score))\n\n        1         2         3         4         5         6         7         8 \n0.1996119 0.3300527 0.4336013 0.5401996 0.6345994 0.7234544 0.8055171 0.8764902 \n        9        10 \n0.9469088 1.0000000 \n\n# decile_score > 4 represents high risk (approximately)\ndata$northpointe <- as.integer(data$decile_score > 4)\ndata$decile_score <- NULL\nnames(data) <- gsub(\"_count\", \"\", names(data))\nnames(data)[which(names(data) == \"c_charge_degree\")] <- \"charge\"\n\nknitr::kable(head(data), caption = \"COMPAS dataset.\")\n\n\nCOMPAS dataset.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsex\nage\nrace\njuv_fel\njuv_misd\njuv_other\npriors\ncharge\ntwo_year_recid\nnorthpointe\n\n\n\n\nMale\n69\nMinority\n0\n0\n0\n0\nF\n0\n0\n\n\nMale\n34\nMinority\n0\n0\n0\n0\nF\n1\n0\n\n\nMale\n24\nMinority\n0\n0\n1\n4\nF\n1\n0\n\n\nMale\n23\nMinority\n0\n1\n0\n1\nF\n0\n1\n\n\nMale\n43\nMinority\n0\n0\n0\n2\nF\n0\n0\n\n\nMale\n44\nMinority\n0\n0\n0\n0\nM\n0\n0\n\n\n\n\n\nIn Causal Fairness Analysis, we are interested in decomposing the TV measure (also known as the parity gap), into its direct, indirect, and spurious components. We show the causal diagram associated with the data, and also a representation of how the target effects can be visualized as follows:\n\n\n\n\n\n\nFigure 1: COMPAS Causal Diagram\n\n\n\n\n\n\n\nFigure 2: Direct effect visualization.\n\n\n\n\n\n\n\n\n\nFigure 3: Indirect effect visualization.\n\n\n\n\n\n\n\nFigure 4: Confounded effect visualization.\n\n\n\n\n\nAfter obtaining the data, we then specify the Standard Fairness Model, and decompose the TV measure for the true outcome \\(Y\\):\n\nX <- \"race\"\nZ <- c(\"age\", \"sex\")\nW <- c(\"juv_fel\", \"juv_misd\", \"juv_other\", \"priors\", \"charge\")\nY <- c(\"two_year_recid\")\ntwo_year <- fairness_cookbook(data, X = X, W = W, Z = Z, Y = Y,\n                              x0 = \"Majority\", x1 = \"Minority\")\n\nautoplot(two_year, decompose = \"xspec\") + \n  ggtitle(TeX(\"$Y$ disparity decomposition COMPAS\"))\n\n\n\n\nCausal decomposition of the TV measure for two-year recidivism.\n\n\n\n\nHowever, we are also interested in the disparity for the predictor \\(\\widehat{Y}\\), so we can decompose the TV measure for the predictor, too:\n\nYhat <- \"northpointe\"\nnorth_decompose <- fairness_cookbook(data, X = X, W = W, Z = Z, Y = Yhat,\n                                     x0 = \"Majority\", x1 = \"Minority\")\nautoplot(north_decompose, decompose = \"xspec\") +\n  ggtitle(TeX(\"$\\\\widehat{Y}$ disparity decomposition COMPAS\"))\n\n\n\n\nCausal decomposition of the TV measure for Northpointe’s predictions.\n\n\n\n\nTo perform the complete analysis, we plot the two results side-by-side, and shade the areas depending on whether the associated measure is included in the business necessity set or not:\n\n\n\n\n\nCausal decompositions of the TV measure for the true and predicted outcomes visualized together."
  },
  {
    "objectID": "pages/compas-t1.html#compas-under-business-necessity",
    "href": "pages/compas-t1.html#compas-under-business-necessity",
    "title": "COMPAS - Task 1",
    "section": "COMPAS under Business Necessity",
    "text": "COMPAS under Business Necessity\nThe courts at Broward County, Florida, were using machine learning to predict whether individuals released on parole are at high risk of re-offending within 2 years. The algorithm is based on the demographic information \\(Z\\) (\\(Z_1\\) for gender, \\(Z_2\\) for age), race \\(X\\) (\\(x_0\\) denoting White, \\(x_1\\) Non-White), juvenile offense counts \\(J\\), prior offense count \\(P\\), and degree of charge \\(D\\).\n\n\n\n\n\nFigure 1: Causal diagram of the COMPAS dataset. Business Necessity variables highlighted in green.\n\n\n\n\nA causal analysis using the fairness_cookbook() revealed that:\n\ndata <- get(data(\"compas\", package = \"faircause\"))\nset.seed(2022)\nmdata <- SFM_proj(\"compas\")\nfc_compas <- fairness_cookbook(data, X = mdata$X, Z = mdata$Z, W = mdata$W,\n                               Y = mdata$Y, x0 = mdata$x0, x1 = mdata$x1)\n\nautoplot(fc_compas, decompose = \"xspec\", dataset = \"COMPAS\")\n\n\n\n\nFigure 2: Fairness Cookbook on the COMPAS dataset.\n\n\n\n\nThat is, the computed measures equal: \\[\n\\begin{align}\n  \\text{Ctf-IE}_{x_1, x_0}(y\\mid x_1) &= -5.7\\% \\pm 0.5\\%,\\\\\n  \\text{Ctf-SE}_{x_1, x_0}(y) &= -4.0\\% \\pm 0.9\\%,\n\\end{align}\n\\] and are also shown graphically in Figure 2, potentially indicating presence of disparate impact. Based on this information, a legal team of ProPublica filed a lawsuit to the district court, claiming discrimination w.r.t. the Non-White subpopulation based on the doctrine of disparate impact. After the court hearing, the judge rules that using the attributes age (\\(Z_2\\)), prior count (\\(P\\)), and charge degree (\\(D\\)) is not discriminatory, but using the attributes juvenile count (\\(J\\)) and gender (\\(Z_1\\)) is discriminatory. The causal diagram with a visualization of which variables are included in the business-necessity set is given in Figure 1. Data scientists at ProPublica need to consider how to proceed in the light of this new requirement for discounting the allowable attributes in the quantiative analysis.\nThe difficulty in this example is that the quantity Ctf-SE\\(_{x_1, x_0}(y)\\) measures the spurious discrimination between the attribute \\(X\\) and outcome \\(Y\\) as generated by both confounders \\(Z_1\\) and \\(Z_2\\). Since using the confounder \\(Z_2\\) is not considered discriminatory, but using the confounder \\(Z_1\\) is, the quantity Ctf-SE\\(_{x_1, x_0}(y)\\) needs to be refined such that the spurious variations based on the different confounders are disentangled. In particular, one might be interested in finding a decomposition of the spurious effect such that \\[\n\\begin{align}\n    \\text{Ctf-SE}_{x_1, x_0}(y) = \\underbrace{\\text{Ctf-SE}^{Z_1}_{x_1, x_0}(y)}_{\\text{gender variations}} + \\underbrace{\\text{Ctf-SE}^{Z_2}_{x_1, x_0}(y)}_{\\text{age variations}},\n\\end{align}\n\\] which would allow the data analyst to further distinguish the variations explained by each of the confounders. A similar challenge is present for the Ctf-IE\\(_{x_1, x_0}(y\\mid x_1)\\) measure, since it has contributions explained by juvenile offense counts \\(J\\), prior counts \\(P\\), and the charge degree \\(D\\). Therefore, we might be interested in decomposing the indirect effect into \\[\n\\begin{align}\n    \\text{Ctf-IE}_{x_1, x_0}(y \\mid x_1) =& \\underbrace{\\text{Ctf-IE}^J_{x_1, x_0}(y \\mid x_1)}_{\\text{juvenile count variations}} + \\underbrace{\\text{Ctf-IE}^P_{x_1, x_0}(y \\mid x_1)}_{\\text{prior count variations}} \\\\ &+ \\underbrace{\\text{Ctf-IE}^D_{x_1, x_0}(y \\mid x_1)}_{\\text{charge degree variations}}. \\nonumber\n\\end{align}\n\\] Again, such a decomposition would allow the data analyst to better understand the contribution of each of the mediators to the totality of the indirect effect. In situations when some mediating variables are in the business necessity set, while others are not, such a decomposition would allow for assessment of disparate impact claims.\n\nTowards the solution"
  },
  {
    "objectID": "pages/census-t1.html",
    "href": "pages/census-t1.html",
    "title": "Census Dataset Analysis",
    "section": "",
    "text": "A data scientist loads the data and performs the following initial analysis:\n\ndata <- get(data(\"gov_census\", package = \"faircause\"))\ndata <- as.data.frame(data[seq_len(20000)])\nknitr::kable(head(data), caption = \"Census dataset.\")\n\n\nCensus dataset.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsex\nage\nrace\nhispanic_origin\ncitizenship\nnativity\nmarital\nfamily_size\nchildren\neducation_level\nenglish_level\nsalary\nhours_worked\nweeks_worked\noccupation\nindustry\neconomic_region\n\n\n\n\nmale\n64\nblack\nno\n1\nnative\nmarried\n2\n0\n20\n0\n43000\n56\n49\n13-1081\n928P\nSoutheast\n\n\nfemale\n54\nwhite\nno\n1\nnative\nmarried\n3\n1\n20\n0\n45000\n42\n49\n29-2061\n6231\nSoutheast\n\n\nmale\n38\nblack\nno\n1\nnative\nmarried\n3\n1\n24\n0\n99000\n50\n49\n25-1000\n611M1\nSoutheast\n\n\nfemale\n41\nasian\nno\n1\nnative\nmarried\n3\n1\n24\n0\n63000\n50\n49\n25-1000\n611M1\nSoutheast\n\n\nfemale\n40\nwhite\nno\n1\nnative\nmarried\n4\n2\n21\n0\n45200\n40\n49\n27-1010\n611M1\nSoutheast\n\n\nfemale\n46\nwhite\nno\n1\nnative\ndivorced\n3\n1\n18\n0\n28000\n40\n49\n43-6014\n6111\nSoutheast\n\n\n\n\ntapply(data$salary, data$sex, mean)\n\n  female     male \n49741.66 64795.35 \n\n\nTherefore, the data scientist observed that male employees on average earn $14000/year more than female employees, that is \\[\nE[y \\mid x_1] -  E[y \\mid x_0] = \\$14000.\n\\]\nFollowing the Fairness Cookbook, the data scientist does the following:\nSFM projection: the SFM projection of the causal diagram \\(\\mathcal{G}\\) of this dataset is given by \\[\n\\Pi_{\\text{SFM}}(\\mathcal{G}) = \\langle X = \\lbrace X \\rbrace,  Z = \\lbrace Z_1, Z_2, Z_3 \\rbrace, W = \\lbrace M, L, R\\rbrace, Y = \\lbrace Y \\rbrace\\rangle.\n\\] She then inputs this SFM projection into the faircause R-package,\n\nset.seed(2022)\nmdata <- get_metadata(\"census\")\nmdata\n\n$X\n[1] \"sex\"\n\n$W\n[1] \"marital\"         \"family_size\"     \"children\"        \"education_level\"\n[5] \"english_level\"   \"hours_worked\"    \"weeks_worked\"    \"occupation\"     \n[9] \"industry\"       \n\n$Z\n[1] \"age\"             \"race\"            \"hispanic_origin\" \"citizenship\"    \n[5] \"nativity\"        \"economic_region\"\n\n$Y\n[1] \"salary\"\n\n$x0\n[1] \"male\"\n\n$x1\n[1] \"female\"\n\n$ylvl\n[1] NA\n\nfc_census <- fairness_cookbook(data, X = mdata$X, Z = mdata$Z, W = mdata$W,\n                               Y = mdata$Y, x0 = mdata$x0, x1 = mdata$x1)\n\nautoplot(fc_census, decompose = \"xspec\", dataset = \"Census\")\n\n\n\n\nFigure 1: Fairness Cookbook on the Census dataset.\n\n\n\n\nUsing these results, she considers the following:\nDisparate treatment: when considering disparate treatment, she computes \\(x\\text{-DE}^{\\text{sym}}_{x}(y \\mid x_0)\\) and its 95% confidence interval to be \\[\nx\\text{-DE}^{\\text{sym}}_{x}(y \\mid x_0) = \\$9980 \\pm \\$1049.\n\\] The hypothesis \\(H_0^{(x\\text{-DE})}\\) is thus rejected, providing evidence of disparate treatment of females.\nDisparate impact: when considering disparate impact, she notice that Ctf-SE, Ctf-IE and their respective 95% confidence intervals equal: \\[\n    \\begin{align}\n        x\\text{-IE}^{\\text{sym}}_{x}(y \\mid x_0) &= \\$5126 \\pm \\$778,\\\\\n        x\\text{-SE}_{x_1, x_0}(y) &= -\\$1675 \\pm  \\$955.\n    \\end{align}\n\\] The data scientist decides that the differences in salary explained by the spurious correlation of gender with age, race, and nationality are not considered discriminatory. Therefore, she tests the hypothesis \\[H_0^{(x\\text{-IE})}: x\\text{-IE}^{\\text{sym}}_{x}(y \\mid x_0) = 0,\\] which is rejected, indicating evidence of disparate impact on female employees of the government. The measures computed in this example are visualized in Figure 1."
  },
  {
    "objectID": "pages/python-use.html",
    "href": "pages/python-use.html",
    "title": "Causal Fairness Analysis - from Python",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "pages/python-use.html#running-code",
    "href": "pages/python-use.html#running-code",
    "title": "Causal Fairness Analysis - from Python",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\ndat <- get(data(\"compas\", package = \"faircause\"))\nknitr::kable(head(dat), caption = \"COMPAS dataset.\")\n\n\nCOMPAS dataset.\n\n\n\n\n\n\n\n\n\n\n\n\n\nsex\nage\nrace\njuv_fel_count\njuv_misd_count\njuv_other_count\npriors_count\nc_charge_degree\ntwo_year_recid\n\n\n\n\nMale\n69\nNon-White\n0\n0\n0\n0\nF\n0\n\n\nMale\n34\nNon-White\n0\n0\n0\n0\nF\n1\n\n\nMale\n24\nNon-White\n0\n0\n1\n4\nF\n1\n\n\nMale\n23\nNon-White\n0\n1\n0\n1\nF\n0\n\n\nMale\n43\nNon-White\n0\n0\n0\n2\nF\n0\n\n\nMale\n44\nNon-White\n0\n0\n0\n0\nM\n0\n\n\n\n\n\n\nimport rpy2\nfrom rpy2.robjects.packages import importr\n# import R's \"base\" package\nfaircause = importr('faircause')\n\nfaircause.fairness_cookbook(\n  dat, X = \"race\", \n  W = [\"juv_fel_count\", \"juv_misd_count\", \"juv_other_count\", \"priors_count\",        \"c_charge_degree\"],\n  Z = [\"age\", \"sex\"],\n  Y = \"two_year_recid\",\n  x0 = \"White\",\n  x1 = \"Non-White\"\n)\n\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "pages/python-use.html#quarto",
    "href": "pages/python-use.html#quarto",
    "title": "Causal Fairness Analysis - from Python",
    "section": "Quarto",
    "text": "Quarto\nQuarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "pages/compas-t1-extended.html",
    "href": "pages/compas-t1-extended.html",
    "title": "COMPAS - Task 1",
    "section": "",
    "text": "In this vignette, we generalize the analysis of the Fairness Cookbook, to consider more refined settings described by an arbitrary causal diagram. The main motivation for doing so comes from the observation that when analyzing disparate impact, quantities such as Ctf-DE\\(_{x_0,x_1}(y\\mid x_0)\\), Ctf-IE\\(_{x_0,x_1}(y\\mid x_0)\\), and Ctf-SE\\(_{x_0,x_1}(y)\\) are insufficient to account for certain business necessity requirements. For concreteness, consider the following example."
  },
  {
    "objectID": "pages/compas-t1-extended.html#compas-under-business-necessity",
    "href": "pages/compas-t1-extended.html#compas-under-business-necessity",
    "title": "COMPAS - Task 1",
    "section": "COMPAS under Business Necessity",
    "text": "COMPAS under Business Necessity\nThe courts at Broward County, Florida, were using machine learning to predict whether individuals released on parole are at high risk of re-offending within 2 years. The algorithm is based on the demographic information \\(Z\\) (\\(Z_1\\) for gender, \\(Z_2\\) for age), race \\(X\\) (\\(x_0\\) denoting White, \\(x_1\\) Non-White), juvenile offense counts \\(J\\), prior offense count \\(P\\), and degree of charge \\(D\\).\n\n\n\n\n\nFigure 1: Causal diagram of the COMPAS dataset. Business Necessity variables highlighted in green.\n\n\n\n\nA causal analysis using the fairness_cookbook() revealed that:\n\ndata <- get(data(\"compas\", package = \"faircause\"))\nset.seed(2022)\nmdata <- SFM_proj(\"compas\")\nfc_compas <- fairness_cookbook(data, X = mdata$X, Z = mdata$Z, W = mdata$W,\n                               Y = mdata$Y, x0 = mdata$x0, x1 = mdata$x1)\n\nautoplot(fc_compas, decompose = \"xspec\", dataset = \"COMPAS\")\n\n\n\n\nFigure 2: Fairness Cookbook on the COMPAS dataset.\n\n\n\n\nThat is, the computed measures equal: \\[\n\\begin{align}\n  \\text{Ctf-IE}_{x_1, x_0}(y\\mid x_1) &= -5.7\\% \\pm 0.5\\%,\\\\\n  \\text{Ctf-SE}_{x_1, x_0}(y) &= -4.0\\% \\pm 0.9\\%,\n\\end{align}\n\\] and are also shown graphically in Figure 2, potentially indicating presence of disparate impact. Based on this information, a legal team of ProPublica filed a lawsuit to the district court, claiming discrimination w.r.t. the Non-White subpopulation based on the doctrine of disparate impact. After the court hearing, the judge rules that using the attributes age (\\(Z_2\\)), prior count (\\(P\\)), and charge degree (\\(D\\)) is not discriminatory, but using the attributes juvenile count (\\(J\\)) and gender (\\(Z_1\\)) is discriminatory. The causal diagram with a visualization of which variables are included in the business-necessity set is given in Figure 1. Data scientists at ProPublica need to consider how to proceed in the light of this new requirement for discounting the allowable attributes in the quantiative analysis.\nThe difficulty in this example is that the quantity Ctf-SE\\(_{x_1, x_0}(y)\\) measures the spurious discrimination between the attribute \\(X\\) and outcome \\(Y\\) as generated by both confounders \\(Z_1\\) and \\(Z_2\\). Since using the confounder \\(Z_2\\) is not considered discriminatory, but using the confounder \\(Z_1\\) is, the quantity Ctf-SE\\(_{x_1, x_0}(y)\\) needs to be refined such that the spurious variations based on the different confounders are disentangled. In particular, one might be interested in finding a decomposition of the spurious effect such that \\[\n\\begin{align}\n    \\text{Ctf-SE}_{x_1, x_0}(y) = \\underbrace{\\text{Ctf-SE}^{Z_1}_{x_1, x_0}(y)}_{\\text{gender variations}} + \\underbrace{\\text{Ctf-SE}^{Z_2}_{x_1, x_0}(y)}_{\\text{age variations}},\n\\end{align}\n\\] which would allow the data analyst to further distinguish the variations explained by each of the confounders. A similar challenge is present for the Ctf-IE\\(_{x_1, x_0}(y\\mid x_1)\\) measure, since it has contributions explained by juvenile offense counts \\(J\\), prior counts \\(P\\), and the charge degree \\(D\\). Therefore, we might be interested in decomposing the indirect effect into \\[\n\\begin{align}\n    \\text{Ctf-IE}_{x_1, x_0}(y \\mid x_1) =& \\underbrace{\\text{Ctf-IE}^J_{x_1, x_0}(y \\mid x_1)}_{\\text{juvenile count variations}} + \\underbrace{\\text{Ctf-IE}^P_{x_1, x_0}(y \\mid x_1)}_{\\text{prior count variations}} \\\\ &+ \\underbrace{\\text{Ctf-IE}^D_{x_1, x_0}(y \\mid x_1)}_{\\text{charge degree variations}}. \\nonumber\n\\end{align}\n\\] Again, such a decomposition would allow the data analyst to better understand the contribution of each of the mediators to the totality of the indirect effect. In situations when some mediating variables are in the business necessity set, while others are not, such a decomposition would allow for assessment of disparate impact claims.\n\nTowards the solution"
  }
]